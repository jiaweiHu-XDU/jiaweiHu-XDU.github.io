<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Markov Decision Process Model Based on Value Iteration - Jiawei Hu - Jiawei Hu&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Jiawei Hu - Jiawei Hu&#039;s Blog"><meta name="msapplication-TileImage" content="/img/logo.png"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jiawei Hu - Jiawei Hu&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Using the taxi example of OpenAI Gym to achieve and tune MDP model in Reinforcement Learning based on value iteration."><meta property="og:type" content="blog"><meta property="og:title" content="Markov Decision Process Model Based on Value Iteration"><meta property="og:url" content="https://jiaweihu-xdu.github.io/projects/ValueIterationMDP/"><meta property="og:site_name" content="Jiawei Hu - Jiawei Hu&#039;s Blog"><meta property="og:description" content="Using the taxi example of OpenAI Gym to achieve and tune MDP model in Reinforcement Learning based on value iteration."><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/DRL/randomPolicy.pnt0kxzusv4.gif"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/DRL/optimalPolicy.2wskea2qtzi0.gif"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/DRL/Rewards.2uwj07wcwru0.webp"><meta property="article:published_time" content="2023-10-20T06:51:26.000Z"><meta property="article:modified_time" content="2023-12-15T06:52:53.760Z"><meta property="article:author" content="Jiawei Hu"><meta property="article:tag" content="Deep Reinforcement Learning"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/DRL/randomPolicy.pnt0kxzusv4.gif"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://jiaweihu-xdu.github.io/projects/ValueIterationMDP/"},"headline":"Markov Decision Process Model Based on Value Iteration","image":["https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/DRL/randomPolicy.pnt0kxzusv4.gif","https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/DRL/optimalPolicy.2wskea2qtzi0.gif","https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/DRL/Rewards.2uwj07wcwru0.webp"],"datePublished":"2023-10-20T06:51:26.000Z","dateModified":"2023-12-15T06:52:53.760Z","author":{"@type":"Person","name":"Jiawei Hu"},"publisher":{"@type":"Organization","name":"Jiawei Hu - Jiawei Hu's Blog","logo":{"@type":"ImageObject","url":"https://jiaweihu-xdu.github.io/img/logo.png"}},"description":"Using the taxi example of OpenAI Gym to achieve and tune MDP model in Reinforcement Learning based on value iteration."}</script><link rel="canonical" href="https://jiaweihu-xdu.github.io/projects/ValueIterationMDP/"><link rel="icon" href="/img/logo.png"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/6.0.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body class="is-3-column"><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Jiawei Hu - Jiawei Hu&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">JIAWEI HU&#039;S LOG BOOK</a><a class="navbar-item" href="/blog">BLOG</a><a class="navbar-item" href="/knowledge">KNOWLEDGE</a><a class="navbar-item" href="/publications">PUBLICATIONS</a><a class="navbar-item" href="/projects">PROJECTS</a><a class="navbar-item" href="/life">LIFE</a><a class="navbar-item" href="/readings">READINGS</a><a class="navbar-item" href="/collaboration">COLLABORATION</a><a class="navbar-item" href="/archives">ARCHIVES</a><a class="navbar-item" href="/categories">CATEGORIES</a><a class="navbar-item" href="/tags">TAGS</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/jiaweiHu-XDU/jiaweiHu-XDU.github.io"><i class="fab fa-github"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Gitee" href="https://gitee.com/hujiawe_i"><i class="fa-brands fa-gofore"></i></a><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">Markov Decision Process Model Based on Value Iteration</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2023-10-20T06:51:26.000Z" title="2023-10-20T06:51:26.000Z">2023-10-20</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="2023-12-15T06:52:53.760Z" title="2023-12-15T06:52:53.760Z">2023-12-15</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/projects/">projects</a></span><span class="level-item"><i class="far fa-clock"></i> 11 minutes read (About 1704 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><h2 id="Theories"><a href="#Theories" class="headerlink" title="Theories"></a>Theories</h2><h3 id="Markov-Decision-Process"><a href="#Markov-Decision-Process" class="headerlink" title="Markov Decision Process"></a>Markov Decision Process</h3><p>Generally, we notes a MDP model as $(S, A, T_a, R_a, \gamma)$. Its transition function is $T_a(s,s’)=\Pr(s_{t+1}|s_t=s, a_t=a)$, reward function is $R_a(s,s’)$. And actions choosing satisfies a specific distribution.<br>The cotinuous decisions are noted as trace $\tau$, formally in formula:</p>
<center>$\tau=${$s_t, a_t, r_t, s_{t+1}, \cdots, a_{t+n}, r_{t+n}, s_{t+n+1}$}</center>

<p>And in many situations, we very care about the expected reward of a specific trace because that will support us to choose the optimal action currently. So we use the method like weighted time series to calculate cumulative reward:</p>
<p>$$<br>R(\tau_t) = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots=r_t+\sum_{i=1}^\infty \gamma^ir_{t+i}<br>$$</p>
<p>After we got the return value of traces, we can just calculate the value of a state to form our policy.</p>
<p>$$<br>V^{\pi}(s)=E_{\tau\sim p(\tau_t)}[\sum_{i=0}^\infty \gamma^ir_{t+i}|s_t=s]<br>$$</p>
<p>However, although we can get the value function to form optimal policy, we cann’t still calculate the values of all states. So we need Bellmax Equation to solve the problem.</p>
<h3 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h3><p>$$<br>V^{\pi}(s)=\sum_{a\in A}\pi(a|s)[\sum_{s’\in S}T_a(s,s’)[R_a(s,s’)+\gamma V^{\pi}(s’)]<br>$$</p>
<p>For a specific state $s$, when choosing some action, we will get a stochastic new state which satisfies some distribution. <em>Bellman Equation</em> tells us to calculate the expected average value of these possible new states’ return. And in detail, the return of each state have two parts: the immediate reward $R_a(s,s’)$ and the future reward $\gamma V^{\pi}(s’)$. That inspires us that we can calculate the value of states recursively.</p>
<h3 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h3><p>Value Iteration is a method to calculate <em>Bellman Equation</em> by traversing the state and action space. Firstly, it stores a value table of all states. And in traversing process, it will calculate the value of each state and update the value table by choosing the action with the highest return.</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Taxi-Environment-of-OpenAI-Gym"><a href="#Taxi-Environment-of-OpenAI-Gym" class="headerlink" title="Taxi Environment of OpenAI Gym"></a>Taxi Environment of OpenAI Gym</h3><ul>
<li>Taxi Enviroment<br>The Taxi example is an environment where taxis move up, down, left, and right, and pichup and dropoff passengers. There are four disignated locations in the Grid world indicated by R(ed), B(lue), G(reen), and Y(ellow).</li>
<li>Taxi Activities<br>In an episode, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger’s location, picks up the passenger, then drives to the passenger’s  destination(another one of the four specified locations), and drops off the passenger.</li>
<li>States and Actions Space<ul>
<li>$500=25\times5\times4$ discrete states<br>With the grid size of $5 \times 5$, there are $25$ taxi positions. For the passenger, there are $5$ possible locations(including the case when the passenger is in the taxi). For the destination, there are $4$ possible locations.</li>
<li>$6$ discrete deterministic actions<br>For the Taxi diver,<ul>
<li>$0$: Move south</li>
<li>$1$: Move north</li>
<li>$2$: Move east</li>
<li>$3$: Move west</li>
<li>$4$: Pick up passenger</li>
<li>$5$: Drop off passenger</li>
</ul>
</li>
</ul>
</li>
<li>Rewards<ul>
<li>$-1$ for each action</li>
<li>$+20$ for delivering the passenger</li>
<li>$-10$ for picking up and dropping off the passenger illegally</li>
</ul>
</li>
</ul>
<p>The following pictures are taxi example demostration. The left shows taxi actions with a random policy and the right shows taxi actions with the optimal policy.</p>
<div class="justified-gallery">
<img src="https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/DRL/randomPolicy.pnt0kxzusv4.gif" alt="randomPolicy" />
<img src="https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/DRL/optimalPolicy.2wskea2qtzi0.gif" alt="optimalPolicy" />
</div>

<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Now we want to check how the discount factor influences the value function from the same start state. So we choosing the discount factor ranging from $0.0$ to $1.0$ with footstep of 0.05 to measure the average rewards and cumulative rewards on random group and optimal group.</p>
<img src="https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/DRL/Rewards.2uwj07wcwru0.webp" alt="Tuning MDP Results" width="70%"/>

<table>
<thead>
<tr>
<th align="center">Discount Factor</th>
<th align="center">Random Cum_Reward</th>
<th align="center">Random_Aver_Reward</th>
<th align="center">Optimal Cum_Reward</th>
<th align="center">Optimal_Aver_Reward</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0.00</td>
<td align="center">-37</td>
<td align="center">-3.70</td>
<td align="center">-20</td>
<td align="center">-2.00</td>
</tr>
<tr>
<td align="center">0.05</td>
<td align="center">-10</td>
<td align="center">-1.00</td>
<td align="center">-20</td>
<td align="center">-1.00</td>
</tr>
<tr>
<td align="center">0.10</td>
<td align="center">-55</td>
<td align="center">-5.50</td>
<td align="center">10</td>
<td align="center">0.91</td>
</tr>
<tr>
<td align="center">0.15</td>
<td align="center">-37</td>
<td align="center">-3.70</td>
<td align="center">11</td>
<td align="center">1.10</td>
</tr>
<tr>
<td align="center">0.20</td>
<td align="center">-55</td>
<td align="center">-5.50</td>
<td align="center">-20</td>
<td align="center">-1.00</td>
</tr>
<tr>
<td align="center">0.25</td>
<td align="center">-28</td>
<td align="center">-2.80</td>
<td align="center">15</td>
<td align="center">2.50</td>
</tr>
<tr>
<td align="center">0.30</td>
<td align="center">-46</td>
<td align="center">-4.60</td>
<td align="center">11</td>
<td align="center">1.10</td>
</tr>
<tr>
<td align="center">0.35</td>
<td align="center">-28</td>
<td align="center">-2.80</td>
<td align="center">5</td>
<td align="center">0.31</td>
</tr>
<tr>
<td align="center">0.40</td>
<td align="center">-10</td>
<td align="center">-1.00</td>
<td align="center">7</td>
<td align="center">0.50</td>
</tr>
<tr>
<td align="center">0.45</td>
<td align="center">-37</td>
<td align="center">-3.70</td>
<td align="center">7</td>
<td align="center">0.50</td>
</tr>
<tr>
<td align="center">0.50</td>
<td align="center">-64</td>
<td align="center">-6.40</td>
<td align="center">7</td>
<td align="center">0.50</td>
</tr>
<tr>
<td align="center">0.55</td>
<td align="center">-19</td>
<td align="center">-1.90</td>
<td align="center">13</td>
<td align="center">1.60</td>
</tr>
<tr>
<td align="center">0.60</td>
<td align="center">-28</td>
<td align="center">-2.80</td>
<td align="center">9</td>
<td align="center">0.75</td>
</tr>
<tr>
<td align="center">0.65</td>
<td align="center">-46</td>
<td align="center">-4.60</td>
<td align="center">10</td>
<td align="center">0.91</td>
</tr>
<tr>
<td align="center">0.70</td>
<td align="center">-37</td>
<td align="center">-3.70</td>
<td align="center">9</td>
<td align="center">0.75</td>
</tr>
<tr>
<td align="center">0.75</td>
<td align="center">-46</td>
<td align="center">-4.60</td>
<td align="center">6</td>
<td align="center">0.40</td>
</tr>
<tr>
<td align="center">0.80</td>
<td align="center">-37</td>
<td align="center">-3.70</td>
<td align="center">4</td>
<td align="center">0.24</td>
</tr>
<tr>
<td align="center">0.85</td>
<td align="center">-37</td>
<td align="center">-3.70</td>
<td align="center">7</td>
<td align="center">0.50</td>
</tr>
<tr>
<td align="center">0.90</td>
<td align="center">-28</td>
<td align="center">-2.80</td>
<td align="center">7</td>
<td align="center">0.50</td>
</tr>
<tr>
<td align="center">0.95</td>
<td align="center">-37</td>
<td align="center">-3.70</td>
<td align="center">5</td>
<td align="center">0.31</td>
</tr>
<tr>
<td align="center">1.00</td>
<td align="center">-37</td>
<td align="center">-3.70</td>
<td align="center">11</td>
<td align="center">1.10</td>
</tr>
</tbody></table>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>From the following experimental results, we can conclude that the discount factor has a significant impact on the value function. The optimal group has a higher average and cumulative reward than the random group, and the discount factor has a lower bound $\gamma=0.4$ to get optimal policy.<br>In my opinion, the discount factor reflects the future reward’s influence on the current state. If it is set too small, that means the most reward comes from the immediate reward which is a greedy policy with the possibility of failure. On the other hand, if set too high, we also cann’t get the best action with the highest reward. So we’d better to set the discount factor to an appropriate value.</p>
<h2 id="Codes"><a href="#Codes" class="headerlink" title="Codes"></a>Codes</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaseOptions</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.parser = ArgumentParser()</span><br><span class="line">        self.parser.add_argument(<span class="string">&#x27;--algorithm&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;ValueItration&#x27;</span>)</span><br><span class="line">        self.parser.add_argument(<span class="string">&#x27;--n_rounds&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">500</span>, <span class="built_in">help</span>=<span class="string">&#x27;Number of rounds&#x27;</span>)</span><br><span class="line">        self.parser.add_argument(<span class="string">&#x27;--ub_gamma&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">1</span>, <span class="built_in">help</span>=<span class="string">&#x27;upper bound of discount factor&#x27;</span>)</span><br><span class="line">        self.parser.add_argument(<span class="string">&#x27;--lb_gamma&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0</span>, <span class="built_in">help</span>=<span class="string">&#x27;lower bound of discount factor&#x27;</span>)</span><br><span class="line">        self.parser.add_argument(<span class="string">&#x27;--NA&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">6</span>, <span class="built_in">help</span>=<span class="string">&#x27;Length of Actions Space&#x27;</span>)</span><br><span class="line">        self.parser.add_argument(<span class="string">&#x27;--NS&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">500</span>, <span class="built_in">help</span>=<span class="string">&#x27;Length of States Space&#x27;</span>)</span><br><span class="line">        self.parser.add_argument(<span class="string">&#x27;--end_delta&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.00001</span>, <span class="built_in">help</span>=<span class="string">&#x27;end delta&#x27;</span>)</span><br><span class="line">        self.parser.add_argument(<span class="string">&#x27;--print_interval&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">50</span>, <span class="built_in">help</span>=<span class="string">&#x27;print interval&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.parser.parse_args()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">-------------------------------------------------------</span></span><br><span class="line"><span class="string">Project: Solving as MDP using Value Iteration Algorithm</span></span><br><span class="line"><span class="string">Author: Zhihao Li</span></span><br><span class="line"><span class="string">Date: October 19, 2023</span></span><br><span class="line"><span class="string">Research Content: Deep Reinforcement Learning</span></span><br><span class="line"><span class="string">-------------------------------------------------------</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> options <span class="keyword">import</span> BaseOptions</span><br><span class="line"><span class="keyword">from</span> value_iteration <span class="keyword">import</span> ValueMDP</span><br><span class="line"><span class="keyword">import</span> gym                 <span class="comment"># openAi gym</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> FontProperties</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up Seaborn style</span></span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&quot;darkgrid&quot;</span>)</span><br><span class="line">Efont_prop = FontProperties(fname=<span class="string">&quot;C:\Windows\Fonts\ARLRDBD.TTF&quot;</span>)</span><br><span class="line">label_prop = FontProperties(family=<span class="string">&#x27;serif&#x27;</span>, size=<span class="number">7</span>, weight=<span class="string">&#x27;normal&#x27;</span>)</span><br><span class="line">legend_font = FontProperties(family=<span class="string">&#x27;serif&#x27;</span>, size=<span class="number">7</span>, weight=<span class="string">&#x27;normal&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    opts = BaseOptions().parse()         <span class="comment"># set project&#x27;s options</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set OpenAI Gym environment</span></span><br><span class="line">    env = gym.make(<span class="string">&#x27;Taxi-v3&#x27;</span>, render_mode=<span class="string">&quot;rgb_array&quot;</span>)</span><br><span class="line"></span><br><span class="line">    gamma_delta = <span class="number">0.01</span></span><br><span class="line">    aver_rewards = np.zeros(<span class="built_in">len</span>(np.arange(opts.lb_gamma, opts.ub_gamma + gamma_delta, gamma_delta)))</span><br><span class="line">    random_aver_rewards = np.zeros(aver_rewards.shape)</span><br><span class="line">    cum_rewards = np.zeros(aver_rewards.shape)</span><br><span class="line">    random_cum_rewards = np.zeros(aver_rewards.shape)</span><br><span class="line">    <span class="keyword">for</span> t, gamma <span class="keyword">in</span> <span class="built_in">enumerate</span>(np.arange(opts.lb_gamma, opts.ub_gamma + gamma_delta, gamma_delta)):</span><br><span class="line">        <span class="comment"># Init env and value iteration process</span></span><br><span class="line">        VIMDP = ValueMDP(env, opts, gamma)</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># Apply the random policy</span></span><br><span class="line">        VIMDP.env.reset(seed=t+<span class="number">101</span>)</span><br><span class="line">        VIMDP.ApplyRandomPolicy(steps=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Value Iteration in MDP</span></span><br><span class="line">        observation = VIMDP.env.reset(seed=t+<span class="number">101</span>)</span><br><span class="line">        VIMDP.IterateValueFunction()</span><br><span class="line">      </span><br><span class="line">        <span class="comment"># Apply the optimal policy</span></span><br><span class="line">        VIMDP.ApplyOptimalPolicy(observation[<span class="number">0</span>], steps=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save reward results</span></span><br><span class="line">        aver_rewards[t] = VIMDP.aver_reward</span><br><span class="line">        random_aver_rewards[t] = VIMDP.random_aver_reward</span><br><span class="line">        cum_rewards[t] = VIMDP.cum_reward</span><br><span class="line">        random_cum_rewards[t] = VIMDP.random_cum_reward</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;discount factor: %f&quot;</span> % gamma)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Applying the random policy, accumulated reward: %.5f, average reward: %.5f&quot;</span> % (random_cum_rewards[t], random_aver_rewards[t]))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Applying the optimal policy, accumulated reward: %.5f, average reward: %.5f&quot;</span> % (cum_rewards[t], aver_rewards[t]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the rewards</span></span><br><span class="line">    xdata = np.arange(opts.lb_gamma, opts.ub_gamma + gamma_delta, gamma_delta)</span><br><span class="line">    plt.subplot(<span class="number">211</span>)</span><br><span class="line">    plt.plot(xdata, random_aver_rewards, <span class="string">&#x27;b-&#x27;</span>, label=<span class="string">&#x27;random policy&#x27;</span>)</span><br><span class="line">    plt.plot(xdata, aver_rewards, <span class="string">&#x27;g-&#x27;</span>, label=<span class="string">&#x27;optimal policy&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Average Rewards&#x27;</span>, fontproperties=Efont_prop, fontsize=<span class="number">9</span>)</span><br><span class="line">    plt.yticks(fontproperties=label_prop, fontsize=<span class="number">7</span>)</span><br><span class="line">    plt.xticks(fontproperties=label_prop, fontsize=<span class="number">7</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>, fontsize=<span class="number">7</span>, prop=legend_font)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">212</span>)</span><br><span class="line">    plt.plot(xdata, random_cum_rewards, <span class="string">&#x27;b--&#x27;</span>, label=<span class="string">&#x27;random policy&#x27;</span>)</span><br><span class="line">    plt.plot(xdata, cum_rewards, <span class="string">&#x27;g--&#x27;</span>, label=<span class="string">&#x27;optimal policy&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Discount Factor&#x27;</span>, fontproperties=Efont_prop, fontsize=<span class="number">9</span>) </span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Cumulative Rewards&#x27;</span>, fontproperties=Efont_prop, fontsize=<span class="number">9</span>)</span><br><span class="line">    plt.yticks(fontproperties=label_prop, fontsize=<span class="number">7</span>)</span><br><span class="line">    plt.xticks(fontproperties=label_prop, fontsize=<span class="number">7</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;lower right&#x27;</span>, fontsize=<span class="number">7</span>, prop=legend_font)</span><br><span class="line"></span><br><span class="line">    plt.savefig(<span class="string">&quot;Rewards.png&quot;</span>, dpi=<span class="number">400</span>)</span><br><span class="line">    env.close()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">--------------------------------------------------------------------------------------</span></span><br><span class="line"><span class="string">This section is for Value Iteration Algorithm for Taxi Gym.</span></span><br><span class="line"><span class="string">Author: Zhihao Li</span></span><br><span class="line"><span class="string">Date: October 19, 2023</span></span><br><span class="line"><span class="string">Arguments:</span></span><br><span class="line"><span class="string">    env: OpenAI env. env.P represents the transition probabilities of the environment.</span></span><br><span class="line"><span class="string">        env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).</span></span><br><span class="line"><span class="string">    end_delta: Stop evaluation once value function change is less than end_delta for all states.</span></span><br><span class="line"><span class="string">    discount_factor: Gamma discount factor.</span></span><br><span class="line"><span class="string">--------------------------------------------------------------------------------------</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ValueMDP</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, env, opts, gamma</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.env = env                    <span class="comment"># taxi gym environment</span></span><br><span class="line">        self.gamma = gamma           <span class="comment"># discount_factor</span></span><br><span class="line">        self.NA = opts.NA                 <span class="comment"># Actions Space&#x27;s Length</span></span><br><span class="line">        self.NS = opts.NS                 <span class="comment"># States Space&#x27;s Length</span></span><br><span class="line">        self.V = np.zeros(self.NS)        <span class="comment"># Value Function</span></span><br><span class="line">        self.end_delta = opts.end_delta   <span class="comment"># Delta value for stopping iteration</span></span><br><span class="line">        self.new_policy = np.zeros(self.NS)    <span class="comment"># the optimal policy</span></span><br><span class="line">        self.cum_reward = <span class="number">0</span>               <span class="comment"># apply new policy and get all rewards</span></span><br><span class="line">        self.aver_reward = <span class="number">0</span></span><br><span class="line">        self.random_cum_reward = <span class="number">0</span>        <span class="comment"># rewards applying random actions</span></span><br><span class="line">        self.random_aver_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">SingleStepIteration</span>(<span class="params">self, state</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Function: calculate the state value for all actions in a given state </span></span><br><span class="line"><span class="string">                  and update the value function.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            The estimate of actions.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        action_V = np.zeros(self.NA)     <span class="comment"># Record the value of each action</span></span><br><span class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> <span class="built_in">range</span>(self.NA):</span><br><span class="line">            <span class="keyword">for</span> prob, nextState, reward, is_final <span class="keyword">in</span> self.env.P[state][action]:</span><br><span class="line">                action_V[action] += prob * (reward + self.gamma * self.V[nextState] * (<span class="keyword">not</span> is_final))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> action_V</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">IterateValueFunction</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            delta = <span class="number">0</span>           <span class="comment"># initialize the every round of delta</span></span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.NS):</span><br><span class="line">                newValue = np.<span class="built_in">max</span>(self.SingleStepIteration(s))</span><br><span class="line">                delta = <span class="built_in">max</span>(delta, np.<span class="built_in">abs</span>(newValue - self.V[s]))</span><br><span class="line">                self.V[s] = newValue          <span class="comment"># updates value function</span></span><br><span class="line">          </span><br><span class="line">            <span class="keyword">if</span> delta &lt; self.end_delta:    <span class="comment"># the maximum delta of all states</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">      </span><br><span class="line">        <span class="comment"># get optimal policy</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> <span class="built_in">range</span>(self.NS):         <span class="comment"># for all states, create deterministic policy</span></span><br><span class="line">            newAction = np.argmax(self.SingleStepIteration(s))</span><br><span class="line">            self.new_policy[s] = newAction</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ApplyOptimalPolicy</span>(<span class="params">self, observation, steps</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(steps):</span><br><span class="line">            action = self.new_policy[observation]</span><br><span class="line">            observation, reward, is_final, truncated, info = self.env.step(np.int8(action))</span><br><span class="line">            self.cum_reward += reward</span><br><span class="line"></span><br><span class="line">            <span class="comment"># self.env.render()</span></span><br><span class="line">            <span class="keyword">if</span> is_final:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        self.aver_reward = self.cum_reward / (i + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ApplyRandomPolicy</span>(<span class="params">self, steps</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(steps):</span><br><span class="line">            observation, reward, is_final, truncated, info = self.env.step(self.env.action_space.sample())</span><br><span class="line">            self.random_cum_reward += reward</span><br><span class="line">            <span class="comment"># self.env.render()</span></span><br><span class="line">            <span class="keyword">if</span> is_final:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        self.random_aver_reward = self.random_cum_reward / (i+<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Contributors"><a href="#Contributors" class="headerlink" title="Contributors"></a>Contributors</h2><ul>
<li><a target="_blank" rel="noopener" href="https://lzhms.github.io/">Zhihao Li</a></li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://www.gymlibrary.dev/">OpenAI Gym</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>Markov Decision Process Model Based on Value Iteration</p><p><a href="https://jiaweihu-xdu.github.io/projects/ValueIterationMDP/">https://jiaweihu-xdu.github.io/projects/ValueIterationMDP/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Jiawei Hu</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-10-20</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-12-15</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/Deep-Reinforcement-Learning/">Deep Reinforcement Learning </a></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=64ad5faad2ddeb0019614bc5&amp;product=inline-share-buttons&amp;source=platform" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/alipay.png" alt="Alipay"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/wechat.png" alt="Wechat"></span></a><a class="button donate" href="https://www.buymeacoffee.com/DustValor" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/OpenSSLConnectionWithGithub/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Solution to OpenSSL Connection Problems With Github</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/collaboration/DataBase/"><span class="level-item">数据库系统概论</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://jiaweihu-xdu.github.io/projects/ValueIterationMDP/';
            this.page.identifier = 'projects/ValueIterationMDP/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'eblog-5' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-96x96 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpg" alt="Jiawei Hu"></figure><p class="title is-size-4 is-block" style="line-height: 'inherit'; font-family: Times New Roman">Jiawei Hu</p><p style="white-space: pre-line; font-style: italic; font-family: Times New Roman; margin-bottom: 0.50rem; font-size: 1.0em">Computer Science
Machine Learning
</p><p class="is-size-5 is-flex justify-content-center" style="font-family: Times New Roman"><i class="fas fa-map-marker-alt mr-1"></i><span>Xidian University, China</span></p></div></div></nav><nav class="level menu-list is-mobile" style="margin-bottom:1rem"><a class="level-item has-text-centered is-marginless" href="/archives"><div><p class="heading">Posts</p><div><p class="title">40</p></div></div></a><a class="level-item has-text-centered is-marginless" href="/categories"><div><p class="heading">Categories</p><div><p class="title">7</p></div></div></a><a class="level-item has-text-centered is-marginless" href="/tags"><div><p class="heading">Tags</p><div><p class="title">25</p></div></div></a></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/JiaweiHu-XDU" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/JiaweiHu-XDU"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Cnblog" href="https://www.cnblogs.com/MarkStiff/"><i class="fa-brands fa-blogger"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Notion" href="https://zhihaoli.notion.site/"><i class="fa-solid fa-desktop"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:jiaweihu_xdu@163.com"><i class="fa-solid fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="QQ" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1444980878&amp;website=www.oicqzone.com"><i class="fab fa-qq"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Theories"><span class="level-left"><span class="level-item">1</span><span class="level-item">Theories</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Markov-Decision-Process"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Markov Decision Process</span></span></a></li><li><a class="level is-mobile" href="#Bellman-Equation"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">Bellman Equation</span></span></a></li><li><a class="level is-mobile" href="#Value-Iteration"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">Value Iteration</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Experiments"><span class="level-left"><span class="level-item">2</span><span class="level-item">Experiments</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Taxi-Environment-of-OpenAI-Gym"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Taxi Environment of OpenAI Gym</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Results"><span class="level-left"><span class="level-item">3</span><span class="level-item">Results</span></span></a></li><li><a class="level is-mobile" href="#Conclusions"><span class="level-left"><span class="level-item">4</span><span class="level-item">Conclusions</span></span></a></li><li><a class="level is-mobile" href="#Codes"><span class="level-left"><span class="level-item">5</span><span class="level-item">Codes</span></span></a></li><li><a class="level is-mobile" href="#Contributors"><span class="level-left"><span class="level-item">6</span><span class="level-item">Contributors</span></span></a></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">7</span><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/"><span class="level-start"><span class="level-item">blog</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/collaboration/"><span class="level-start"><span class="level-item">collaboration</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/knowledge/"><span class="level-start"><span class="level-item">knowledge</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/life/"><span class="level-start"><span class="level-item">life</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/projects/"><span class="level-start"><span class="level-item">projects</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/publications/"><span class="level-start"><span class="level-item">publications</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/readings/"><span class="level-start"><span class="level-item">readings</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Collaboration-Project/"><span class="tag">Collaboration Project</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/College-Life/"><span class="tag">College Life</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Computer-Network/"><span class="tag">Computer Network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Computer-Principle/"><span class="tag">Computer Principle</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Computer-Vision/"><span class="tag">Computer Vision</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Visualization/"><span class="tag">Data Visualization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-Reinforcement-Learning/"><span class="tag">Deep Reinforcement Learning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Eletromagnetic-B%E6%B5%8B/"><span class="tag">Eletromagnetic B测</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Eletromagnetic-Physics/"><span class="tag">Eletromagnetic Physics</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embedded-System/"><span class="tag">Embedded System</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Health/"><span class="tag">Health</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Life-Knowledge/"><span class="tag">Life Knowledge</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Life-Wisdom/"><span class="tag">Life Wisdom</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Literature-Survey/"><span class="tag">Literature Survey</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mathematical-Modeling/"><span class="tag">Mathematical Modeling</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Microcomputer/"><span class="tag">Microcomputer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenSSL/"><span class="tag">OpenSSL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Professional-Knowledge/"><span class="tag">Professional Knowledge</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Project-Habits/"><span class="tag">Project Habits</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Project-Tools/"><span class="tag">Project Tools</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reinforcement-Learning/"><span class="tag">Reinforcement Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Research-Habits/"><span class="tag">Research Habits</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Jiawei Hu - Jiawei Hu&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2023-2024 Jiawei Hu</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv"><span id="busuanzi_container_site_uv">Site UV: <span id="busuanzi_value_site_uv"></span></span>   <span id="busuanzi_container_site_pv">Site PV: <span id="busuanzi_value_site_pv"></span></span></span></p><p class="is-size-7"> </p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/jiaweiHu-XDU/jiaweiHu-XDU.github.io"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><div id="dark" onclick="switchDarkMode()"></div><script type="text/javascript" src="/js/universe.js"></script></body></html>