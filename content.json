{"posts":[{"title":"计算机通信与网络","text":"数据通信与网络课件第一章 概述1.1 第一讲 绪论1.2 第二讲 网络模型第二章 物理层和介质2.1 第三讲 物理层2.2 第四讲 数字传输2.3 第五讲 模拟传输2.4 第六讲 带宽利用2.5 第七讲 传输介质2.6 第八讲 交换2.7 第九讲 使用电话网和有线电视网进行数据传输第三章 数据链路层3.1 第十讲 检错与纠错3.2 第十一讲 数据链路控制3.3 第十二讲 多路访问3.4 第十三讲 有线局域网：以太网3.5 第十四讲 无线局域网3.6 第十五讲 连接局域网、主干网和虚拟局域网3.7 第十七讲 广域网3.8 第十八讲 虚电路网络：帧中断和ATM第四章 网络层4.1 第十九讲 逻辑寻址4.2 第二十讲 IP协议4.3 第二十一讲 地址映射、差错报告和多播4.4 第二十二讲 传递、转发和路由选择第五章 传输层5.1 第二十三讲 UDP、TCP和SCTP5.2 第二十四讲 拥塞控制和服务质量计算机通信与网络课程总复习知识点第一部分 概述第1章 绪论1、数据通信基本概念及模型数据通信是在两台设备之间通过诸如线缆的某种形式的传输介质进行的数据交换。【数据通信系统模型】——五个组成部分 报文：进行通信的信息（数据）发送方：发送数据报文的设备接收方：接受报文的设备传输介质：报文从发送方到接收方之间所经过的物理通路协议：管理数据通信的一组规则，表示通信设备之间的一组约定 2、数据流：单工、半双工、全双工 单工模式（Simplex Mode)：通信是单方向的，两台设备只有一台能够发送，另一台则只能接受半双工模式（Half-duplex Mode)：每台主机均能发送和接受，但不能同时进行全双工模式（Full-duplex Mode)：双方主机都能同时发送和接受 3、连接类型和拓扑结构 点到点连接：提供两台设备之间专用的链路，链路全部的能力均为两台设备之间的传输所共用多点连接：两台以上设备共享单一链路的情形，通道的能力在空间和时间上共享拓扑结构：网状、星型、总线、环状 4、网络分类：局域网、城域网、广域网 网络 规模 介质 拓扑结构 局域网LAN 小于2英里 一种类型的传输介质 总线结构、环状结构和星型结构 城域网MAN 十几英里 广域网WAN 世界范围 5、协议和标准【协议三要素】 语法：指数据的结构或格式，即它们是以何种顺序表示的语义：指每一位片段的含义：如何解释一个特别的位模式时序：报文发送的时间和发送的顺序 【TCP/IP】 传输控制协议（TCP）负责高层功能，比如分段、重组和差错控制网际协议（TP）负责处理数据包路由 第2章 网络模型1、层次结构、层间接口和封装的概念【层次结构】：把相关的网络功能组合在一层中，每层都使用其直接下层提供的服务，保持网络灵活且易修改【层间接口】：数据和网络信息从高层向低层传递和从低层向高层传递都通过相邻两层的接口进行，每一层接口都定义了该层必须向上层提供信息和服务【封装】：第N-1层的分组中的数据部分是第N层的完整分组（数据、头部、也可能有尾部）2、OSI参考模型【开放系统互联（Open System Interconnection)模型】 对等协议：传输层、会话层、表示层、应用层中间节点只涉及下三层：物理层、数据链路层、网络层 物理层：负责位从一个节点到另一个节点的传递 位的表示：要进行位流的传输必须将位编码成信号——电信号或光信号（物理层定义编码的类型）数据速率：定义传输速率（每秒发送的位数）位同步：发送方与接收方不仅使用相同的比特率还必须位同步保证时钟同步线路配置：点到点连接或多点共享连接物理拓扑结构：定义设备连接成网络的连接方式（网状、星型、总线、环状）传输方式：定义两台设备之间的传输方式（单工、半双工和全双工） 数据链路层：负责帧从一跳到下一跳的传递 成帧：数据链路层将接收到的来自网络层的位流分成称为帧的易处理数据单元物理寻址：1、不同系统 数据链路层在帧的头部添加发送方的物理地址和接收方的物理地址 2、不同网络 数据链路层在帧的头部添加发送方的物理地址和连接下一个网络的路由地址流量控制：采用流量控制机制防止接收方过载差错控制：检测与重发损坏帧或丢失帧 网络层：负责将各个分组从源地址传递到目的地址 逻辑寻址：物理寻址负责处理本地网络寻址问题（数据链路层），逻辑寻址负责处理分组通过网络边界的寻址问题路由选择：在互联网中由连接设备（路由器或网关）负责把分组送到指定网络的目的地 传输层：负责一个报文从一个进程到另一个进程的传递 端口寻址：传输层信息的头部必须包含端口地址（服务点地址），将整个报文传送到计算机上的指定进程分段和组装：将报文分解成可传输的片段并进行编号，使得1、传输层可以在接收端将报文正确地组装 2、用来标识和替换传输中丢失的分组连接控制：传输层可以是无连接的或面向连接的流量控制：负责端到端上的流量控制而不是单条链路差错控制：负责进程到进程上的差错控制而不是单条链路上 会话层：负责对话控制和同步 对话控制：允许两个进程之间以半双工或全双工方式进行通信同步：会话层允许一个进程在数据流中增加检查点或同步点 表示层：负责翻译、加密和压缩数据 应用层：负责向用户提供服务 3、TCP/IP协议族【层次组成】：物理层和数据链路层（主机到网络层）、网络层、传输层和应用层 网络层 IP协议：不可靠、无连接、主机到主机协议 组成部分： 地址解析协议（ARP)：将逻辑地址与物理地址联系起来逆地址解析协议（RARP)：允许主机在仅知道物理地址的情况下寻找因特网地址因特网控制报协议（ICMP)：用来向发送方通知数据报所发生的问题因特网组报文协议（IGMP)：用于将一个报文发送给一组接收者 传输层 用户数据报协议（UDP)：进程到进程的协议即端到端的协议传输控制协议（TCP)：是面向连接的、可靠的流传输协议流控制传输协议（SCTP） 4、寻址【物理（链路）地址（MAC地址）】 用于物理层和数据链路层以太网中使用6个字节（48位）物理地址，标明在网卡上，长度格式可变 【逻辑（IP）地址】 用于网络层因特网的逻辑地址是32位地址，唯一地标识了连接到因特网上的一台主机 【端口地址】 用于传输层赋予进程的标识符称为端口地址，长度固定16位地址 跳到跳时物理地址将改变，但逻辑地址和端口地址保持不变 【专用地址】 用于应用层 第二部分 物理层和介质第3章 物理层1、数字信号传输【电平编码】 信号电平数 $L$，编码电平需要比特位数: $N=\\log_2L$ 注：通过对多电平编码可以增加每个电平表示的比特位 数字信号是无穷大带宽的复合模拟信号 【基带传输】$Define:$ 通过通道发送数字信号而不转换为模拟信号，即以信号本身的频谱进行传输 只能用带宽下限频率为 $0$ 的低通通道在基带传输中，所需的带宽与比特率成正比 $Eample\\ 1:$ 基带传输发送比特率 $n=1Mbps$，求低通通道所需带宽 最小带宽、使用第一谐波 $\\Longrightarrow B_1=\\frac{n}{2}=500KHz$ 使用第一、三谐波 $\\Longrightarrow B_3=3\\times B_1=1.5MHz$ 使用第一、三、五谐波 $\\Longrightarrow B_5=5\\times B_1=2.5MHz$ $Eample\\ 2:$ 一条带宽 $B=100kHz$ 的低通通道，求最大比特率 使用第一谐波得到最大比特率 $\\Longrightarrow n = 2\\times B=200kbps$ 【宽带传输】$Define:$ 宽带传输或调制把数字信号转换成模拟信号进行传输 调制允许使用带通通道，即带宽不从0开始的通道发送模拟信号用带通通道 2、典型的传输减损【衰减】$Define:$ 通过某种介质传输时，信号能量下降；分贝表示信号损失或增益强度 $dB=10\\log_{10}\\frac{P_2}{P_1}$ 其中，$P_1$、$P_2$ 表示信号前后的功率 【失真】$Define:$ 不同信号成分具有不同的传播速度导致信号波形失真 【噪声】$Define:$ 外界干扰，电缆中的电子随机移动而产生的额外信号 【信噪比】$Define:$ 信号功率与噪声功率的比率 $SNR = \\frac{P_1}{P_2}$ 其中，$P_1$、$P_2$ 表示平均信号功率和平均噪声功率 分贝单位$\\Longrightarrow SNR_dB = 10\\log_{10}SNR$ 3、数据速率限制【尼奎斯特定理】——无噪声通道 $S = 2B$ 注：有限带宽 $B$ 下，通道的极限波特率(码元速率)为 $2B$ 【奈奎斯特速率】——无噪声通道 $N = 2\\times B\\times log_2L$ 注：定义了给定带宽下理论上的最大比特率 【香农容量定理】——有噪声通道 $C = B\\times \\log_2(1+SNR)$ 其中通道容量是指通道的传输容量，即每秒的比特数等于比特率 注：确定噪声通道理论上的最高数据速率 当 $SNR$ 较大时，$SNR+1\\approx SNR$，则 $C = B\\times \\frac{SNR_{dB}}{3}$ $Eample:$一通道带宽 $B=1MHz$，信噪比 $SNR=63$，求合适的比特率以及信号电平解：由香农公式确定比特率上限： $$C = B\\log_2(1+SNR)=6Mbps$$ 为了获得更好的性能，取通道比特率 $N=4Mbps$由奈奎斯特公式计算信号电平数: $$N = 2B\\log_2L\\rightarrow L=4$$ 香农容量定理给出数据速率的上限，奈奎斯特公式给出所需的信号电平数 4、性能【带宽】——链路的潜在衡量值，链路最大的数据传输速率【吞吐量】——发送速度快慢的实际衡量值，小于带宽【延迟】——第一个位开始发出到整个报文完全到达目标所经历的时间 延迟=传播时间+传输时间+排队时间+处理延迟 传播时间——一个位从源传输到目标所需时间（与介质相关） $$Tp = \\frac{d}{v}$$ 传输时间——传输一个报文的时间 $$Tt = \\frac{LF}{B}$$ 其中，帧长 $LF$ 表示一个报文长度 排队时间——每个中间或端设备在处理报文前保持报文所需的时间 【带宽与延迟乘积】——定义了能充满链路的位数（通常用的是传播时延） 第4章 数字传输1、线路编码——将数字数据（比特位）转换为数字信号（高低电平）的过程【数据元素】表示一块信息的最小实体即位【信号元素】承载数据单元，是数字信号的最小单元$Define:$ 承载比率 $r$: 每个信号元素承载的数据元素的数量 $$ r = \\log_2L $$ 【数据速率（比特率）】表示1秒发送的数据元素（位）的数量（单位：bps）【信号速率（波特率）】表示1秒发送的信号元素的数量（单位：波特baud)比特率和波特率关系: $$S = c\\times N\\times \\frac{1}{r}\\ \\ baud$$ 其中，$N$ 表示比特率，$S$ 表示波特率，$c$ 表示情形因子(一般取 $0.5$)，$r$ 表示承载比率 波特率决定了数字信号的带宽，并且带宽与波特率成正比 【最小带宽】 $$B_{min}=c\\times N\\times \\frac{1}{r}$$ 最小带宽$B_{min}$与波特率相等（至少保证能够传输信号） 当通道带宽为 $B$ 时，得出最大数据速率： $B_{min}=c\\times N\\times\\frac{1}{r}\\leq B\\rightarrow N\\leq \\frac{1}{c}\\times B\\times r$ $\\Longrightarrow N_{max}=\\frac{1}{c}\\times B\\times r$ 注：$B_{min}$ 表示信号所需最小带宽，$B$ 表示通道带宽 【直流分量】——信号电平保持一段时间的恒定，使得频谱中对应的频率很低称为 DC（直流）分量【自同步】——接收方的位间隔必须与发送方的位间隔一致 数字信号在传输的数据中包含有定时信息：提示接收方起始、中间和结束位置的脉冲的跳变 2、线路编码方案不同编码方案平均带宽计算: $B_{min}=S = c\\times N\\times\\frac{1}{r}$ $c=0.5\\Longrightarrow B_{min} = \\frac{N}{2r}$ 单极性编码 极性编码方案(NRZ-L) 极性编码方案(NRZ-I) 归零码(RZ) 正电平定义成位 $1$ 而零电平定义成位 $0$，位中间信号不会回到 $0$ $r=1$，平均带宽 $B = \\frac{N}{2}$ 成本高，$0$ 或 $1$ 的长序列没有自同步，存在 $DC$ 问题 正电平定义成位 $0$ 而零电平定义成位 $1$，位中间信号不会回到 $0$，电平决定位值 $r=1$，平均带宽 $B = \\frac{N}{2}$ 全 $0$ 或全 $1$ 的长序列没有自同步，存在基线偏移，$DC$ 问题 信号电平是否反相转决定了位值，位中间信号不会回到 $0$ 对于本位，如果本位电平相比上位电平有反相，$bite = 1$，否则 $bite=0$ $r=1$，平均带宽 $B = \\frac{N}{2}$ 全 $0$ 的长序列没有自同步，存在基线偏移，$DC$ 问题 使用正值、负值和零三个信号电平 正电平定义成位 $1$，负电平定义成位 $0$，只是信号在位中间跳变回到 $0\\Rightarrow$ 解决同步问题 $r=\\frac{1}{2}$，平均带宽 $B = N$ 能够自同步，没有 $DC$ 问题，但是需要高带宽 双相编码——曼彻斯特编码 双相编码——差分曼彻斯特编码 双极性编码——AMI 双极性编码——伪三元编码 $RZ+NRZ-L$ 使用正值、负值两个电平，正电平定义成位 $0$，负电平定义成位 $1$，位中间跳变成另一电平 $\\Rightarrow$ 解决同步问题 $RZ+NRZ-I$ 使用正值、负值两个电平，正电平定义成位 $0$ 位中间跳变，如果下一位是 $1$，位开始处不跳变，否则进行跳变 $\\Rightarrow$ 解决同步问题 使用正值、负值和零三个电平，位 $0$ 始终由零电平表示，而位 $1$ 由交替正负电平表示 $r=1$，平均带宽 $B = \\frac{N}{2}$ 全 $0$ 的长序列没有自同步，无 $DC$ 问题 使用正值、负值和零三个电平，位 $1$ 始终由零电平表示，而位 $0$ 由交替正负电平表示 $r=1$ 3、块编码：4B/5B、8B/10B(了解)4、扰码【B8ZS】——8零置换的双极编码方案 $$B8ZS:00000000\\Rightarrow 000VB0VB$$ 其中，$V$ 表示与前一个非零脉冲极性相同的极性，$B$ 表示与前一个非零脉冲极性相反的极性 5、脉冲码调制PCM【采样】——方顶采样模拟信号采样周期$T_S$，采样频率$f_S=\\frac{1}{T_S}$根据奈奎斯定理，采样速率必须至少是信号所含最高频率的 $2$ 倍: $$f_S\\geq 2f_m$$ 【量化】 将最低振幅 $V_{min}$ 和最高振幅 $V_{max}$ 范围量化为 $L$ 个区间 区间高度：$\\Delta = \\frac{V_{max}-V_{min}}{L}$ 标准化 $PAM$ 值/振幅= 实际振幅/$\\Delta$ 标准化量化值=样本所在区间的中间量化值(单位为 $\\Delta$) 量化误差=标准化量化值-标准化振幅值，一般 $-\\frac{\\Delta}{2}\\leq$ 误差 $\\leq\\frac{\\Delta}{2}$ 量化码：$0\\thicksim7$ 编码对应 $-4\\Delta\\thicksim 4\\Delta$ 每一个区间 编码码字：$0\\thicksim7\\Longrightarrow 000\\thicksim111$ 【量化等级】——取决于模拟信号的振幅范围【量化误差】对信号 $SNR_{dB}$ 的影响： $$SNR_{dB}=6.02n_b+1.76dB$$ 其中，$n_b$ 表示每个样本的编码位数，$n_b=log_2L$ ($L$ 表示量化等级数即电平数) 【PCM数字化模拟信号的比特率和带宽】 $f_S=2f_m$ $\\Rightarrow N = f_S\\times n_b =2f_m\\times\\log_2L$ $B_{min}=c\\times N\\times\\frac{1}{r}$ $=c\\times f_S\\times n_b\\times \\frac{1}{r}$ $=c\\times 2\\times B_{analog}\\times n_b\\times \\frac{1}{r}$ $=n_b\\times B_{analog}$ 其中，$B_{analog}$ 表示低通模拟信号的带宽即信号最高频率 6、Delta调制 (DM)的概念【调制】 数字数据生成：将模拟信号的值与梯形信号的最后一个值比较，如果模拟信号的振幅大，则数字数据下一个位为 $1$，否则为 $0$;梯形信号生成：如果数字数据下一位为 $1$，则梯形信号的最后一个点上移 $\\delta$，否则下移 $\\delta$; 7、传输模式【并行传输】 使用 $n$ 个通信线路同时发送n位优点：速度快，用于短距离通信缺点：成本高 【串行传输】 包括异步和同步传输优点：成本低，适合远距离通信缺点：速度慢 【异步传输】 在传输中信号的时序并不重要信息的接受和转换通过约定的模式进行字节级异步：将位流组成字节(通常是 $8$ 位)的方式进行分组发送字节内同步：没有同步时钟，每个字节增加起始位和停止位，并且每个字节之间有一个时间间隔 【同步传输】 位流被组合成更长的帧，一帧包含多个字节，按序发送依次发送位流而不含起始位、停止位和间隙，接收方负责将位进行分组帧间可能有不等的时间间隔，常用于大块二进制数据传送 同步传输比异步传输速度快 第5章 模拟传输1、比特率和波特率$Define:$ $r = log_2L$ S=N\\times\\frac{1}{r} 其中，$L$ 表示不同类型的信号元素个数(数字传输中 $L$ 为电平个数) 2、数字到模拟转换的概念 数字数据转换为带通模拟信号 $\\Rightarrow$ 数字到模拟转换 低通模拟信号转换为带通模拟信号 $\\Rightarrow$ 模拟到模拟转换 【幅移键控（(B)ASK）】 通过改变载波信号的振幅来生成信号元素，只有振幅变化而频率和相位保持不变 【带宽】 $B=(1+d)\\times S$ $S = N\\times \\frac{1}{r}\\Rightarrow B = (1+d)\\times N\\times\\frac{1}{r}$ 反推 $N$ 【频移键控（(B)FSK）】 通过改变载波信号的频率来生成信号元素，只有频率变化而振幅和相位保持不变 【带宽】 $$B=(1+d)\\times S+2\\Delta f$$ 其中，$\\Delta f$ 表示两个信号元素的频率差 【相移键控（PSK）】 通过改变载波信号的相位来生成信号元素，只有相位变化而振幅和频率保持不变 BPSK：二进制PSK，只用两个信号元素，一个相位是$0$ 度，另一个相位是 $180$ 度 QPSK：正交PSK，使用两个独立的BPSK调制，一个是同向的，另一个是正交的 QPSK中相位可能是 $45$ 度，$-45$ 度，$135$ 度和 $-135$度之一，输出信号有 $4$ 种信号元素，每个信号元素可以承载 $2$ 位【带宽】 $B=(1+d)\\times S=(1+d)\\times N\\times\\frac{1}{r}$ $BPSK:\\ r=1\\Longrightarrow B = (1+d)\\times N$ $QPSK:\\ r=2\\Longrightarrow B = (1+d)\\times \\frac{N}{2}$ 【正交振幅调制（QAM）】 ASK+PSK:使用两个载波，一个同向而另一个正交，每个载波都用不同的振幅 【n-QAM带宽】 $$r = \\log_2n\\Rightarrow B = (1+d)\\times N\\times\\frac{1}{r}$$ 第6章 带宽利用1、复用：FDM、同步TDM、统计TDM和WDM概念【复用带宽要求】连接两台设备的介质带宽要比设备间传输所要求的带宽高【频分复用（FDM）】——用于模拟信号 链路带宽（以Hz为单位）大于要传输的信号的带宽之和 将链路带宽进行分频给每个信号通道，通道间存在未使用的带宽即防护频带 【波分复用（WDM）】 用于具有高数据速率传输能力的光缆，是合并多个光信号的模拟多路复用技术 【时分复用（TDM）】 TDM在时间上共享，每个连接占用链路的一个时间段，是组合多个低速的通道为一个高速通道数据的复用技术 【同步TDM】 每个输出单元的持续时间是输入单元的$n$ 分之一，其中 $n$ 表示连接数 链路速率是数据速率的$n$ 倍，并且比单元持续时间短 $n$ 倍 帧同步，每帧开始增加一位帧指示位，交替变换为 $0$ 和 $1$ 【同步TDM数据速率管理】各数据线数据速率不相等时处理技术： 处理策略 使用情况 特点 多级复用 一条输入数据线数据速率是其他一些输入数据线数据速率的整倍数 合并低数据速率通道，$m\\times n$，$m$表示连接数 多时隙复用 在一个帧中允许对一条输入分配多个时隙 分离高数据速率通道，$\\frac{n}{m}$，$m$表示分配的连接数 脉冲填充 输入线的比特率不是其他每个输入线比特率的整数倍 低速率的输入线添加虚位 【统计TDM】——动态分配时隙，提高链路利用率 输出时隙必须携带数据和目的地址(目的地址+数据) $N$ 条输出线编码表示： $$ n=\\log_2N $$ 无同步位，存在通道寻址因此不需要同步位 链路容量小于各通道之和 【脉冲调制PCM体制两大国际标准】 北美24路T1载波 欧洲30路E1载波 服务 $DS-1$ 线路 $T-1$ 速率：$1.544Mbps$ 用途：传输音/视频，也可用于模拟传输 语音通道：$24$ 个 时分多路复用技术(TDM) $DS-1$ 需要 $8kbps$ 开销 欧洲版 $T$ 线路 速率：$2.048Mbps$ 语音通道：$30$ 个 时分多路复用技术(TDM) 我国采用的是欧洲的E1标准 3、波分和码分的概念4、FHSS和DSSS的概念 第7章 传输介质1、有向介质：双绞线及其特点、同轴电缆和光纤的概念2、无线传输介质：无线波谱、无线电波、微波和红外波 第8章 交换（传输时延分析）1、电路交换网络——效率低、延迟最小电路交换网络是由物理链路连接的一组交换机组成的，每条链路划分成 $n$ 个通道，两个站点的连接是由一条或多条链路组成的专用路径来实现 电路交换在物理层资源预留：通信开始前，站点需要对通信所用的资源进行预留连续数据流：数据传输不打包数据传输期间无寻址 【建立阶段】——每条链路上预定一个通道，联合起来指定一条专用路径 建立阶段进行资源预留、端到端寻址 【网络延迟】 延迟时间=建立连接时间+数据传输时间+拆除电路时间 建立连接时间=请求信号传播+请求信号传输时间+确认信号传播时间+确认信号传输时间 数据传输时间=数据传播时间+数据传输时间 2、分组交换网 数据报网络虚电路网络 分组交换网中，不存在资源预留，资源按需分配 3、数据报网络——效率较高，延迟较长 每个分组独立处理，数据报交换在网络层不需要建立连接阶段和拆除阶段每个分组头部包含目的地址，且在分组传送期间保持不变（物理地址会变） 数据报网中的交换机使用基于目的地址的路由表 【网络延迟】 延迟时间=传输时间+传播时间+等待时间 $Example:$ 分组通过两个交换机传送分析： 总延迟时间 $=3\\times$ 传输时间 $+3\\times$ 传播时间 $+$ 在交换机 $1$ 处等待时间 $+$ 在交换机 $2$ 处等待时间 因特网在网络层用数据报方法对数据进行交换 4、虚电路网络—— 电路交换+数据报网络虚电路网络在数据链路层实现在数据传输阶段，存在建立阶段与拆除阶段按需在建立阶段期间分配资源每一分组头部含有目的地址，具有本地权限而非端到端权限（定义下一个交换机和分组的通道地址，虚电路标识符实现）所有分组沿着建立的路径进行传送 【虚电路标识符（VCI）】一个VCI仅是在交换机范围内的小数字，由两个交换机之间的帧来使用，通过交换机时更新VCI【虚电路网络表】 表项构成：输入端口，输入VCI，输出端口，输出VCI 【网络延迟】 延迟时间=传输时间+传播时间+建立阶段延迟+拆除阶段延迟 $Example:$ 分组通过两个交换机传送分析： 总延迟时间 $=3\\times$ 传输时间 $+3\\times$ 传播时间 $+$ 建立阶段延迟 $+$ 拆除阶段延迟 第三部分 数据链路层第10章 检错与纠错1、差错的类型【单个位差错】——数据单元仅有一位发生变化【突发性差错】——数据单元有两位或多位发生变化2、块编码 报文划分成块，每个块有k位——数据字数据字+r个冗余位形成长度n=k+r的块——码字 数据字: $2^k$ 码字: $2^n$ $\\Longrightarrow$ 非法码: $2^n-2^k\\ (n>k)$ 3、纠错方法【汉明距离】——两个相同长度的字的汉明距离是对应位不同的数量 $$10101\\oplus 11110=01011\\Rightarrow d(10101, 11110)=3$$ 【最小汉明距离】——一组字中所有可能对中的最小汉明距离 $d_{min} = \\min${两两间的汉明距离} 为了检测出所有情况下最多$s$个差错，块编码中最小汉明距离一定是$d_{min}=s+1$ 为了纠正所有情况下最多$t$个差错，块编码中最小汉明距离一定是$d_{min}=2t+1$ 4、线性块编码【线性块编码最小距离】——最小汉明距离 最小汉明距离是具有最少1的个数的非$0$ 有效码字中 $1$ 的个数 注：每个非零码字中，具有最少$1$ 的码字对应 $1$ 的个数 【简单奇偶校检码】 校检编码$n=k+1、d_{min}=2$，单个位或奇数个位检错编码，不能纠错 【二维奇偶校检码】——最多能检测 $3$ 个差错【汉明编码】 $n=2^m-1,\\ \\ k=n-m$ $\\Longrightarrow$ 校检位个数: $r=m$ $d_{min}=3\\Rightarrow$ 校检 $2$ 位，纠正 $1$ 位 编码分析 $a_3a_2a_1a_0r_2r_1r_0\\Longrightarrow b_3b_2b_1b_0q_2q_1q_0$ 奇偶校检位: $r_0=a_2+a_1+a_0$ $r_1=a_3+a_2+a_1$ $r_2=a_3+a_1+a_0$ 奇偶校检方程: $s_0=b_2+b_1+b_0+q_0$ $s_1=b_3+b_2+b_1+q_1$ $s_2=b_3+b_1+b_0+q_2$ 校正子 $s_2s_1s_0$ 出错位 6 110 $b_3$ 3 011 $b_2$ 7 111 $b_1$ 5 101 $b_0$ 5、循环冗余编码校检码（CRC码） 数据字：$k$位，码字：$n$位，校检位：$n-k$位，除数：$n-k+1$位，余数：$n-k$位 编码分析 余数=(数据字+($n-k$)位 $0$) $\\%$ 除数 (模 $2$ 除法) $\\Longrightarrow$ 校检位=余数 $\\Longrightarrow$ 码字=数据字+校检位 校检： 校正子=码字$\\%$除数 if 校正子== $0$: 接受数据字 else 丢弃数据字 检错类型 （除数）生成多项式$g(x)$ 捕捉单个位差错 $g(x)$至少有两项并且$x^0$的系数为1 两个独立的单个位差错 $g(x)$不能整除$x^t+1(t\\in [0,n-1], t\\in Z)$ 奇数个差错 $g(x)$有因式$x+1$ 突发性差错 所有$L\\leq r$的差错均可被检测;$L=r+1$的差错被检测概率$P=1-{\\frac{1}{2}}^{r-1}$;$L&gt;r+1$的差错被检测概率$P=1-{\\frac{1}{2}}^r$;（其中$L$表示差错长度，$r$表示冗余位长度 6、校检和及其计算方法 需要将数据划分为多组的16bits数据 第11章 数据链路控制1、组帧 在数据链路层，成帧将一条从源端到目的端的报文分离开来，或者将到不同目的端的报文分离开来 【固定长度成帧】——不需要定义帧的边界，长度本身可以用作分隔符【可变长度成帧】 面向字符协议——帧的开始和结尾加一个字节的标记帧格式：标记+头部+可变数量的字符（来自上层的数据）+尾部+标记转义字节：$01111101$【PPP协议】 标记冲突解决：字节填充，在字符标记的前面添加一个字节（换义字符ESC）以跳过该字符标记 面向位协议帧格式：标记+头部+可变数量的位（来自上层的数据）+尾部+标记特定的标记：$01111110$【HDLC协议】 标记冲突解决：位填充，遇到一个$0$ 后面紧跟 $5$ 个 $1$ 便添加一个 $0$ 数据链路控制：流量控制+差错控制 2、流量控制与差错控制【流量控制】——一系列程序，用来限制发送方在等到确认之前发送的数据数量【差错控制】——基于自动重复请求（ARQ）进行差错检测和重传【ARQ】ARQ技术就是自动请求重发技术，结合了流控和自动重发技术，该技术的主要思想是利用差错检测技术自动对丢失的帧和错误帧请求重发 3、停止等待ARQ——有噪声通道 差错检测由保留已发送帧的副本并当计时器到时重传这个帧来实现使用序列号给帧编号，该序列号基于模 $2$ 运算确认号总是以模2运算宣布期待收到下一帧的编号 【链路利用率】 带宽 $B$，一个位往返时间：$t$，数据帧的长度：$L$ $\\Longrightarrow$ 带宽延迟乘积=$B\\times t$ $\\Longrightarrow$ 链路利用率 = $L$/带宽延迟乘积 4、后退N帧ARQ——有噪声通道 序列号是模 $2^m$，$m$ 表示以位为单位的序列号字段长度一个ACK可以确认一个以上的帧 【协议原理】 后退 $N$ 帧ARQ协议就是从出错处重发已发过的 $N$ 个帧； 在后退 $N$ 帧协议中，发送方按照窗口中帧的编号顺序连续发送帧，接收方窗口大小总是 $1$； 接收方只能按顺序接受，并按顺序发送应答信号； 对于没有按顺序达到的帧丢弃，以后要重新发送 发送方窗口大小$2^m-1$ (一定要小于 $2^m$)，接收方窗口大小为 $1$ 5、选择重发ARQ——有噪声通道 发送方窗口大小 $\\leq2^{m-1}$，接收方窗口大小与发送方一致 6、高级数据链路控制（HDLC） 面向比特的点到点和多点链路进行通信的协议 【正常响应方式】【异步平衡方式】【定义帧】 信息帧 管理帧 无编号帧 7、点到点（PPP）协议——面向字节的方式 第12章 多路访问1、随机访问【纯ALOHA】——每个站点有帧要发送，就发送帧 脆弱时间: $2\\times T_{fr}$ 吞吐量: $S = G\\times e^{-2G}$当 $G=\\frac{1}{2}$ 时，则 $S_{max}=0.184$其中，$G$ 表示一个帧传输时间内系统产生的帧的平均数量 【时隙ALOHA】——强制站点只有在时隙开始之时才能进行发送 脆弱时间: $T_{fr}$ 吞吐量: $S = G\\times e^{-G}$当 $G=1$ 时，则 $S_{max}=0.184$其中，$G$ 表示一个帧传输时间内系统产生的帧的平均数量 【载波侦听多路访问CSMA】——传输前先侦听 脆弱时间=传播时间: $T_{p}$ 吞吐量: $S = G\\times e^{-G}$当 $G=1$ 时，则 $S_{max}=0.184$其中，$G$ 表示一个帧传输时间内系统产生的帧的平均数量 持续方法：1-持续、非持续、p-持续 【带冲突检测的载波侦听多路访问CSMA/CD】——边发边听$Example:$ CMSA/CD网络，带宽 $B=10Mbps$，最大传播时间 $T_{p}=25.6\\mu s$，求帧的最小长度解：帧的传输时间 $T_{fr}=2\\times T_{p}=51.2\\mu s\\ (T_{fr}\\geq T_{p})$帧的最小长度$=B\\times T_{fr}=B\\times 2T_{p}=512\\ bits$ 【带冲突避免的载波侦听多路访问CSMA/CA】 三种方法：帧间间隔、竞争窗口、确认 2、受控访问 预约 轮询 令牌环 3、通道化：FDMA、TDMA、CDMA 第13章 有线局域网：以太网1、IEEE标准 数据链路层划分为两个子层：逻辑链路控制层（LLC）和介质访问控制层（MAC） 2、标准以太网的MAC子层【帧格式】——最小长度 $64$ 字节【MAC地址】 源地址一定是单播地址$6$ 字节 = $12$ 十六进制数字 = $48$位 第一个字节的最低位为0则地址是单播地址否则是多播地址 广播地址是多播地址的一个特例，所有位（48位）都是1 【编码和解码】 标准以太网中实现的编码是曼彻斯特编码 IEEE 802.5中实现的编码是差分曼彻斯特编码 【$10$Base$5$】 传输速度 $10Mbps$，基带传输，传输最大距离 $500$ 米粗缆以太网半双工 【$10$Base$2$】 传输速度 $10Mbps$，基带传输，传输最大距离 $200$ 米细缆以太网半双工 【$10$Base-T】 传输速度 $10Mbps$，基带传输，双绞线最大长度：$100$米双绞线以太网全双工 【$10$Base-F】 传输速度 $10Mbps$，基带传输，光纤最大长度：$2000$ 米光纤以太网全双工 【桥接以太网】 存在网桥——作用：提高带宽和分割冲突域 【交换式以太网】 存在交换机 【全双工以太网】 站点和交换机之间使用两条链路 3、快速以太网、千兆以太网 第14章 无线局域网1、IEEE $802.11$ 标准——涵盖物理层和数据链路层【基本服务集（BSS）】 服务点AP：中央基站 不带AP的BSS称为特别网络，带AP的BSS称为基础网络 【扩展服务集（ESS）】 由两个或多个带有AP的BSS组成各个BSS通过一个分布式系统（通常是有线局域网）连接BSS中的AP而组合起来 2、$802.11$ MAC子层（DCF和PCF）【分布式协调功能DCF】 访问方式：CSMA/CA需要竞争使用信道 【点协调功能PCF】 集中式的、无竞争的轮询访问方式PCF的优先级高于DCF 【帧格式】 帧控制（FC）——$2$ 字节 D——$2$ 字节 地址——有四个地址字段，每个 $6$ 字节 序列控制——$2$ 字节，用于流量控制 帧主体——$0\\sim2312$ 字节 FCS——$4$字节，CRC-$32$的差错检测序列 【帧类型】IEEE $802.11$ 定义帧类型：管理帧、控制帧和数据帧3、隐藏站点和暴露站点【隐藏站点】——B和C对于A来说是相互隐藏的 CSMA/CA握手中的CTS帧可以避免来自隐藏站点的冲突 【暴露站点】——站点被限制使用通道，而这个通道是可用的 第15章 连接局域网、主干网和虚拟局域网1、连接设备【无源集线器】——工作在物理层以下 集线器是冲突点 【中继器】——工作在物理层 中继器连接一个局域网的各个网段，因此不能连接两个不同协议的局域网中继器转发每一帧，没有过滤能力是再生器，重新生成信号，不是放大器 【有源集线器（hub）】——工作在物理层 所有的端口是一个广播域，也是一个冲突域 【网桥】——工作在物理层和数据链路层 所有的端口是一个广播域，每个端口是一个冲突域（需要竞争使用）网桥具有过滤能力，有一个过滤决策的表可靠性高：网络故障只影响其所在的网段不改变帧中所包含的物理地址连接局域网 【透明网桥】 是一个它所连接的站点完全意识不到其存在的网桥 【生成树】 从网桥到LAN的跳数为 $1$，而从LAN到网桥的跳数为 $0$ 选择 $ID$ 最小的网桥作为根网桥 找到从根网桥到其他每个网桥或LAN的最短路径 最短路径的组合生成了最短的树 转发端口：属于生成树部分的端口阻塞端口：不属于生成树部分的端口 【两层交换机】——工作在物理层和数据链路层 没有通信量的竞争对帧的MAC地址具有过滤决策所有端口是一个广播域，每个端口是一个冲突域 【路由器】——工作在物理层、数据链路层和网络层 在硬件上实现了路由器的分组转发逻辑基于分组的逻辑地址进行路由转发每个端口是一个广播域也是一个冲突域 【三层交换机】——工作在物理层、数据链路层和网络层 每个端口是一个广播域也是一个冲突域 【网关】——工作在所有五层或OSI模型全部七层 不仅仅包含路由、交换功能，还包括应用级的功能 2、虚拟局域网 虚拟局域网: 通过软件而非物理线路来配置的局域网 【站点分组的依据】——端口号、MAC地址、IP地址、多播IP地址、联合使用 第18章 虚电路网络：帧中继和ATM1、【帧中继特点】——虚电路广域网 速率较高 允许突发性数据 花费少 工作在物理层和数据链路层 仅在数据链路层有错误检测，没有流量和错误控制 帧中继不提供流量和差错控制，这些必须由上层协议提供 帧中继提供拥塞控制和服务质量 【帧中继的结构】 帧中继是一种虚电路网络，虚电路用数据链路连接标识符（DLCI）定义 帧中继中的VCI（虚电路标识符）称为DLCI 2、异步传输模式（ATM）——信元交换网路 信元中继协议传输的数据单元：信元（长度不变）使用异步时分复用技术处理来自不同通道的信元具有先进的拥塞控制和服务质量需要建立虚电路进行通信 信元网络使用信元作为数据交换的基本单位，信元定义为一个小的、固定大小的信息块($53$B=$5$B头部+$48$B数据) 【传输路径（TP）、虚路径（VP）和虚电路（VC）】 传输路径是一个端点与一个交换机或者两个交换机之间的物理连接 虚路径提供两个交换机之间的一条连接或连接的集合 虚电路是属于同一报文的所有信元沿着同一虚电路传输，同时保持原始次序 【虚路径标识符（VPI）、虚电路标识符（VCI）】 VPI定义特定的VP，VCI定义特定的VC 一个虚连接由一对数字定义：VPI、VCI，其包含在信元的头部 第四部分 网络层第19章 逻辑寻址1、IPv4地址分类，掩码，子网和超网的概念 类别 可指派网络号范围 可指派的网络数 可分配的IP地址数 A 0000 0001— 1111 1110 2^7 -2 2^24 -2 B 128.0—191.255 2^24 2^16 -2 C 192.0.0—223.255.255 2^21 2^8-2 D 多播地址 E 保留 【子网掩码】 将IP地址的主机号部分借用几位作为子网号以区分同一网络下的不同网络(子网) 【超网】 超网(CIDR)消除了传统的A类、B类和C类地址以及划分子网的概念，将网络前缀相同的IP地址组成一个地址块 记法：IP地址 + “/” （斜线后写上网络前缀所占的比特数量） 2、子网划分方法、子网范围计算 子网掩码用连续 $1$ 表示对应的网络号和子网号；连续 $0$ 表示主机号 将IPv4地址与子网掩码“相与”，即可得到所在子网的网络地址 由子网掩码说明只借用了主机号一位，所以共划分出两个子网，每个子网可分配的地址数为： $$2^7-2$$ 3、NAT 为了解决地址短缺，内部可有大量地址，对外通信采用的一个外部格式；能使得大量使用内部专用地址的用户共享外部的全球地址，以访问因特网主机资源 4、IPv6地址及其缩短形式 第20章 IP协议1、IP数据报的格式 固定首部+数据载荷 2、分片与MTU 【MTU】 最大传送单元 : MTU 是链路层可封装数据 的上限 【分片】 更多操作 IP 分组的 MTU 是 1500 字节 , 当网络层的 IP 分组超过 1500 字节 , 此时就要进行分片，将数据报分割，使其能通过不同的网络 根据首部的标识 , 标志 , 片偏移（必须为整数） 进行相应处理 3、IPv4校验和 更多操作将校验和置$0$，以 $16$bit为一组求校验和 4、IPv6分组格式 基本头部固定为$40$B，有效载荷 $Max=65536$B 5、IPv4和IPv6混合 隧道技术：两台使用IPv6电脑的报文通过IPv4区域时，IPv6分组封装成IPv4的技术 头部转换：发送方用IPv6，接收方用IPv4，则IPv6头部应该转换为IPv4 第21章 地址映射、差错报告和多播1、APR 【概念】 逻辑地址映射到物理地址， ARP （Address Resolution Protocol） 【过程】 2、DHCP的概念 可以提供静态/动态/自动/人工的地址配置 3、ICMP 【概念作用】【差错报告】ICMP差错报文总是将报文传递给源方 4、IGMP IGMP是组管理协议，它为多播路由器提供有关连接到该网络上成员的相关信息 第22章 传递、转发和路由协议1、转发技术与转发过程 转发是指将分组路由到它的目的端 下一跳方法：路由表只保留下一跳地址 路由方法：保留完整路由信息 特定主机/特定网络方法 2、路由表、地址聚合和最长掩码匹配3、单播路由协议 【RIP】：以距离向量衡量，用于应用层 【OSPF】：以链路状态衡量，用于传输层 【BGP】：以路径向量衡量，用于应用层 4、多播的概念和多播路由协议（了解） 第五部分 传输层第23章 传输层三种协议：UDP、TCP和SCTP1、端口（重要端口）、套接字 层结构 传输地址 数据链路层 MAC地址 网络层 IP地址 传输层 端口地址 端口号：$16$bits$\\Rightarrow 0\\thicksim 65535$端口划分: 熟知端口: $0\\thicksim 1023$ 注册端口: $1024\\thicksim 49151$(防止端口被重用) 动态端口(临时端口): $49152\\thicksim 65535$(可随意使用) 套接字地址：IP地址+端口号 2、UDP协议 用户数据报协议（UDP）无连接服务、不可靠、无流量控制和差错控制 【数据报结构】 UDP分组称为用户数据报，有8字节的固定头部固定头部：源端口号、目的端口号、长度（定义数据报总长度）、校检和UDP被封装成IP数据报：伪头部是IP分组的头部一部分，而头部是UDP的一部分数据（必须进行填充是数据是16位的倍数） UDP长度=IP长度-IP头部长度 【校检和的计算】——针对UDP的头部加上去除填充的数据 每两个字节模2求和，最后结果取反得到校检和 3、TCP协议 传输控制协议（TCP）全双工、面向连接、可靠的、提供流量控制和差错控制建立虚拟连接，逻辑上的连接TCP允许发送进程以字节流的形式传递数据，并设置有发送和接受缓冲区TCP将多个字节组合在一起成为一个分组，称为段段的头部包含有序号、确认号两个字段（序号指字节序号开始于一个随机数编号而非段序号）段序号指该段的第一个字节的序号，段中确认字段定义了接收方预期接受的下一字节的编号 【连接建立（三次握手）】——SYN表示序列同步号 客户发送第一个段SYN段，用于序列号同步，不携带数据但占用一个序列号 服务器发送第二个段，SYN+ACK段，SYN表示另一方向通信的SYN段（全双工），ACK表示对第一个SYN的确认，不携带数据但占用一个序列号 客户发送第三个段，ACK段进行确认，不携带数据也不占用序列号 【终止连接（三次握手）】——将SYN换做FIN【流量控制】——信贷滑窗协议 面向字节的、窗口大小可变窗口的张开、合拢和收缩由接收方控制 窗口大小=$\\min(rwnd,cwnd)$ 【差错控制】——校检和、确认和重传 ACK段不占用序列号，不需要确认 超时重传：重传计时器到时，快速重传：发送方接收到三个重复的ACK 数据可以失序到达，并被接受的TCP暂时存储，但是TCP确保传递给进程的段是无失序的 第24章 拥塞控制和服务质量1、TCP拥塞控制【慢启动过程】 每次接收到一个确认时，窗口大小增加一个MSS值以 $2$ 的幂次增加，单位是MSS值 在慢启动算法中，拥塞窗口大小按指数规律增长直到达到阈值 【拥塞避免：加性增加】 当拥塞窗口达到慢速启动的阈值时，慢速启动阶段停止，加性增加阶段开始每次整个窗口所有段都被确认时，拥塞窗口才增加 $1$ 拥塞避免算法中，拥塞窗口大小加性增加直到检测到拥塞 【拥塞检测：乘性减少】 发生拥塞时，拥塞窗口阈值下降一半 如果计时器到时，开始一个新的慢速启动阶段如果检测出三个ACK，开始一个新的拥塞避免阶段 2、服务质量【概念】数据流所追求的某种目标 第六部分 应用层第25章 域名系统1、域名空间DNS【域名树】——最多有 $128$ 级，$0$ 级根节点，节点标号为字符串，根节点标号为空串 从下向上读，最后一个字符为 $’.’$全称域名：以空字符串结束，最后一个字符是 $’.’$部分域名：起始于一个节点，但未到根节点 2、域名空间三个部分：反向域、通用域和国家域【通用域】按照已经注册的主机的一般行为对主机进行定义 com edu gov info int mil org pro 【国家域】 cn us 【反向域】用于将地址映射为名字3、域名解析【递归解析】将客户机的请求不断传递给父服务器并等待响应，查询得到解析后响应后向传送直到最终到达发出请求的客户机【迭代解析】服务器或者发送应答，或者返回另一个服务器的IP地址，由客户机负责再次发送请求4、DNS报文【查询报文】——头部+查询记录【响应报文】——头部+查询记录+响应记录+授权部分+附加部分5、URL访问Web的主机统一命名：协议：//主机：//端口：//路径 Contributors Zhihao Li Jiawei Hu Changrong You References Behrouz, a.forouzan. (n.d.). 数据通信与网络 (原书第 4 版)","link":"/collaboration/ComputerNetwork/"},{"title":"数据库系统概论","text":"第一讲 数据库系统概述数据库基本概念 数据库(DataBase,DB) 概念: 长期存储在计算机内、有组织、可共享的数据集合 特点: 永久存储、有组织、可共享 数据库管理系统(DataBaseManagementSystem,DBMS) 概念: 专门用于管理数据库的软件 组成: 相互关联的数据集合、访问数据的程序 数据库系统(DataBaseSystem,DBS) 概念: 引入数据库之后的计算机系统 特点: DBS = DB + OS + DBMS + App + DBA +Users 数据库发展阶段 人工管理阶段 数据不保存 用户/应用程序管理数据 数据不共享，不独立 数据无结构 文件系统阶段 数据可以长期保存 文件系统管理数据 数据共享性差，冗余度大 物理独立性好，逻辑独立性差 记录内有结构，整体无结构 数据库系统阶段 数据可以永久保存 数据由DBMS管理 数据共享性高，冗余度小 具有高度的物理独立性，较好的逻辑独立性 统一数据模型，整体结构化 数据库管理系统采用外模式-模式-内模式三级模式，外模式/模式和模式/内模式两级映像结构来实现的。 数据模型 是数据及其联系在计算机中的表示和组织形式的描述 组成三要素: 数据结构、数据操纵、数据完整性约束 数据库模型 概念模型 E-R图 逻辑模型 层次模型 树状结构: 每个节点是基本单位称为记录，记录之间的联系以树形结构存储 特点: 只能处理一对多联系，无法处理多对多联系 网状模型 网状结构(有向图): 记录之间的联系用连线表达，联系必须标注名称 特点: 将多对多联系转换为多个一对多联系 关系模型 实体和联系都作为数据文件存储 物理模型 数据库系统结构 数据逻辑独立性：由外模式/模式映像保证（当模式改变，仅修改映像，即可保证外模式不变） 数据物理独立性：由模式/内模式映像保证（当内模式改变，仅修改映像，即可保证模式不变） 第二讲 关系模型关系数据结构关系模式: $R(U, D, Dom(), F)$ (简称: $R(U)$)其中，$R$ 表示关系名，$U$ 表示属性集，$D$ 表示关系的域，$Dom$ 表示属性到域上的映射关系，$F$ 表示数据依赖 关系代数 关系代数(Relational Algebra)是过程化的查询语言，是以集合为基础的运算表达式 传统集合运算 并(Union): From the row angle $R \\cup S$={$t|t\\in R\\vee t\\in S$} 差(Difference): From the row angle $R - S$={$t|t\\in R\\wedge t\\notin S$} 交(Intersection): From the row angle $R \\cap S$ = {$t|t\\in R\\wedge t\\in S$}=$R-(R-S)$ 广义笛卡尔积(Cartesian Product): From the row angle $R \\times S$ = {$\\widehat{t_rt_s}|t_r\\in R\\wedge t_s\\in S$} 注: R:$(k_1, n)$, S:$(k_2,m)\\Longrightarrow$R$\\times$S:$(k_1+k_2, n+m)$ 专门关系运算 选择(Selection): From the row angle $\\sigma_F(R)$ ={$t|t\\in R\\wedge F(t)=True$} 投影(Projection): From the column angle $\\pi_A(R)$ = {$t[A]|t\\in R$} 注: 选择出原关系中某些属性列，为避免重复，还可能会取消某些元组 连接(Join): From the cross angle $R\\underset{A\\theta B}{\\bowtie} S$ = {$t_r\\cup t_s|t_r\\in R\\wedge t_s\\in S\\wedge t_r[A]\\theta t_s[B]$} Solution Steps For $\\theta$ Join: Step 1: 确定结果中的属性列 Step 2: 确定参与比较的属性列 Step 3: 逐一取R中的元组分别和S中与其符合条件的元组进行拼接 等值连接(Equi-Join): $\\theta$ is “=” $R\\underset{A=B}{\\bowtie} S$ = {$t_r\\cup t_s|t_r\\in R\\wedge t_s\\in S\\wedge t_r[A]=t_s[B]$} 自然连接(Natural Join): $\\theta$ is “=” and $As = Bs$ which combines As and Bs columns avoiding repeated attributes(As, Bs means a column or multiple columns) $R\\bowtie S$ = {$t_r\\cup t_s - t_s[B]|t_r\\in R\\wedge t_s\\in S\\wedge t_r[B]=t_s[B]$} Practices Used Tables S Table = S(Sno, Sname, Ssex, Sage, Sdept) Sno Sname Ssex Sage Sdept 95001 李勇 男 20 CS 95002 刘晨 女 18 IS 95003 王敏 女 18 MA 95004 张立 男 19 IS SC Table = SC(Sno, Cno, Grade) Sno Cno Grade 95001 c1 92 95001 c2 65 95001 c4 88 95002 c2 90 95002 c5 73 SC$\\times$SC Table SC1.Sno SC1.Cno SC1.Grade SC2.Sno SC2.Cno SC2.Grade Problems 查询选修了 $C_2$ 和 $C_4$ 课程的学生学号 $$pi_1(\\sigma_{1=4\\wedge 2=’c2’\\wedge 5=’c4’}(SC\\times SC))$$ 查询不学 $C_2$ 课程的学生学号 $$pi_{sno}(S)-\\pi_{cno}(\\sigma_{cno=’c2’}(SC))$$ 关系模型由关系数据结构、关系操作集合和关系完整性约束组成 关系数据结构：单一的结构类型即关系，表示现实世界的实体以及实体间的联系 关系操作集合：查询、插入、删除、修改操作 关系完整性约束：实体完整性、参照完整性、用户定义完整性约束 关系数据库语言的共同特点：非过程化的集合操作语言 关系数据语言：关系代数语言、关系演算语言、SQL 第三讲 数据库完整性 数据库完整性包括实体完整性、参照完整性和用户定义完整性。 实体完整性 CREATE TABLE 中用 PRIMARY KEY 定义关系模型的实体完整性 单属性构成的码： 定义为列级约束条件/定义为表级约束条件 多个属性构成的码： 定义为表级约束条件 References BitHachi's Blog 第三讲 SQL概述 SQL概述及数据定义——BitHachi's Blog SQL之数据查询——BitHachi's Blog SQL之基本表更新——BitHachi's Blog 第六讲 关系数据理论之规范化存在的问题关系模式中属性间存在某些依赖关系导致插入异常、删除异常、更新异常以及数据冗余的问题 数据依赖定义: 关系属性与属性之间的一种约束关系，即两个列或列组之间的约束，主要包含函数依赖与多值依赖。 函数依赖 (Functional Dependency, FD)定义: 对于任意关系 $r\\in R(U)$, $r$ 中不可能存在两个元组在 $X$ 上的属性值相等，而在 $Y$ 上的属性值不等。 $X\\rightarrow Y$: $X$ 函数确定 $Y$ 或 $Y$ 函数依赖于 $X$ Notes 函数依赖指 $R$ 的所有关系实例均要满足的约束条件 函数依赖属于语义范畴概念，只能根据数据的语义来确定函数依赖 特殊函数依赖 非平凡的函数依赖：$X\\rightarrow Y$ 且 $Y\\nsubseteq X$ 平凡的函数依赖：$X\\rightarrow Y$ 且 $Y\\subseteq X$ 相互决定: $X\\rightarrow Y$ 且 $Y\\rightarrow X$, denotes $X\\leftrightarrow Y$ $Y$ 不函数依赖于 $X$: $X \\nrightarrow Y$ 完全函数依赖：$X\\rightarrow Y$ 且 $ \\forall X’ \\subset X, X’ \\nrightarrow Y$, denotes $X\\mathop{\\longrightarrow}\\limits^F Y$ 部分函数依赖：$X\\rightarrow Y$ 且 $Y$ 不完全函数依赖于 $X$, denotes $X\\mathop{\\longrightarrow}\\limits^P Y$ 传递函数依赖：$X\\rightarrow Y, Y\\rightarrow Z$ with conditions $Y\\nsubseteq X, Y\\nrightarrow X$，则 $X\\rightarrow Z$, denotes $X\\mathop{\\longrightarrow}\\limits^T Y$ 如果 $Y\\rightarrow X$ 即 $X\\leftrightarrow Y$，则 $Z$ 直接依赖于 $X$ 如果 $Y\\subseteq X$, 则 $X\\mathop{\\longrightarrow}\\limits^P Z$ 候选码(Candidate Key) For $K$ in $R&lt;U, F&gt;$, satisfy $K\\mathop{\\longrightarrow}\\limits^F U$ 主码(Primary Key) 为选定的一个候选码 性质 决定性：$K\\rightarrow U$ 最小性: $\\nexists K’\\subset K$ let $K’\\rightarrow U$ 主属性(Prime Attribute): 所有候选码中出现的属性 非主属性(Nonprime Attribute): 不出现在任何候选码中的属性 全码(All Key): 由关系模式的所有属性构成码 外码(Foreign Key): $X$ 并非是 $R$ 的码，而是另外一个关系模式的码 规范化规范化设计关系表的规范化设计就是要尽可能地减少关系表中列或者列组之间的依赖关系，即函数依赖 范式(Normal Form, NF) Defination 1: 表示关系表的规范程度状态 Defination 2: 表示符合某一种级别的关系模式的集合 第一范式(First Normal Form, 1NF) Defination: 关系模式 $R$ 的所有属性都是不可分的基本数据项，denotes $R\\in 1NF$ 不满足第一范式的数据库模式不是关系数据库 第二范式(Second Normal Form, 2NF) Defination: $R\\in 1NF$ 并且每一个非主属性都完全函数依赖于 $R$ 的任一候选码, denotes $R\\in 2NF$ Notes 不存在非主属性对码的部分依赖 不属于 $2NF$ 关系模式问题：插入异常、删除异常、数据冗余大、修改异常 第三范式(Third Normal Form, 3NF) Defination: $R&lt;U, F&gt;$ 中不存在码 $X$、属性组 $Y$ 及非主属性 $Z(Z\\nsubseteq Y)$ 使得 $X\\rightarrow Y(Y\\nrightarrow X), Y\\rightarrow Z$, denotes $R\\in 3NF$ Notes If $Z\\subseteq Y$, then when $X\\rightarrow Y$, get $X\\rightarrow Z$ 不存在非主属性对码的传递依赖 不属于 $3NF$ 关系模式问题：插入异常、删除异常、数据冗余大、修改异常 修正第三范式(Boyce Codd Normal Form, BCNF) Defination: $R\\in 1NF$, for any $X\\rightarrow Y(Y\\nsubseteq X)$ and $X$ 必包含码, denotes $R\\in BCNF$ Notes 每一个函数依赖的决定因素都包含码 范式定理 If $R\\in 3NF$, then $R\\in 2NF$ Handwritten Notes Contributors Zhihao Li Changrong You","link":"/collaboration/DataBase/"},{"title":"自主可控嵌入式系统设计","text":"第一讲 嵌入式系统概述嵌入式系统三要素 嵌入性：嵌入到对象体系中，有对象环境要求 专用性：软、硬件按对象要求裁剪 计算机：实现对象的智能化功能 嵌入式系统定义 标准定义：以应用为中心、以计算机技术为基础，软、硬件可裁剪，适应应用系统对功能、可靠性、成本、体积、功耗等严格要求的专用计算机系统。 核心要义：嵌入到对象体中的专用计算机系统 嵌入式系统的核心：计算技术；嵌入式系统的灵魂：软件技术 嵌入式系统的分类 嵌入式处理器的分类嵌入式系统硬件的核心部件是嵌入式处理器，按照用途可以为分： 嵌入式微控制器 (Micro Controller Unit, MCU): 单片机 嵌入式 DSP (Digital Signal Processor, DSP) 嵌入式微处理器 (Micro Processor Unit, MPU): ARM SOC(System on a Chip, SOC) SOPC(System on a Programmable Chip, SOPC) 嵌入式操作系统的分类操作系统：连接计算机硬件与应用程序的系统程序 非实时操作系统: Linux 实时操作系统: RTOS (Real-Time Operating System, 程序对时间要求十分严格: 计算的正确性不仅取决于程序的逻辑正确性，更取决于结果产生的时间) 第二讲 嵌入式系统基础体系结构-存储器组织冯诺依曼体系结构 冯诺依曼体系理论 计算机的数制采用二进制 计算机按照程序顺序执行 要点 程序存储、程序执行 输入、存储、运算、控制和输出 程序指令存储器和数据存储器共用同一存储器，统一编址；程序指令和数据的宽度相同 哈佛体系结构 程序指令存储器和数据存储器使用两个独立的存储器，实现并行处理； 使用独立的两条总线，用于CPU与两个存储器进行通信 基本概念 CPU 字长: 微处理器一次执行处理的数据宽度 指令集: CPU 所能执行的所有指令的集合 复杂指令集(CISC, Complex Instruction Set Computer) 具有大量指令和寻址方式 大多数程序只使用少量的指令就能够运行 精简指令集(RISC, Reduced Instruction Set Computer) 只包含最有用的指令 数据通道快速执行每一条指令 特点对比 RISC指令格式和长度固定，指令类型少，功能简单，寻址方式少，采用规则的硬布线逻辑(组合逻辑型) CISC使用微码ROM进行指令译码(存储逻辑型) RICS大多数指令单周期运行，分开的Load/Store结构的存取指令，固定指令格式 CISC强调硬件的复杂性，RISC注重编译器的复杂性 数据通道对比(RISC的任何运算只能在REG上执行，ALU的数据来源只能是REG，提高执行效率，对比CISC有数量级上的改善) 多任务程序结构前后台程序结构 前台 中断实现 处理对时间要求严格事件、突发事件 后台 轮询多任务实现 处理对时间要求不严格事件 注：前后台是两种服务模式，前台对接时间严格事件，通过中断方式进行响应；后台对接时间不严格事件，分配时间片轮询处理每个任务 事件触发结构 状态机：实际上是有限状态机，类比马尔可夫决策 事件查询：(在状态中判断事件)主体是某一特定状态，决策是采取的不同事件，结果是执行不同功能，迁移到新状态 事件触发：(在事件中判断状态)主体是某一特定事件，决策是执行的不同动作，依据是当前的状态，结果是执行不同功能，迁移到新状态 操作系统 实现多任务运行，进程调度 第三讲 ARM 基本编程模型 ARM=Advanced RISC Machines ARM处理器特点 Load/Store 体系结构 16位/32位双指令集 3地址指令格式 Thumb 技术开发背景 RISC代码密度低，需要比较大的存储器空间(RISC指令简单，完成同一个功能需要多条指令，因此需要更大的存储器空间) 需要32位RISC处理器的性能和16位CISC处理器的代码密度(32位RISC的性能, 16位CISC的代码密度)Thumb 概述 技术概述 16位的指令长度 32位的执行效率 拥有2套独立的指令集 ARM处理器工作状态ARM状态(ARM指令集) 执行32位的字对齐的ARM指令 Thumb状态(Thumb指令集) 执行16位的、半字对齐的Thumb指令 状态切换 两个指令集均有切换处理器状态的指令 开始执行代码时处于ARM状态；异常处理时也处于ARM状态；异常处理返回时，可以返回到Thumb状态 运行模式 特权模式：除了用户模式外的其他模式 异常模式：除了用户模式、系统模式外的其他模式 功能说明 系统模式运行特权操作系统任务(与用户模式有完全相同的寄存器组)异常模式主要处理中断和异常 寄存器组织ARM有37个32位长的寄存器： 1个PC(Program Counter), 30个通用寄存器, 1个CPSR(Current Program Status Register), 1个SPSR(Saved Program Status Register) 31个通用32位寄存器，6个状态寄存器 寄存器功能说明 R0$\\sim$R13: 保存数据/地址值的通用寄存器，无特殊用途 R0$\\sim$R7: 未分组寄存器，是所有处理器模式对应的通用寄存器，无特殊处理模式 R8$\\sim$R12: 一个分组用于除FIQ模式之外的所有寄存器模式，另一组用于FIQ模式，目的：发生FIQ中断时，可以加速FIQ的处理速度，解释：专门为FIQ中断保留一组特定寄存器，能够保证在发生FIQ中断时，没有其他模式占用这一分组的寄存器(数据占用，处理器只能处于一个模式)，可以使得中断处理更加迅速，因此也称为快中断模式 R13$\\sim$R14: 有6个分组的寄存器，一个用于用户与系统模式，其余5个分别用于5中异常模式，解释：寄存器设置也同理，异常模式下，每当发生特定异常时，处理器需要进行异常处理，需要保证存在相应的寄存器能够为之提供服务，而不至于被其他模式所占用 R13: 堆栈指针寄存器，常作为堆栈指针(SP) 不严谨解释: 用户和系统模式共用同一R13，存放程序的指令代码以及其他数据；异常模式具有自己独立的物理寄存器R13：当程序进入不同的异常模式时，可以将需要保护的寄存器放入对应的R13所指向的堆栈中，当程序从异常模式返回时，则从对应的堆栈中恢复， 初始化要求：需要初始化每种模式下的R13 R14: 链接寄存器(LR)，模式本身的R14用于保存子程序返回地址；进入异常模式时对应的R14用于保存异常返回地址 子程序调用返回：调用子程序时，指令先将下一条指令的PC值拷贝到R14，执行完子程序后，再将R14的值拷贝到PC，从而实现子程序的返回 异常处理的返回 R15: 程序计数器(PC)，指向正在取指的地址 ARM状态下，位[1:0]为0，位[31:2]保存PC Thumb状态下，位[0]为0，位[31:1]保存PC ARM体系结构采用多级流水线技术，使得PC值为当前指令的地址值加8个字节 CPSR、SPSR: 程序状态寄存器，程序状态保存寄存器，SPSR仅在异常模式下能够被访问: 进入异常模式时，SPSR保存CPSR的值，异常返回时，通过SPSR的值恢复CPSR的值 Thumb 状态下的寄存器可以直接访问的寄存器: 8个通用寄存器R0$\\sim$R7，程序计数器(PC)，堆栈指针(SP)，链接寄存器(LR)，程序状态寄存器(CPSR)，程序状态保存寄存器(SPSR) 第四讲 ARM 指令系统Load/Store 结构 所有操作数都存储在通用寄存器中 图解: MEM $\\rightarrow$ REG $\\rightarrow$ ALU $\\rightarrow$ MEM ARM 指令 指令分类：数据处理指令，Load/Store 指令，跳转指令，CPSR处理指令，异常产生指令，协处理器指令; 特殊指令：CPSR 处理指令 ARM 状态下，指令根据 CPSR 中条件码状态和指令条件域条件执行 嵌入式微处理器指令集：强调指令的灵活性，对称性，简单性 指令编码说明: bit $12\\sim15$: 4 bits 编码表示 $16$ 个通用寄存器 bit $28\\sim31$: 指令条件码占据指令的高 $4$ bits, 并且条件编码 “1111” 为系统保留 ARM 寻址方式 理解ARM三地址指令格式的特点 立即寻址: 操作数(立即数)包含在指令的 $32$ 位编码中 指令说明 只有第二源操作数可用立即数 立即数以 “#” 为前缀，”#” 后加 “0x” 或 “&amp;” 表示十六进制，”0b” 表示二进制，”0d” 或缺省表示十进制 Example ADD R$0$, R$0$, #$1$ AND R$3$, R$4$, #$0$xFF 立即数的表示 指令中立即数编码比特位: $12$位 扩散数据表示范围: $12$ 位数据表示 $32$ 位数据空间 立即数组成: $8$(常数) $+$(偶数移位个数)$=12$ 移位方式: 循环右移 特点: 所有有效位必须小于等于 $8$ 位，右移位数必须为偶数 扩散数据原理 寄存器寻址: 操作数在寄存器中，指令地址码字段编码寄存器编号 Example MOV R$1$, R$2$ ; R$1$ = (R$2$) SUB R$0$, R$1$, R$2$ ; R$0$ = (R$1$) - (R$2$) 寄存器移位寻址: ARM 指令集特有寻址方式，主要为了增强精简指令的执行效率 指令说明 ARM 移位寻址中，在取操作数送往ALU过程中，就已经在总线上完成了移位运算操作 Example MOV R$0$, R$2$, LSL #3 ; R$0$ = (R$2$) &lt;&lt; 3 ANDS R$1$, R$2$, LSL R$3$ ; R$1$ = (R$1$) &amp; (R$2$) &lt;&lt; (R$3$) 移位方式 LSL(Logical Shift Left) LSR(Logical Shift Right) ASL(Arithmetic Shift Left) ASR(Arithmetic Shift Right) ROR(Rotate Right) RRX(Rotate Right Extended by 1 Place) 基址寻址: 存储器地址在寄存器中 指令说明寄存器中存储的是存储单元的地址 Example LDR R$0$, [R$1$] STR R$0$, [R$1$] 变址寻址: 基址寄存器+地址偏移量 地址偏移量 立即数 控制位 U=$1$时加偏移量，U=$0$时减偏移量 偏移量占 $12$ Bit(立即数) Example: LDR R$0$, [R$1$, #4] LDR R$0$, [R$1$, #-4] 寄存器 控制位 U=$1$时加索引寄存器值，U=$0$时减索引寄存器值 Example: LDR R$0$, [R$1$, R$2$] LDR R$0$, [R$1$, -R$2$] 寄存器及移位常数 地址偏移量由寄存器通过移位得到 Example: LDR R$0$, [R$1$, R$2$, LSL #2] 变址模式 前变址模式 LDR R$0$, [R$1$, #$4$] ; R$0$ = Mem$32$[R$1$+$4$] 自动变址模式(事先更新方式) LDR R$0$, [R$1$, #$4$]! ; R$0$ = Mem$32$[R$1$+$4$], R$1$ = R$1$ + $4$ 后变址模式(事后更新方式) LDR R$0$, [R$1$], #$4$ ; R$0$ = Mem$32$[R$1$], R$1$ = R$1$ + $4$ 3种地址偏移格式与3种变址模式，可以组合出9种类型的变址寻址方式。 相对寻址: 基址寄存器是程序计数器 PC 主要用于分支指令 Example 1234 BL SUBR ; jump to SUBRSUBR ; entry of subprogram .... MOV PC, R14 块拷贝寻址 多寄存器传送指令 LDM/STM 的寻址方式 寄存器组可以是 $16$ 个通用寄存器的任意子集 地址增长顺序: IA(Increment After), IB(Increment Before), DA(Decrement After), DB(Decrement Before) Example 堆栈寻址 堆栈形式 满递增(指令：LDMFA, STMFA) 空递增(指令：LDMEA, STMEA) 满递减(指令：LDMFD, STMFD) 空递减(指令：LDMFD, STMFD)注：FD(Full Decrement), ED(Empty Decrement), FA(Full Aggrandizement), EA(Empty AdvanceAggrandizement) Example ARM指令中的堆栈 STMFD SP!, {R$1$-R$7$, LR} LDMFD SP!, {R$1$-R$7$, LR} Thumb指令中的堆栈 PUSH {R1-R7, LR} POP {R1-R7, PC} ARM 指令集概述数据处理指令 主要完成寄存器中数据的算术和逻辑运算操作 数据宽度：所有操作数都是 $32$ 位宽度 Load/Store 指令该类指令用于寄存器与内存中数据传送，因为ALU的数据来源只能是寄存器，寄存器中数据要提前从内存中获取。 单寄存器存取指令(LDR/STR)(Load Register/Store Register) 加载/存储字/半字/字节(32位对齐/16位对齐/8位对齐) 多寄存器存取指令(LDM/STM)(Load Multiple Registers/Store Multiple Registers) 堆栈操作和块拷贝 存储器与寄存器交换指令SWP 杂项指令第五讲 ARM 指令详细介绍 体会 ARM 指令体系的规整性 数据处理指令数据传送指令(2条) MOV 指令格式: MOV{条件}{S} 目的寄存器, 源操作数 源操作数: 寄存器、被移位的寄存器、立即数 S: 存在S时表示更新CPSR中条件标志位 Example MOV R0, #100 MOV R0, R1 MOV R0, R1, LSL #2 MVN(Move Not) 指令格式: MVN{条件}{S} 目的寄存器, 源操作数 源操作数: 寄存器、被移位的寄存器、立即数 指令说明: 将源操作数按位取反传送到目的寄存器，实现逻辑非 S: 存在S时表示更新CPSR中条件标志位 Example MVN R0, #0 ; 寄存器置1操作 算术运算指令(6条) 三地址指令格式 ADD 指令格式: ADD{条件}{S} 目的寄存器, 操作数1, 操作数2 指令说明: 目的寄存器 = 操作数1 + 操作数2 操作数1: 寄存器 操作数2: 寄存器、被移位的寄存器、立即数 Example ADD R0, R1, R2 ADD R0, R1, #100 ADD R0, R1, R2 LSL #1 ADC 指令格式: ADC{条件}{S} 目的寄存器, 操作数1, 操作数2 指令说明: 目的寄存器 = 操作数1 + 操作数2 + CF 操作数1: 寄存器 操作数2: 寄存器、被移位的寄存器、立即数 Example: 128位数加法 ; 128Num1: R7-R4, 128Num2: R11-R8, 128Sum: R3-R0 ADDS R0, R4, R8 ; 加低位的字 ADCS R1, R5, R9 ; 加第二个字，带进位 ADCS R2, R6, R10 ; 加第三个字，带进位 ADC R3, R7, R11 ; 加高8位，带进位 SUB 指令格式: SUB{条件}{S} 目的寄存器, 操作数1, 操作数2 指令说明: 目的寄存器 = 操作数1 - 操作数2 操作数1: 寄存器 操作数2: 寄存器、被移位的寄存器、立即数 Example: SUB R0, R1, R2 SUB R0, R1, #256 SUB R0, R1, R2 LSL #1 SBC 指令格式: SBC{条件}{S} 目的寄存器, 操作数1, 操作数2 指令说明: 目的寄存器 = 操作数1 - 操作数2 - $\\overline{CF}$ 操作数1: 寄存器 操作数2: 寄存器、被移位的寄存器、立即数 Example: 64位数减法 ; 64Num1: R1-R0, 64Num2: R3-R2, 64Sub: R1-R0 SUBS R0, R0, R2 ; 加低位的字 SBC R1, R1, R3 ; 加第二个字，带进位 注解: 假设R3-R0为4位寄存器，做减法: 46H-24H; 6H-4H = [6H]补 + [-4H]补 = 0110B + 1100B = 0010B $\\Rightarrow$ R0 = 2H with CF = 1 (数据有溢出) 4H-2H-$\\overline{CF}$ = [4H]补 + [-2H]补 = 0100B + 1110B = 0010B $\\Rightarrow$ R1 = 2H with CF = 1 (数据有溢出) RSB(Reverse Subtract) 指令格式: RSB{条件}{S} 目的寄存器, 操作数1, 操作数2 指令说明: 目的寄存器 = 操作数2 - 操作数1 操作数1: 寄存器 操作数2: 寄存器、被移位的寄存器、立即数 Example: RSB R0, R1, R2 RSB R0, R1, #256 RSB R0, R1, R2 LSL #1 RSC 指令格式: RSC{条件}{S} 目的寄存器, 操作数1, 操作数2 指令说明: 目的寄存器 = 操作数2 - 操作数1 - CF 操作数1: 寄存器 操作数2: 寄存器、被移位的寄存器、立即数 Example: 64位数减法 ; 64Num1: R1-R0, 64Num2: 0, 64Sub: R3-R2 RSBS R2, R0, #0 RSC R3, R1, #0 逻辑运算指令(4条) AND 指令格式: AND{条件}{S} 目的寄存器, 操作数1, 操作数2 指令说明: 目的寄存器 = 操作数1 &amp; 操作数2 操作数1: 寄存器 操作数2: 寄存器、被移位的寄存器、立即数 Example: 屏蔽操作数1的某些位 AND R1, R1, #3 ORR 指令格式: ORR{条件}{S} 目的寄存器, 操作数1, 操作数2 指令说明: 目的寄存器 = 操作数1 | 操作数2 操作数1: 寄存器 操作数2: 寄存器、被移位的寄存器、立即数 Example: 设置操作数1的某些位 ORR R0, R0, #3 EOR 指令格式: EOR{条件}{S} 目的寄存器, 操作数1, 操作数2 指令说明: 目的寄存器 = 操作数1 ^ 操作数2 操作数1: 寄存器 操作数2: 寄存器、被移位的寄存器、立即数 Example: 反转操作数1的某些位 EOR R0, R0, #3 ; 低位反转 BIC 指令格式: BIC{条件}{S} 目的寄存器, 操作数1, 操作数2 指令说明: 清除操作数1的某些位，操作数2作为操作数1的掩码，如果操作数2置位，则清除操作数1对应位 操作数1: 寄存器 操作数2: 寄存器、被移位的寄存器、立即数 Example: BIC R0, R0, #3 ; 清除低两位 比较指令(2条) CMP 指令格式: CMP{条件} 操作数1, 操作数2 指令说明: 操作数1-操作数2=操作数1+[-操作数2]补码=操作数1+[操作数2]求补，不存储结果，只影响CPSR中标志位 操作数1: 寄存器 操作数2: 寄存器、立即数 Example: CMP R1, R0 CMP R1, #100 CMN 指令格式: CMN{条件} 操作数1, 操作数2 指令说明: 操作数1-[操作数2]求补=操作数1+[-[操作数2]求补]补码=操作数1+[[操作数2]求补]求补=操作数1+操作数2，不存储结果，只影响CPSR中标志位 操作数1: 寄存器 操作数2: 寄存器、立即数 Example: CMN R1, R0 CMN R1, #100 测试指令(2条) TST 指令格式: TST{条件} 操作数1, 操作数2 指令说明: 操作数1 &amp; 操作数2，不存储结果，只影响CPSR中标志位(ZF); 操作数2一般作为位掩码 操作数1: 寄存器 操作数2: 寄存器、立即数 Example: TST R1, #1 ; 测试最低位是否为1 TST R1, #0xffe TEQ 指令格式: TEQ{条件} 操作数1, 操作数2 指令说明: 操作数1 ^ 操作数2，不存储结果，只影响CPSR中标志位(ZF); 通常用于比较操作数1和操作数2是否相等 操作数1: 寄存器 操作数2: 寄存器、立即数 Example: TEQ R1, R2 乘法指令(6条) MUL(32位有符号或无符号乘法) 指令格式: MUL{条件}{S} 目的寄存器, 操作数1, 操作数2 指令说明: 目的寄存器 = [操作数1 * 操作数2][31…0]，影响CPSR中标志位 操作数1: 寄存器 操作数2: 寄存器 Example: MUL R0, R1, R2 MULS R0, R1, R2 MLA(32位有符号或无符号乘加法, Mulliply Add) 指令格式: MLA{条件}{S} 目的寄存器, 操作数1, 操作数2, 操作数3 指令说明: 目的寄存器 = [操作数1 * 操作数2 + 操作数3][31…0]，影响CPSR中标志位 操作数1: 寄存器 操作数2: 寄存器 Example: MLA R0, R1, R2, R3 MLAS R0, R1, R2, R3 SMULL(64位有符号数乘法, Signed Mulliply Long) 指令格式: SMULL{条件}{S} 目的寄存器Low, 目的寄存器High, 操作数1, 操作数2 指令说明: 目的寄存器 = [操作数1 * 操作数2][63…0]，影响CPSR中标志位 操作数1: 32位寄存器 操作数2: 32位寄存器 Example: SMULL R0, R1, R2, R3 ; R0(Low32) = (R2R3)(Low32), R1(High32) = (R2R3)(High32) SMULLS R0, R1, R2, R3 SMLAL(64位有符号数乘加法, Signed Mulliply ADD Long) 指令格式: SMLAL{条件}{S} 目的寄存器Low, 目的寄存器High, 操作数1, 操作数2 指令说明: 目的寄存器 = [操作数1 * 操作数2 + 目的寄存器][63…0]，影响CPSR中标志位 操作数1: 32位寄存器 操作数2: 32位寄存器 Example: SMLAL R0, R1, R2, R3 ; R0(Low32) = (R2R3)(Low32) + R0(Low32), R1(High32) = (R2R3)(High32) + R1(High32) SMLALS R0, R1, R2, R3 UMULL(64位无符号数乘法, Unsigned Mulliply Long) 指令格式: UMULL{条件}{S} 目的寄存器Low, 目的寄存器High, 操作数1, 操作数2 指令说明: 目的寄存器 = [操作数1 * 操作数2][63…0]，影响CPSR中标志位 操作数1: 32位寄存器 操作数2: 32位寄存器 Example: UMULL R0, R1, R2, R3 ; R0(Low32) = (R2R3)(Low32), R1(High32) = (R2R3)(High32) UMULLS R0, R1, R2, R3 UMLAL(64位无符号数乘加法, Unsigned Mulliply ADD Long) 指令格式: UMLAL{条件}{S} 目的寄存器Low, 目的寄存器High, 操作数1, 操作数2 指令说明: 目的寄存器 = [操作数1 * 操作数2 + 目的寄存器][63…0]，影响CPSR中标志位 操作数1: 32位寄存器 操作数2: 32位寄存器 Example: UMLAL R0, R1, R2, R3 ; R0(Low32) = (R2R3)(Low32) + R0(Low32), R1(High32) = (R2R3)(High32) + R1(High32) UMLALS R0, R1, R2, R3 Load/Store指令 ARM中用于在寄存器和存储器之间传送数据的指令 单寄存器存取指令(6条) LDR(字数据加载指令) 指令格式: LDR{条件} 目的寄存器, &lt;存储器地址&gt; 指令说明: 目的寄存器 = Mem[Address]——32位字数据 Example: LDR R0, [R1] ; R0 = Mem[R1] LDR R0, [R1, R2] ; R0 = Mem[R1+R2] LDR R0, [R1, #4] ; R0 = Mem[R1+4] LDR R0, [R1, R2]! ; R0 = Mem[R1+R2], R1 = R1 + R2 LDRB(字节数据加载指令) 指令格式: LDR{条件}B 目的寄存器, &lt;存储器地址&gt; 指令说明: 目的寄存器[7…0] = Mem[Address]——8位字节数据, 目的寄存器[31…8] = 0 Example: LDRB R0, [R1] ; R0 = Mem[R1] LDRB R0, [R1, #8] ; R0 = Mem[R1+8] LDRH(半字数据加载指令) 指令格式: LDR{条件}H 目的寄存器, &lt;存储器地址&gt; 指令说明: 目的寄存器[15…0] = Mem[Address]——16位字节数据, 目的寄存器[31…16] = 0 Example: LDRB R0, [R1] ; R0 = Mem[R1] LDRB R0, [R1, #8] ; R0 = Mem[R1+8] STR(字数据存储指令) 指令格式: STR{条件} 源寄存器, &lt;存储器地址&gt; 指令说明: Mem[Address] = 源寄存器——32位字数据 Example: STR R0, [R1], #8 ; Mem[R1] = R0, R1 = R1 + 8 STR R0, [R1, #4] ; Mem[R1+4] = R0 STR R0, [R1, R2]! ; Mem[R1+R2] = R0, R1 = R1 + R2 STRB(字节数据存储指令) 指令格式: STR{条件}B 源寄存器, &lt;存储器地址&gt; 指令说明: Mem[Address] = 源寄存器Low8——8位字节数据 Example: STRB R0, [R1] ; Mem[R1] = R0(Low8) STRB R0, [R1, #8] ; Mem[R1+8] = R0(Low8) STRH(半字数据存储指令) 指令格式: STR{条件}H 源寄存器, &lt;存储器地址&gt; 指令说明: Mem[Address] = 源寄存器Low16——16位半字数据 Example: STRH R0, [R1] ; Mem[R1] = R0(Low16) STRH R0, [R1, #8] ; Mem[R1+8] = R0(Low16) Check Point: Load/Store指令的基址寻址于变址变址 多寄存器存取指令(2条)ARM中支持的批量数据加载/存储指令可以一次在一片连续的存储器单元和多个寄存器之间传送数据，通常用于堆栈操作 STM(批量数据存储指令) 指令格式: STM{条件}{类型} 基址寄存器{!}, 寄存器列表{^} 指令说明: 多个寄存器中字数据传送到基址寄存器指向的连续存储器单元中 类型——地址增长顺序: IA(Increment After), IB(Increment Before), DA(Decrement After), DB(Decrement Before) Example: STMFD R0!, {r1-r4} LDM(批量数据加载指令) 指令格式: LDM{条件}{类型} 基址寄存器{!}, 寄存器列表{^} 指令说明: 基址寄存器指向的连续存储器单元中字数据传送到多个寄存器中 Example: LDMFD R0!, {r1-r4} 存储器和寄存器交换指令(2条) 主要用于实现信号量操作，信号量用于进程的同步与互斥 SWP 指令格式: SWP{条件} 目的寄存器, 源寄存器1, [源寄存器2] 指令说明: 目的寄存器=Mem[源寄存器2]，Mem[源寄存器2] = 源寄存器1 Example: 寄存器和存储器交换 SWP R0, R0, [R2] 跳转指令ARM程序跳转概述 PC寄存器中写入目标地址值 实现4GB地址空间的任意跳转 跳转指令实现 向前或向后32MB地址空间跳转 ARM程序跳转 B(Branch) 分支跳转指令，根据条件进行分支跳转 B{条件} 目标地址 不加条件时，表示无条件跳转指令 BL(Branch with Link) 分支链接指令 BL{条件} 目标地址 将下一条指令的地址拷贝到链接寄存器(R14/LR)中，然后跳转到指定的地址运行程序 BX(Branch and eXchange) 分支交换指令 BX{条件} 目标地址 带状态切换的跳转指令，具体状态有目标寄存器最低位决定 BLX(Branch with Link and eXchange) 分支链接交换指令 BLX 目标地址 带状态切换的分支链接指令 程序状态寄存器访问指令用于在程序状态寄存器(CPSR/SPSR)和通用寄存器之间传送数据 MRS 指令格式: MRS{条件} 通用寄存器, 程序状态寄存器 指令说明: 状态寄存器的内容读取到通用寄存器, Rn$\\rightarrow$ CPSR/SPSR Example: 临时保存程序状态寄存器的内容 异常处理或进程切换时，先读再存程序状态寄存器中的值 MSR 指令格式: MSR{条件} 程序状态寄存器_&lt;域&gt;, 操作数 指令说明: 将操作数的内容传送到状态寄存器的特定域中,操作数可以为通用寄存器或立即数 域：对32位程序状态字寄存器的划分，分为4个8位域 c位[7:0]为控制位域, 表示为c x位[15:8]为扩展位域, 表示为x s位[23:16]为状态位域, 表示为s f位[31:24]为条件标志位域, 表示为f Example: MSR CPSR, R0 MSR CPSR_c, R0 异常产生指令 SWI 软件中断指令 BKPT 断点中断指令 协处理器指令第六讲 ARM汇编ARM汇编语言源程序一般由ARM指令、伪操作(包括伪指令)和宏指令组成；其中伪操作(包括伪指令)是由汇编器在对源程序汇编期间由汇编程序处理的。 伪操作符号定义伪操作用于定义ARM汇编程序中的变量，对变量赋值和寄存器重命名 定义全局变量(GBLA(GloBaL Arithmetic), GBLL(GloBaL Logical), GBLS(GloBaL String)) 指令格式: GBLA(GBLL/GBLS) 全局变量名 指令说明: GBLA算术, GBLL逻辑, GBLS字符串 定义ARM程序中全局变量并初始化为0 Example: GBLA Test1 Test1 SETA 0xaa GBLL Test2 Test2 SETL {TRUE} GBLS Test3 Test3 SETS “Testing” 定义局部变量(LCLA(LoCaL Arithmetic), LCLL(LoCaL Logical), LCLS(LoCaL String)) 指令格式: LCLA(LCLL/LCLS) 局部变量名 指令说明: LCLA算术, LCLL逻辑, LCLS字符串 定义ARM程序中局部变量并初始化为0 Example: LCLA Test1 Test1 SETA 0xaa LCLL Test2 Test2 SETL {TRUE} LCLS Test3 Test3 SETS “Testing” 对变量进行赋值(SETA(SET Arithmetic), SETL(SET Logical), SETS(SET String)) 指令格式: 变量名 SETA(SETL/SETS) 表达式 指令说明: 为已定义过的全局变量或局部变量进行赋值 Example: LCLA Test1 Test1 SETA 0xaa LCLL Test2 Test2 SETL {TRUE} 命名通用寄存器列表(RLIST(Register List)) 指令格式: 名称 RLIST {寄存器列表} 指令说明: 用于对一个通用寄存器列表定义名称，列表中寄存器访问次序为寄存器的编号由低到高 Example: Reglist RLIST {R0-R5, R8, R10} 数据定义伪操作汇编控制伪操作 控制汇编程序的执行流程 IF…ELSE…ENDIF WHILE…WEND MACRO…MEND(Macro) 宏定义伪操作，标识一个宏的开始与结束 MEXIT 从宏定义中跳转出去 信息报告伪操作 ASSERT 指令格式: ASSERT Logical_Expression 指令说明: 断言表达式成立否则报错，并且该伪操作在第二次扫描汇编程序中判断 Example: ASSERT Top&lt;&gt;Temp INFO 指令格式: INFO numeric_expr, string_expr 指令说明: 该伪操作在第一次或第二次扫描时报告诊断信息 参数说明: numeric_expr = 0 第一遍扫描时报告，否则第二遍时；string_expr: 诊断信息 Example: INFO 0, “Version 0.1” 其他伪操作 AREA 指令格式: AREA 段名, 属性1, 属性2, … 指令说明: CODE属性(代码段)，DATA属性(数据段)，READONLY属性，READWRITE属性 Example: AREA Init，CODE，READONLY … END CODE 指令格式: CODE16/CODE32 指令说明: 通过CODE16伪操作通知编译器其后指令为16位的Thumb指令；CODE32通知编译器其后指令为32位的ARM指令；以此进行ARM的状态切换 ENTRY 指令格式: ENTRY 指令说明: ENTRY伪操作用于指定汇编程序入口点; 在一个完整的汇编程序中至少要有一个ENTRY Example: AREA Init，CODE，READONLY ENTRY END 指令格式: END 指令说明: END伪操作用于通知编译器已经到了源程序的结尾 Example: AREA Init，CODE，READONLY … END EQU 指令格式: 名称 EQU 表达式 {，类型} 指令说明: EQU伪操作用于为程序中的标号、数字常量和寄存器的值等定义一个字符名称；可以用’*’代替 Example: Test EQU 50 Addr * 0x55，CODE32 ; 定义Addr的值为0x55, 且该处为32位的ARM指令(类型) EXPORT/GLOBAL 指令格式: EXPORT 标号 指令说明: 用于在程序中声明一个全局的标号 Example: 1234AREA Init CODE，READONLY EXPORT Test ; 声明一个可全局引用的标号Test ... END IMPORT 指令格式: IMPORT 标号 指令说明: 用于通知编译器要使用的标号在其他的源文件中定义，但要在当前源文件中引用; 如果没有引用，该标号实际上会被加入到当前源文件中 Example: 1234AREA Init CODE，READONLY IMPORT Main ; 引用一个全局的标号Main ... END EXTERN 指令格式: EXTERN 标号 指令说明: 用于通知编译器要使用的标号在其他的源文件中定义，但要在当前源文件中引用; 如果没有引用，该标号实际上不会被加入到当前源文件中 GET 指令格式: GET 文件名 指令说明: GET伪操作用于将一个源文件包含到当前的源文件中，并将被包含的源文件在当前位置进行汇编处 可以使用INCLUDE代替GET ARM语言伪指令伪指令不是真正的ARM指令,在汇编过程中会被替换成对应的ARM指令或指令序列 ADR(AdDRess) 指令格式: ADR {cond} register, 标号 指令说明: 将基于PC的地址值(程序标号)或基于寄存器的地址值加载到寄存器中 Example: 123start MOV R0, #0 ADR R1, start =&gt; SUB R1, pc, #0xC ADRL(AdDRess Large) 指令格式: ADRL {cond} register, 标号 指令说明: 将基于PC的地址值(程序标号)或基于寄存器的地址值加载到寄存器中,可被替换成两条合适的指令，所以加载的地址范围更大 Example: 1234start MOV R0, #0 ADRL R1, start + 60000 =&gt; ADD R1, pc, 0xE800 =&gt; ADD R1, R1, 0x254 LDR(Load Register) 指令格式: LDR {cond} register, = expr 或 label-expr 指令说明: 可将任意一个32位常数或地址值加载到寄存器中 Example: 1234LDR R1, =0xFF=&gt; MOV R1, 0xFFLDR R1, =0xFFF=&gt; LDR R1, [PC, OFFSET_TO_LPOOL] NOP(NO Operation) 指令格式: NOP 指令说明: 汇编时会被替换成一条无用指令, 不影响CPSR中的条件标志位 Example: 12NOP=&gt; MOV R0, R0 第七讲 ARM下的C编程了解C运行时库C运行时库(C Runtime Library)用于在C程序运行时提供支持的函数和例程集合，包含C语言所需的基本功能和支持： ARM的C编译器支持ANSI C运行时库 ARM C运行时库以二进制形式提供 用户可以建立自己的运行时库 宏和函数的区别宏：在C程序中定义的命名代码段。 编译器会将相关代码放到宏名出现的每一个地方，宏的代码在任何出现宏名的地方都会编译一次；使用宏时不需要保存/恢复上下文，也不必返回 在编译器预处理阶段会被直接展开到代码中，编译时用宏内容替换宏调用位置，可能导致代码膨胀 代码简单用宏 函数： 函数的代码只需要编译一次 在编译时定义，但实际执行发生在运行时，会有函数调用开销 代码复杂用函数 三、什么是内嵌汇编 标识符：_ _asm：用于告诉编译器后面是汇编代码 内嵌汇编12345678910void enable_IRQ(void){ int temp; __asm { MRS tmp, CPSR BIC tmp, tmp, #0x80 MSR CPSR_c, tmp }} _irq、volatile关键字的作用_irq：使用该关键字声明的函数可以被用作异常中断的中断处理程序，该函数通过将lr-4的值赋予PC寄存器，并将 SPSR的值赋予CPSR实现函数返回 volatile：用于声明变量，告诉编译器该变量可能在程序之外修改；编译时不能优化对volatile变量的操作；不能对volatile变量使用缓冲技术 汇编和 C 的混合编程参数传递规则： 参数个数可变的子程序参数传递规则： 参数不超过4个时，使用R0～R3来传递； 参数超过4个时，可以使用数据栈来传递 参数个数固定的子程序参数传递规则: 第一个整数参数，通过R0～R3来传递 其他参数通过数据栈来传递 有关浮点运算，需特别处理 子程序结果返回规则: 结果为一个32位整数时，可以通过寄存器R0返回 结果为一个64位整数时，可以通过寄存器R0和R1返回， 依次类推 ARM 异常响应过程 ARM异常处理 当异常中断发生时, 系统执行完当前指令后，跳转到相应的异常中断处理 程序处执行 执行完成后，程序返回到发生中断的指令的下一条指 令处执行 进入异常中断处理程序时要保存现场，返回时要恢复 现场 异常向量表 当异常发生时，处理器会把PC设置为一个特定的存储器地址 这一地址放在被称为向量表(vector table)的特定地址范围内 异常向量表通常放在存储地址的低端 第八讲 RISC-V&amp;华为鲲鹏指令集架构(ISA)和国内ISA生态 指令：是指处理器进行操作的最小单元（如算术/逻辑运算）； 指令集：顾名思义是指一组指令的集合； 指令集架构：又称为“处理器架构”，不仅是指令的集合，还包括编程需要的硬件信息，如支持的数据类型、存储器、寄存器状态、寻址模式和寄存器模型等； 指令集架构（ISA, Instruction Set Architecture）才是区分不同CPU的标准； 微架构：处理器的具体硬件实现方案称为微架构； 常见指令集架构：x86(CISC), SPARC, MIPS, Power(国产), Alpha(国产), ARC, ARM(RISC), Intel RISV-V的主要特征RISC-V，是一种基于“精简指令集（RISC）”原则的指令集架构，它具有开源、重新设计(后发优势)、简单哲学、模块化指令集、指令集可扩展的特点。 华为鲲鹏 鲲鹏 920 使用的指令集架构是 ARM v8 64 位 鲲鹏 920 生产使用的制程是 7nm 相比于上一代的鲲鹏 916，鲲鹏 920 处理器的计算核数提升 1 倍，最多支持 64 核 第九讲 最小系统设计嵌入式系统最小系统CPU能够运行所需的模块有(最小系统)：电源、时钟、复位、内存、调试接口（JTAG） DC-DC 转换器的常见形式线性稳压器、开关稳压器和充电泵 晶体和晶振时钟一般由晶体振荡器提供，而晶体和晶振是构成晶体振荡器的两种方式: 晶体(无源) 封装内部只含有晶体, 没有内部电源 驱动电路由设计者提供 晶振(有源) 封装中包含了完整的晶体振荡器电路 需要电源 复位的基本功能 在系统上电时提供复位信号 保证能够进行手动复位 SRAM,DRAM,SDRAM和Flash SRAM(静态RAM) 数据存入静态RAM后，只要电源维持不变，其中存储的数据就能够一直维持不变 , 不需要刷新操作 读写速度快 由触发器构成基本单元，接口简单 存储单元结构复杂，集成度较低 常常用作高速缓冲存储器[读写速度快] 处理器内部集成存储器多选择SRAM DRAM(动态RAM) 依靠电容存储信息，需要不断刷新 读写速度慢 集成度高，成本低 地址引脚少，地址总线采用多路技术，接口复杂 多用于外部存储器扩展(外存)[读写速度慢] SDRAM(同步动态RAM) SDRAM因为要同CPU和芯片组共享时钟，所以芯片组可以主动的在每个时钟的上升沿发给引脚控制命令 动态RAM加上同步特性[在每个时钟的上升沿发给引脚控制命令] Nor Flash 芯片内执行 读速度快（相比Nand Flash） 写入与擦除速度很低 擦除按块进行，写入前必须先擦除 带有SRAM接口，有足够的地址引脚来寻址，可以很容易地存取其内部的每一个字节 常用来存储代码 Nand Flash NAND读和写操作按512字节的块进行 写入与擦除速度比Nor Flash快 擦除按块进行，写入前必须先擦除 NAND的擦除单元更小，相应的擦除电路更少 NAND器件使用复杂的I/O口来串行地存取数据 常用来存储数据 存储器映射存储器映射到物理地址, 通过将存储器按照物理地址划分为不同模块, 进行RAM, DRAM, SDRAM, FLASH等存储设计. 第十讲 外部设备及通信接口常用外部接口I/O（Input/Output）接口是一个微控制器必须具备的最基本的外设功能. 每个I/O口一般都对应了两个寄存器: 数据寄存器：数据寄存器的各位都直接引到芯片外部 控制寄存器：控制数据寄存器中每位的信号流通方向和方式 GPIO General-Purpose I/O ports，也就是通用I/O口，是I/O的最基本形式 LCD 液晶显示器(Liquid Crystal Display): 晶显示器是一种被动光源的显示器，自身不能发光，只能借助外界光源, 具有省电、体积小、低成本、低功率等特点，被广泛应用于嵌入式系统中 液晶: 以液态形式存在的晶体 有电流流过，液晶分子会以电流的方向进行排列；没有电流时，平行排列 基本原理: 通过给不同的液晶单元供电，控制其光线的通过与否而达到显示的目的 触摸屏 触摸屏由触摸检测部件和触摸屏控制器组成, 按照触摸屏的工作原理和传输信息的介质主要分为: 电阻式, 电容感应式, 红外线式, 表面声波式四种 ADC 模/数转换器就是把电模拟量转换成数字量的电路 DAC 数/模转换器就是把数字量转换成模拟量的电路 第十一讲 嵌入式操作系统嵌入式操作系统相对于通用操作系统，嵌入式操作系统更为精巧, 但并不意味着可以通过裁剪通用操作系统来实现, 它有自身的特点: 实时性 可移植性 可配置、可裁剪性 可靠性 应用编程接口 每个操作系统提供的系统调用的功能和种类都不同 嵌入式操作系统的任务主要任务是尽可能地屏蔽底层硬件的差异，对上层应用软件和底层硬件提供标准化服务 嵌入式操作系统的功能内核是嵌入式操作系统的基础, 具有以下功能: 任务管理 中断管理 通信, 同步和互斥机制 内存管理 I/O管理 嵌入式操作系统的实时性实时性是实时内核最重要的特征之一 实时系统的正确性不仅依赖于系统计算的逻辑结果(计算逻辑)，还依赖于产生这些结果的时间(计算时间) 相关概念 确定性 实时性是指内核应该尽可能快的响应外部事件 确定性是指对事件响应的最坏时间是可预知的 响应性 确定性关心系统在识别外部事件之前的延迟(响应启动延迟) 响应性关心的是在识别外部事件后，系统要花多长时间来服务该事件(响应时间) 响应时间 确定性和响应性结合在一起构成事件响应时间。 中断响应时间 = 最长关中断时间 + 保护CPU内部寄存器的时间 + 进入中断服务函数的执行时间 + 开始执行中断服务程序(ISR)的第一条指令时间 任务响应时间 = 中断响应时间+中断服务时间 对于强实时内核，响应时间应该在 $\\mu s$ 级 多任务程序设计结构前后台结构 后台循环, 前台中断 后台行为: 应用程序是一个无限的循环，循环中调用相应的函数完成相应的操作，这部分可以看成后台行为 前台行为: 中断服务程序处理异步事件，这部分可以看成前台行为 多任务系统的相关概念嵌入式实时系统中，多采用多任务程序设计的方法，其特点有： 单个任务规模较小，容易编码和调试 任务间独立性高、耦合性小，便于扩充功能 系统实时性强，以保证紧急事件得到优先处理 任务的概念进程是资源分配的最小单位, 线程是进程内部一个相对独立的控制流，是调度执行的最小单位【基本概念】 大多数实时内核都把整个应用当作一个没有定义的进程，应用则被划分为多个任务来处理 整个内核是一个单进程/多线程模型，简单称为多任务模型 【概念阐述】多任务系统是一个单进程或多进程的系统内核, 其中多个进程是由内核将整个应用划分成多个任务进行处理而建立的.【任务的要素】 代码: 一段可执行的程序 初始数据: 程序执行所需的相关数据 堆栈: 保存局部变量和现场的存储区 任务控制块: 包含任务相关信息的数据结构以及任务执行过程中所需要的所有信息 【任务的特点】 动态性(就绪, 运行或等待), 并行性(同时存在多个任务, 宏观上并行), 异步独立性(各任务相互独立,运行速度不可预知) 任务的基本状态及转换 运行状态(running) 该任务已获得运行所必需的资源，它的程序正在处理机上执行 阻塞状态(wait) 任务正等待着某一事件的发生而暂时停止执行; 这时，即使给它CPU控制权，它也无法执行, 则称该任务处于阻塞态 就绪状态(ready) 任务已获得除CPU之外的运行所必需的资源，一旦得到CPU控制权，立即可以运行 就绪$\\longrightarrow$运行 调度程序选择一个新的进程运行(进程被调度) 运行$\\longrightarrow$就绪 运行进程用完了时间片(时间片用完) 运行进程被中断，因为一高优先级进程处于就绪状态(被高优先级进程抢占) 运行$\\longrightarrow$等待 进程必须等待某个事件： OS尚未完成服务 对一资源的访问尚不能进行 初始化I/O 且必须等待结果 等待$\\longrightarrow$就绪 所等待的事件已经发生 任务的切换 保存当前任务的上下文 当多任务内核决定运行另外的任务时，任务切换要求保存正在运行任务的当前状态，即CPU寄存器中的全部内容被保存到任务的当前状态保存区(任务自己的堆栈区) 恢复需要运行任务的上下文 原任务状态入栈工作完成后，就把下一个将要运行任务的当前状态从该任务的堆栈中重新装入CPU寄存器，并开始下一个任务的运行 任务的调度【调度的功能】调度只是一个函数调用，可在内核各个部分调用 用来确定多任务环境下任务执行的顺序(任务执行顺序) 用来确定任务获得CPU资源后能够执行的时间(任务执行时间)[基于时间片的调度] 【调度点与调度时机】调用调度程序的具体位置称为调度点： 中断服务程序的结束位置 任务因等待资源而进入等待状态 调度策略 实时任务就绪的原因 中断处理过程中使实时任务就绪: 存在任务请求中断, 中断服务程序会使得中断请求任务就绪【中断处理程序】 当前运行任务调用操作系统功能，使实时任务就绪【系统调用】 非抢占式调度 低优先级任务运行过程中，一个中断到达; 若中断被允许，CPU进入中断服务程序; 中断处理过程使一个高优先级任务就绪; 中断完成后，CPU归还给原先被中断的低优先级任务; 低优先级任务继续运行; 低优先级任务完成或因其它原因被阻塞而释放CPU，内核进行任务调度，切换到就绪的高优先级任务;【调度点】 高优先级任务运行 特点 任务运行空间是封闭的，几乎不需要使用信号量保护共享数据 内核的任务级响应时间是不确定的，完全取决于当前任务何时释放CPU 抢占式调度 低优先级任务运行过程中，一个中断到达; 若中断被允许，CPU进入中断服务程序; 中断处理过程使一个高优先级任务就绪; 中断完成后，内核进行任务调度，让刚就绪的高优先级任务获得CPU;【调度点】 高优先级任务运行; 高优先级任务完成或因其它原因被阻塞而释放CPU，内核进行任务调度;【调度点】 低优先级任务获得CPU，从被中断的代码处继续运行 特点 最高优先级的任务一旦就绪，总能得到CPU的控制权 任务运行空间不再是封闭的，任务不可使用不可重入型函数 需要对共享数据进行必要的保护 实时操作系统大多基于抢占式内核 可重入型函数的理解 可重入型函数 该函数可以被一个以上的任务调用，而不必担心数据被破坏[基本特点] 可重入型函数任何时候都可以被中断，一段时间以后又可以运行，而相应数据不会丢失[可以随时中断或运行] 可重入型函数只使用局部变量，即变量保存在CPU寄存器中或堆栈中 【注解】这使得可重入型函数在发生中断时，其数据会被中断处理程序自动保存，当中断返回后，数据从堆栈中自动恢复，因此不会丢失任何数据. 不可重入型函数1234567int temp;void swap(int *x, int *y){ temp = *x; *x = *y; *y = temp;} 可重入型函数1234567void swap(int *x, int *y){ int temp; temp = *x; *x = *y; *y = temp;} 对临界区的理解【基本概念】临界区，又称为代码的临界区，指处理时不可分割的代码，代码一旦开始执行，则不允许任何中断打断.【临界区的保护】 在进入临界区之前要关中断 临界区代码执行完以后要立即开中断 【内核的关中断时间】 关中断影响中断延迟时间 内核在中断响应时间上的差异主要来自内核最大关中断时间 对共享资源的互斥管理 关中断 禁止任务切换 使用测试并置位指令 使用信号量（提供任务间通信、同步和互斥的最优选择 实时内核的重要性能指标【时间性能指标】 中断延迟时间 指从中断发生到系统获知中断，并开始执行中断服务程序所需要的最大滞后时间 中断延迟时间 = 最大关中断时间 + 中断嵌套的时间+ 硬件开始处理中断到开始执行ISR第一条指令之间的时间 实时内核应尽量使内核最大关中断时间减小 缩短关中断时间: 在临界区的一些非关键代码段开中断，增加内核代码中的可抢占点 中断响应时间 中断响应时间指从中断发生到开始执行用户中断服务程序的第一条指令之间的时间 中断响应时间 = 中断延迟时间 + 保存CPU内部寄存器的时间 + 该内核的ISR进入函数的执行时间 中断恢复时间 中断恢复时间指用户中断服务程序结束后回到被中断代码之前的时间。(对抢占式内核还应包括可能发生的任务切换时间) 中断恢复时间 = 恢复CPU内部寄存器的时间 + 执行中断返回指令的时间 任务响应时间 任务响应时间指从任务对应的中断产生到该任务真正开始运行所花费的时间，又称调度延迟 内核调度算法是决定调度延迟的主要因素 基于优先级的抢占式调度内核中，调度延迟比较小 第十二讲 $\\mu$C/OS-II的内核结构$\\mu$C/OS-II 中任务的状态$\\mu$C/OS–II支持最多64个任务，每个任务有一个特定的优先级，且优先级越高，其数值越小 运行态(运行) 任何时刻只能有一个任务处于运行态 就绪态(可以运行) 任务一旦建立，这个任务就进入就绪态，准备运行 挂起态(不可运行) 正在运行的任务可能需要等待某一事件的发生或将自己延迟一段时间 中断服务态(不属于多任务管理的范围) 正在运行的任务是可以被中断的，除非该任务将中断关闭; 被中断了的任务进入了中断服务态 休眠态(不属于多任务管理的范围) 任务创建之前的状态，仅驻留在程序空间，还没有交给$\\mu$C/OS-II管理 【任务控制块(TCB)】任务控制块是管理任务的数据结构: 任务控制块OS_TCB保存着该任务的相关参数 任务堆栈指针、状态、优先级、任务表位置、任务链表指针等 所有的任务控制块均在$\\mu$C/OS-II初始化时生成，分别存在于两条链表中: 空闲链表和使用链表 任务就绪表的工作机制空闲任务列表 初始态：所有任务控制块都被放置在任务控制块列表数组中 系统初始化: 所有任务控制块被链接成空任务控制块的单向链表 任务建立: 空任务控制块指针指向的任务控制块分配给该任务，链表中指针进行后移 任务队列一般情况下，操作系统通过任务队列的方式进行任务管理。将任务组织为就绪队列和等待队列: 任务就绪时，把任务控制块放在就绪队列尾 任务挂起时，把任务控制块放在等待队列尾 从就绪任务队列选择当前运行任务 【特点】任务处理时间与任务数量密切相关 优先级位图算法【任务就绪表】【注解】 优先级位图: 任务状态分为就绪态和非就绪态，对应两个状态，一比特位编码 任务优先级(0$\\sim 63$)划分为组号(高三位)与组内编号(低三位)，并且高优先级对应小的优先级号，其中组号索引OSRdyTbl，用组内编号索引OSRdyTbl OSRdyTbl类似二维数组，每个元素对应就绪表每一行 OSRdyGrp数组标记OSRdyTbl的每一行是否存在1，即全组任务中没有一个进入就绪态时，OSRdyGrp的相应位才为零 掩码数组OSMapTbl[7]用于对OSRdyTbl和OSRdyGrp置位，预存数组，使用时只是一次取内存的操作 OSMapTbl[0]=$2^0$=0x01(0000 0001) OSMapTbl[1]=$2^1$=0x02(0000 0010) OSMapTbl[7]=$2^7$=0x80(1000 0000) Op1: 使任务进入就绪态prio是任务的优先级，也是任务的识别号，则将任务放入就绪表，或使任务进入就绪态的方法： OSRdyGrp |= OSMapTbl[prio>>3] OSRdyTbl[prio>>3] |= OSMapTbl[prio & 0x07] Op2: 使任务脱离就绪态将任务就绪表OSRdyTbl[prio&gt;&gt;3]相应元素的相应位清零(掩码取反)，而且当OSRdyTbl[prio&gt;&gt;3]中的所有位都为零时，即全组任务中没有一个进入就绪态时，OSRdyGrp的相应位才为零 If((OSRdyTbl[prio>>3] &= ∼OSMapTbl[prio & 0x07])== 0) OSRdyGrp &= ∼OSMapTbl[prio>>3] Op3: 根据就绪表确定最高优先级$$\\min j (j=0, 1, \\cdots, 7), \\min i (j=0, 1, \\cdots, 7)$$ $\\max prio = j*8+i$ 查表法优化[优先级判定表]查表法具有确定的时间，增加了系统的可预测性，$\\mu$C/OS中所有的系统调用时间都是确定的High3 = OSUnMapTbl[OSRdyGrp] Low3 =OSUnMapTbl[OSRdyTbl[High3]] Prio =(Hign3> 3] pevent->OSEventTbl[prio >> 3] |= OSMapTbl[prio & 0x07] 注解：取出prio的低三位利用掩码数组OSMapTbl对组内编号置位，pevent是当前的任务控制块 从等待事件任务列表中使任务脱离等待状态(释放等待时间) if ((pevent->OSEventTbl[prio >> 3] &= ~OSMapTbl[prio & 0x07]) == 0) pevent->OSEventGrp &= ~OSMapTbl[prio >> 3] 在等待事件任务列表中查找优先级最高的任务(寻找最高优先级) y = OSUnMapTbl[pevent->OSEventGrp] x = OSUnMapTbl[pevent->OSEventTbl[y]] prio = (y < 3) + x","link":"/collaboration/EmbeddedSystem/"},{"title":"Signal Filters Design Based on Digital Signal Processing","text":"ThoeriesI. Fourier Series Expansion AlgorithmWe can utilize the Fourier Series to produce the analog signal with some frequency components. For any signal, its Fourier series expansion is defined as $$x(t) = \\frac{A_0}{2}+\\sum_{n=1}^{\\infty}A_n\\cos(n\\Omega t+\\varphi_n)$$ In the equation，$\\frac{A_0}{2}$ represents the DC component, $A_1\\cos(\\Omega t+\\varphi_1)$, represents the fundamental component of the signal, $A_n\\cos(n\\Omega t+\\varphi_n)$ represents the nth harmonic component of the signal. Moreover, analog angular frequency $\\Omega = \\frac{2\\pi}{T}=2\\pi f$.Therefore, in this project we select three different frequency components, that is $f_1, f_2, f_3$, to synthesize the final required analog signal: $$x(t) = \\frac{A_0}{2}+A_1\\cos(2\\pi f_1t+\\varphi_1)+A_2\\cos(2\\times 2\\pi f_2t+\\varphi_2)+A_3\\cos(3\\times 2\\pi f_3t+\\varphi_3)$$ For simplicity, there we respectively select these values: $A_0=0, A_1=1, A_2=1, A_3=1$ $$ \\varphi_1=\\varphi_2=\\varphi_3=0 $$ Following above expression, we can get the generated analog signal: $$ x(t) = \\cos(2\\pi f_1t)+\\cos(4\\pi f_2t)+\\cos(6\\pi f_3t) $$ II. Sample the Analog SignalTime Domain Sampling TheoremAccording to the time domain sampling theorem, the sampling frequency must be greater than twice the signal cutoff frequency.Let’s assume that the sampling frequency is $F_s$, and the generated analog signal frequency satisfies: $F_1&lt;F_2&lt;F_3$, so the signal cutoff frequency is $F_c = F_3$. The sampling theorem is formally expressed as: $$F_s &gt; 2F_c$$ In this experiment，we respectively selected $F_1=10Hz, F_2=20Hz, F_3=30Hz$ to produce analog signal. So we can get the period and cutoff frequency of sampled signal: $$T_c = \\frac{1}{F_1}=0.1s, F_c = F_3 = 30Hz$$ Time-domain WindowFor periodic continuous signals, we intercept at integer multiples of the period to obtain a sequence for spectrum analysis. $$T_p=N*T_c, N\\in Z^+$$ Sampling FrequencyFor a specific sampling frequency, we can get the sampling period $T_s$, and the number of sampling points $N$: $$T_s = \\frac{1}{F_s}, N=T_p*F_c$$ Therefore, we use sampling frequency of $F_s=90Hz, F_s=60Hz, F_s=40Hz$ to get time-domain signals. Spectral ResolutionSpectral resolution is defined as the minimum separation between two signals of different frequencies: $$\\Delta f = \\frac{F_s}{N}=\\frac{1}{NT_s}=\\frac{1}{T_p}$$ III. Spectral AnalysisIn this section, we will analyse the Amplitude-Frequency Characteristics and Phase-Frequency Characteristics of the sampled signal. Convert to FrequencyWhen analysing the spectral, we need to convert the $0\\sim N-1$ to frequency sequence: $$f_k = k*\\frac{F_s}{N}, k=0,1,…N-1$$ Convert to Real AmplitudeAfter we apply Discrete Fourier Transform to the sampled signal, the frequency-domain signal is complex-valued. And due to the time-domain signal is real-valued, the the frequency-domain signal is conjugate symmetric: $$X(k) = X^*(N-k), k=0,1,…N-1$$ For complex values, that means its real part is even symmetric about the middle point, and its imaginary part is odd symmetric about the middle point. This will be showed in the following figures. ExperimentsExperiment I: $T_p=3T_c, F_s=90$Hz Samping Frequency $F_s = 3F_c(F_s &gt; 2F_c)$We use the sampling frequency of $F_s=90$Hz under the condition of $T_p=3T_c$.&lt;img src=&quot;https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/EBlog/Courses/Np3_90Hz_Sampling.qkarodnm2tc.png&quot; alt=&quot;Sampled Signal&quot; width=&quot;50%&quot;/&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/EBlog/Courses/Np3_90Hz_DFT.1oearbvg9zq8.webp&quot; alt=&quot;Spectral Analysis Graph&quot; width=&quot;50%&quot;/&gt; ConclusionsThe sampling frequency satisfies the Time Domain Sampling Theorem so we can see there is no overlap in frequency domain about the amplitude-frequency characteristic. And when $f=10$Hz, $f=20$Hz, $f=30$Hz, we can get the amplitude very close to $1$ which is us defined in analop signal. Experiment II: $T_p=3T_c, F_s=60$Hz Samping Frequency $F_s = 2F_c(F_s = 2F_c)$We use the sampling frequency of $F_s=60$Hz under the condition of $T_p=3T_c$.&lt;img src=&quot;https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/EBlog/Courses/Np3_60Hz_Sampling.6gd816c8cxg0.png&quot; alt=&quot;Sampled Signal&quot; width=&quot;50%&quot;/&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/EBlog/Courses/Np3_60Hz_DFT.57gxppdnrrg0.webp&quot; alt=&quot;Spectral Analysis Graph&quot; width=&quot;50%&quot;/&gt; ConclusionsThe sampling frequency equals the threhold of Time Domain Sampling Theorem so we can easily see that it will just become overlapping in frequency domain. And when $f=30$Hz that is also $F_s/2$ point, we can get this point very close to its symmetric frequency point. Experiment III: $T_p=3T_c, F_s=40$Hz Samping Frequency $F_s = \\frac{4}{3}F_c(F_s &lt; 2F_c)$We use the sampling frequency of $F_s=40$Hz under the condition of $T_p=3T_c$.&lt;img src=&quot;https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/EBlog/Courses/Np3_40Hz_Sampling.48kfqsokcha0.webp&quot; alt=&quot;Sampled Signal&quot; width=&quot;50%&quot;/&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/EBlog/Courses/Np3_40Hz_DFT.kf1tgvtvhgg.webp&quot; alt=&quot;Spectral Analysis Graph&quot; width=&quot;50%&quot;/&gt; ConclusionsThe sampling frequency do not equal the Time Domain Sampling Theorem so we can obviously see that it has discarded the third frequency $f=30$Hz, which is caused by overlapping in frequency domain. Note: in order to clearly analyse spectral of sampled signal, we also select the Time-domain Window of $T_p=50T_c$ to conduct experiments. ResultsExperiment I: $F_s=40$Hz Codes12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182%% Project Introduction:% This project is developed to design some signal filters based on digital% signal processing.clear, close all;%% Produce and sample digital signalf1 = 10;f2 = 20;f3 = 30; % so the fc = f3 = 30HzNp = 50; % number of periods for time-domain window%% Experiment 1 (Choosing samling frequency fs = 3fc (fs &gt; 2fs))fs1 = 90; % sampling frequencyxn1 = ProduceSamplingSignal(f1, f2, f3, fs1, Np, 'Sampling Analog Signal(fs = 3fc)');DFTAnalysis(xn1, fs1, 'Frequency Response Characteristics(fs = 3fc)');%% Experiment 2 (Choosing samling frequency fs = 2fc)fs2 = 60; % sampling frequencyxn2 = ProduceSamplingSignal(f1, f2, f3, fs2, Np, 'Sampling Analog Signal(fs = 2fc)');DFTAnalysis(xn2, fs2, 'Frequency Response Characteristics(fs = 2fc)');%% Experiment 3 (Choosing samling frequency fs &lt; 2fc)fs3 = 40; % sampling frequencyxn3 = ProduceSamplingSignal(f1, f2, f3, fs3, Np, 'Sampling Analog Signal(fs &lt; 2fc)');DFTAnalysis(xn3, fs3, 'Frequency Response Characteristics(fs &lt; 2fc)');%% Experiment Description% Experiment 4-7: Design a digital filter respectively with band pass, high% pass, low pass, band stop based on ellipord.%% Experiment 4: Design a digital filter with band pass using ellipordfpl = 15; fpu=25; fsl=13; fsu=28;rp = 1; rs = 40;ellipBandPass(fpl, fpu, fsl, fsu, rp, rs, xn1, fs1, f1, Np, 'Digital Filter With Band Pass Using Ellipord(fs = 3fc)');ellipBandPass(fpl, fpu, fsl, fsu, rp, rs, xn2, fs2, f1, Np, 'Digital Filter With Band Pass Using Ellipord(fs = 2fc)');ellipBandPass(8, 10, 6, 12, rp, rs, xn3, fs3, f1, Np, 'Digital Filter With Band Pass Using Ellipord(fs &lt; 2fc)');%% Experiment 5: Design a digital filter with high pass using ellipordfpz = 16; fsz = 13;rp = 1; rs = 40;ellipHighPass(fpz, fsz, rp, rs, xn1, fs1, f1, Np, 'Digital Filter With High Pass Using Ellipord(fs = 3fc)');ellipHighPass(fpz, fsz, rp, rs, xn2, fs2, f1, Np, 'Digital Filter With High Pass Using Ellipord(fs = 2fc)');ellipHighPass(15, 12, rp, rs, xn3, fs3, f1, Np, 'Digital Filter With High Pass Using Ellipord(fs &lt; 2fc)');%% Experiment 6: Design a digital filter with low pass using ellipordfpz = 23; fsz=28; rp = 1; rs = 40;ellipLowPass(fpz, fsz, rp, rs, xn1, fs1, f1, Np, 'Digital Filter With Low Pass Using Ellipord(fs = 3fc)');ellipLowPass(fpz, fsz, rp, rs, xn2, fs2, f1, Np, 'Digital Filter With Low Pass Using Ellipord(fs = 2fc)');ellipLowPass(12, 15, rp, rs, xn3, fs3, f1, Np, 'Digital Filter With Low Pass Using Ellipord(fs &lt; 2fc)');%% Experiment 7: Design a digital filter with band stop using ellipordfpl = 15; fpu=25; fsl=17; fsu=22;rp = 1; rs = 40;ellipBandStop(fpl, fpu, fsl, fsu, rp, rs, xn1, fs1, f1, Np, 'Digital Filter With Band Stop Using Ellipord(fs = 3fc)');ellipBandStop(fpl, fpu, fsl, fsu, rp, rs, xn2, fs2, f1, Np, 'Digital Filter With Band Stop Using Ellipord(fs = 2fc)');ellipBandStop(5, 17, 8, 12, rp, rs, xn3, fs3, f1, Np, 'Digital Filter With Band Stop Using Ellipord(fs &lt; 2fc)');%% Experiment Description% Experiment 8-11: Design a digital filter respectively with high pass, low% pass, band pass, band stop based on hamming window.%% Experiment 8: Design a digital filter with high pass using hamming windowfpz = 16; fsz = 13;firlHighPass(fpz, fsz, xn1, fs1, f1, Np, 'Digital Filter With High Pass Using Hamming Window(fs = 3fc)');firlHighPass(fpz, fsz, xn2, fs2, f1, Np, 'Digital Filter With High Pass Using Hamming Window(fs = 2fc)');firlHighPass(15, 12, xn3, fs3, f1, Np, 'Digital Filter With High Pass Using Hamming Window(fs &lt; 2fc)');%% Experiment 9: Design a digital filter with low pass using hamming windowfpz = 23; fsz = 28;firlLowPass(fpz, fsz, xn1, fs1, f1, Np, 'Digital Filter With Low Pass Using Hamming Window(fs = 3fc)');firlLowPass(fpz, fsz, xn2, fs2, f1, Np, 'Digital Filter With Low Pass Using Hamming Window(fs = 2fc)');firlLowPass(13, 17, xn3, fs3, f1, Np, 'Digital Filter With Low Pass Using Hamming Window(fs &lt; 2fc)');%% Experiment 10: Design a digital filter with band pass using hamming windowfpl = 15; fpu = 25;firlBandPass(fpl, fpu, xn1, fs1, f1, Np, 'Digital Filter With Band Pass Using Hamming Window(fs = 3fc)');firlBandPass(fpl, fpu, xn2, fs2, f1, Np, 'Digital Filter With Band Pass Using Hamming Window(fs = 2fc)');firlBandPass(7, 15, xn3, fs3, f1, Np, 'Digital Filter With Band Pass Using Hamming Window(fs &lt; 2fc)');%% Experiment 11: Design a digital filter with band stop using hamming windowfsl = 15; fsu = 25;firlBandStop(fsl, fsu, xn1, fs1, f1, Np, 'Digital Filter With Band Stop Using Hamming Window(fs = 3fc)');firlBandStop(fsl, fsu, xn2, fs2, f1, Np, 'Digital Filter With Band Stop Using Hamming Window(fs = 2fc)');firlBandStop(7, 15, xn3, fs3, f1, Np, 'Digital Filter With Band Band Stop Hamming Window(fs &lt; 2fc)'); 1234567891011121314151617181920212223242526272829303132333435363738394041function xn = ProduceSamplingSignal(f1, f2, f3, fs, Np, Alltitle)% Function Description: % We want to make a digital signal composed of three frequency% components and sample the produced signal.% Inputs: % f1, f2, f3: means our selected frequency components, fs% represents the sampling frequency.% Np: means the number of periods.% Outputs:% xn: represents the sampled signal. period = 1/f1; % the period of analog signal(assuming f1 is the minimal) T = Np*period; % sampling time-domain window(several periods) Ts = 1 / fs; % sampling timestep t = 0: Ts : T; % samping sequence of discrete sampling points % t = 0: 0.0001: T; % analog time sequence % Step I: Produce digital signal xt = cos(2*pi*f1*t) + cos(2*pi*f2*t) + cos(2*pi*f3*t); % Step II: Sample produced signal xn = cos(2*pi*f1*t) + cos(2*pi*f2*t) + cos(2*pi*f3*t); % Step III: Visualize produced signal and sampled signal figure('Position', [210, 80, 950, 750]); subplot(2, 1, 1); plot(t, xt); title('Time-domain signal $x(t)$', 'Interpreter', 'latex', 'FontSize', 12); xlabel('$t/s$', 'Interpreter', 'latex', 'FontSize', 12); ylabel('Amplitude', 'Interpreter', 'latex', 'FontSize', 12); ylim([-2.5, 3.5]); grid on subplot(2, 1, 2); stem(t, xn); title('Time-domain sampled signal $x(n)$', 'Interpreter', 'latex', 'FontSize', 12); ylabel('Amplitude', 'Interpreter', 'latex', 'FontSize', 12); xlabel('$t/s$', 'Interpreter', 'latex', 'FontSize', 12); ylim([-2.5, 3.5]); grid on sgtitle(Alltitle, 'FontName', 'Times New Roman', 'FontSize', 14);end 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455function DFTAnalysis(xn, fs, Alltitle)% Function Description:% This function calculates the DFT[x(n)] and do spectral analysis.% Inputs:% xn: digital discrete signal% fs: sampling frequency% Outputs:% No return N = length(xn); % number of sampling points df = fs / N; % spectral resolution f1 = (0:N-1)*df; % tranverse to the frequncy sequence f2 = 2*(0:N-1)/N; % DFT using FFT algorithm Xk = fft(xn, N); % Tranverse to the real amplitude RM = 2*abs(Xk)/N; Angle = angle(Xk); figure('Position', [210, 80, 950, 750]); % Amplitude-Frequency Characteristics subplot(4,1,1); stem(f1, RM,'.'); title('Amplitude-Frequency Characteristics', 'Interpreter', 'latex', 'FontSize', 12); xlabel('$f$/Hz', 'Interpreter', 'latex', 'FontSize', 12); ylabel('Amplitude', 'Interpreter', 'latex', 'FontSize', 12); grid on; % Phase-Frequency Characteristics subplot(4,1,2); stem(f1, Angle,'.'); line([(N-1)*df, 0],[0,0]); title('Phase-Frequency Characteristics', 'Interpreter', 'latex', 'FontSize', 12); xlabel('$f$/Hz', 'Interpreter', 'latex', 'FontSize', 12); ylabel('Phase', 'Interpreter', 'latex', 'FontSize', 12); grid on; % Amplitude-Frequency Characteristics subplot(4,1,3); plot(f2, abs(Xk)); title('Amplitude-Frequency Characteristics', 'Interpreter', 'latex', 'FontSize', 12); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('Amplitude', 'Interpreter', 'latex', 'FontSize', 12); grid on; % Phase-Frequency Characteristics subplot(4,1,4); plot(f2, Angle); title('Phase-Frequency Characteristics', 'Interpreter', 'latex', 'FontSize', 12); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('Phase', 'Interpreter', 'latex', 'FontSize', 12); ylim([-3.5, 3.5]); grid on; sgtitle(Alltitle, 'FontName', 'Times New Roman', 'FontSize', 14);end 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152function ellipBandPass(fpl, fpu, fsl, fsu, rp, rs, x, fs, f1, Np, Alltitle) wp = [2*fpl/fs, 2*fpu/fs]; ws = [2*fsl/fs, 2*fsu/fs]; [N, wn] = ellipord(wp, ws, rp, rs); % 获取阶数和截止频率 [B, A] = ellip(N, rp, rs, wn, 'bandpass'); % 获得转移函数系数 filter_bp_s = filter(B, A, x); X_bp_s = abs(fft(filter_bp_s)); X_bp_s_angle = angle(fft(filter_bp_s)); % plot the graphs period = 1/f1; % the period of analog signal(assuming f1 is the minimal) T = Np*period; % sampling time-domain window(several periods) Ts = 1 / fs; % sampling timestep t = 0: Ts : T; % samping sequence of discrete sampling points N = length(x); % number of sampling points f = 2*(0:N-1)/N; % 带通滤波器频谱特性 figure('Position', [210, 80, 950, 750]); subplot(4,4,[1,2,5,6]); M = 512; wk = 0:pi/M:pi; Hz = freqz(B,A,wk); plot(wk/pi, 20*log10(abs(Hz))); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('$20lg|Hg(\\omega)|$', 'Interpreter', 'latex', 'FontSize', 12); title('带通滤波器频谱特性'); axis([0.2,0.9,-80,20]);set(gca,'Xtick',0:0.1:1,'Ytick',-80:20:20); grid on; subplot(4,4,[3,4,7,8]); plot(t, filter_bp_s); xlabel('t/s', 'Interpreter', 'latex', 'FontSize', 12); ylabel('Amplitude', 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[9, 10, 11, 12]); plot(f, X_bp_s); title('带通滤波后频域幅度特性'); ylabel('Amplitude', 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[13, 14, 15, 16]); plot(f, X_bp_s_angle); title('带通滤波后频域相位特性'); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('Phase', 'Interpreter', 'latex', 'FontSize', 12); ylim([-3.5, 3.5]); grid on; sgtitle(Alltitle, 'FontName', 'Times New Roman', 'FontSize', 14);end 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354function ellipHighPass(fpz, fsz, rp, rs, x, fs, f1, Np, Alltitle) wpz = 2*fpz/fs; wsz = 2*fsz/fs; [N, wn] = ellipord(wpz, wsz, rp, rs); % 获取阶数和截止频率 [B, A] = ellip(N, rp, rs, wn, 'high'); % 获得转移函数系数 filter_hp_s = filter(B, A, x); X_hp_s = abs(fft(filter_hp_s)); X_hp_s_angle = angle(fft(filter_hp_s)); % plot the graphs period = 1/f1; % the period of analog signal(assuming f1 is the minimal) T = Np*period; % sampling time-domain window(several periods) Ts = 1 / fs; % sampling timestep t = 0: Ts : T; % samping sequence of discrete sampling points N = length(x); % number of sampling points f = 2*(0:N-1)/N; % 高通滤波器频谱特性 figure('Position', [210, 80, 950, 750]); subplot(4,4,[1,2,5,6]); M = 512; wk = 0:pi/M:pi; Hz = freqz(B,A,wk); plot(wk/pi, 20*log10(abs(Hz))); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('$20lg|Hg(\\omega)|$', 'Interpreter', 'latex', 'FontSize', 12); title('高通滤波器频谱特性'); axis([0.2,0.8,-80,20]); set(gca,'Xtick',0:0.1:1,'Ytick',-80:20:20); grid on; subplot(4,4,[3,4,7,8]); plot(t, filter_hp_s); title('高通滤波后时域图形'); xlabel('t/s', 'Interpreter', 'latex', 'FontSize', 12); ylabel('Amplitude', 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[9, 10, 11, 12]); plot(f, X_hp_s); title('高通滤波后频域幅度特性'); ylabel('Amplitude', 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[13, 14, 15, 16]); plot(f, X_hp_s_angle); title('高通滤波后频域相位特性'); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('Phase', 'Interpreter', 'latex', 'FontSize', 12); ylim([-3.5, 3.5]); grid on; sgtitle(Alltitle, 'FontName', 'Times New Roman', 'FontSize', 14);end 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354function ellipLowPass(fpz, fsz, rp, rs, x, fs, f1, Np, Alltitle) wpz = 2*fpz/fs; wsz = 2*fsz/fs; [N, wn] = ellipord(wpz, wsz, rp, rs); % 获取阶数和截止频率 [B, A] = ellip(N, rp, rs, wn, 'low'); % 获得转移函数系数 filter_hp_s = filter(B, A, x); X_hp_s = abs(fft(filter_hp_s)); X_hp_s_angle = angle(fft(filter_hp_s)); % plot the graphs period = 1/f1; % the period of analog signal(assuming f1 is the minimal) T = Np*period; % sampling time-domain window(several periods) Ts = 1 / fs; % sampling timestep t = 0: Ts : T; % samping sequence of discrete sampling points N = length(x); % number of sampling points f = 2*(0:N-1)/N; % 低通滤波器频谱特性 figure('Position', [210, 80, 950, 750]); subplot(4,4,[1,2,5,6]); M = 512; wk = 0:pi/M:pi; Hz = freqz(B,A,wk); plot(wk/pi, 20 * log10(abs(Hz))); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('$20lg|Hg(\\omega)|$', 'Interpreter', 'latex', 'FontSize', 12); title('低通滤波器频谱特性'); axis([0.2,0.9,-80,20]); set(gca,'Xtick',0:0.1:1,'Ytick',-80:20:20) grid on; subplot(4,4,[3,4,7,8]); plot(t, filter_hp_s); title('低通滤波后时域图形'); xlabel('t/s', 'Interpreter', 'latex', 'FontSize', 12); ylabel('Amplitude', 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[9, 10, 11, 12]); plot(f, X_hp_s); title('低通滤波后频域幅度特性'); ylabel('Amplitude', 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[13, 14, 15, 16]); plot(f, X_hp_s_angle); title('低通滤波后频域相位特性'); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('Phase', 'Interpreter', 'latex', 'FontSize', 12); ylim([-3.5, 3.5]); grid on; sgtitle(Alltitle, 'FontName', 'Times New Roman', 'FontSize', 14);end 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354function ellipBandStop(fpl, fpu, fsl, fsu, rp, rs, x, fs, f1, Np, Alltitle) wp = [2*fpl/fs, 2*fpu/fs]; ws = [2*fsl/fs, 2*fsu/fs]; [N, wn] = ellipord(wp, ws, rp, rs); % 获取阶数和截止频率 [B, A] = ellip(N, rp, rs, wn, 'stop'); % 获得转移函数系数 filter_bp_s = filter(B, A, x); X_bp_s = abs(fft(filter_bp_s)); X_bp_s_angle = angle(fft(filter_bp_s)); % plot the graphs period = 1/f1; % the period of analog signal(assuming f1 is the minimal) T = Np*period; % sampling time-domain window(several periods) Ts = 1 / fs; % sampling timestep t = 0: Ts : T; % samping sequence of discrete sampling points N = length(x); % number of sampling points f = 2*(0:N-1)/N; % 带阻滤波器频谱特性 figure('Position', [210, 80, 950, 750]); subplot(4,4,[1,2,5,6]); M = 512; wk = 0:pi/M:pi; Hz = freqz(B,A,wk); plot(wk/pi, 20*log10(abs(Hz))); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('$20lg|Hg(\\omega)|$', 'Interpreter', 'latex', 'FontSize', 12); title('带阻滤波器频谱特性'); axis([0.2,0.9,-80,20]); set(gca,'Xtick',0:0.1:1,'Ytick',-80:20:20); grid on; subplot(4,4,[3,4,7,8]); plot(t, filter_bp_s); title('带阻滤波后时域图形'); xlabel('t/s', 'Interpreter', 'latex', 'FontSize', 12); ylabel('Amplitude', 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[9, 10, 11, 12]); plot(f, X_bp_s); title('带阻滤波后频域幅度特性'); ylabel('Amplitude', 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[13, 14, 15, 16]); plot(f, X_bp_s_angle); title('带阻滤波后频域相位特性'); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('Phase', 'Interpreter', 'latex', 'FontSize', 12); ylim([-3.5, 3.5]); grid on; sgtitle(Alltitle, 'FontName', 'Times New Roman', 'FontSize', 14);end 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162function firlHighPass(fpz, fsz, x, fs, f1, Np, Alltitle) wpz = 2 * pi * fpz / fs; wsz = 2 * pi * fsz / fs; DB = wpz - wsz; % 计算过渡带宽度 N0 = ceil(6.2 * pi / DB); % 计算所需h(n)长度N0 N = N0 + mod(N0 + 1, 2); % 确保h(n)长度N是奇数 wc = (wpz + wsz) /2 / pi; % 计算理想高通滤波器通带截止频率 hn = fir1(N-1, wc, 'high', hamming(N)); filter_hp_s = filter(hn, 1, x); X_hp_s = abs(fft(filter_hp_s)); X_hp_s_angle = angle(fft(filter_hp_s)); % plot the graphs period = 1/f1; % the period of analog signal(assuming f1 is the minimal) T = Np*period; % sampling time-domain window(several periods) Ts = 1 / fs; % sampling timestep t = 0: Ts : T; % samping sequence of discrete sampling points N = length(x); % number of sampling points f = 2*(0:N-1)/N; figure('Position', [210, 80, 950, 750]); subplot(4,4,[1,2,5,6]); M = 1024; k = 1:M / 2; wk = 2*(0:M/2-1)/M; Hz = freqz(hn, 1); plot(wk, 20*log10(abs(Hz(k)))); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('$20lg|Hg(\\omega)|$', 'Interpreter', 'latex', 'FontSize', 12); title('高通滤波器频谱特性') axis([0.2,0.8,-80,20]); set(gca,'Xtick',0:0.1:1,'Ytick',-80:20:20) grid on; subplot(4,4,[3,4,7,8]); plot(t, filter_hp_s); title('高通滤波后时域图形'); txt = xlabel('t/s', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); txt = ylabel('Amplitude', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[9, 10, 11, 12]); plot(f, X_hp_s); title('高通滤波后频域幅度特性'); txt = ylabel('Amplitude', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[13, 14, 15, 16]); plot(f, X_hp_s_angle); title('高通滤波后频域相位特性'); txt = ylabel('Phase', 'FontSize', 12); ylim([-3.5, 3.5]); xlabel('\\omega/\\pi', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); grid on; sgtitle(Alltitle, 'FontName', 'Times New Roman', 'FontSize', 14);end 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162function firlLowPass(fpz, fsz, x, fs, f1, Np, Alltitle) wpz = 2 * pi * fpz / fs; wsz = 2 * pi * fsz / fs; DB = wsz - wpz; N0 = ceil(6.2 * pi / DB); N = N0 + mod(N0 + 1, 2); wc = (wpz + wsz) / 2 / pi; hn = fir1(N-1, wc, 'low', hamming(N)); filter_hp_s = filter(hn, 1, x); X_hp_s = abs(fft(filter_hp_s)); X_hp_s_angle = angle(fft(filter_hp_s)); % plot the graphs period = 1/f1; % the period of analog signal(assuming f1 is the minimal) T = Np*period; % sampling time-domain window(several periods) Ts = 1 / fs; % sampling timestep t = 0: Ts : T; % samping sequence of discrete sampling points N = length(x); % number of sampling points f = 2*(0:N-1)/N; figure('Position', [210, 80, 950, 750]); subplot(4,4,[1,2,5,6]); M = 1024; k = 1:M / 2; wk = 2*(0:M/2-1)/M; Hz = freqz(hn, 1); plot(wk, 20*log10(abs(Hz(k)))); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('$20lg|Hg(\\omega)|$', 'Interpreter', 'latex', 'FontSize', 12); title('低通滤波器频谱特性') axis([0.2,0.9,-80,20]); set(gca,'Xtick',0:0.1:1,'Ytick',-80:20:20) grid on; subplot(4,4,[3,4,7,8]); plot(t, filter_hp_s); title('低通滤波后时域图形'); txt = xlabel('t/s', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); txt = ylabel('Amplitude', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[9, 10, 11, 12]); plot(f, X_hp_s); title('低通滤波后频域幅度特性'); txt = ylabel('Amplitude', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[13, 14, 15, 16]); plot(f, X_hp_s_angle); title('低通滤波后频域相位特性'); txt = ylabel('Phase', 'FontSize', 12); ylim([-3.5, 3.5]); xlabel('\\omega/\\pi', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); grid on; sgtitle(Alltitle, 'FontName', 'Times New Roman', 'FontSize', 14);end 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859function firlBandPass(fpl, fpu, x, fs, f1, Np, Alltitle) wpl = 2 * fpl / fs; wpu = 2 * fpu / fs; fpass = [wpl, wpu]; N = 111; hn = fir1(N-1, fpass, 'bandpass', hamming(N)); filter_hp_s = filter(hn, 1, x); X_hp_s = abs(fft(filter_hp_s)); X_hp_s_angle = angle(fft(filter_hp_s)); % plot the graphs period = 1/f1; % the period of analog signal(assuming f1 is the minimal) T = Np*period; % sampling time-domain window(several periods) Ts = 1 / fs; % sampling timestep t = 0: Ts : T; % samping sequence of discrete sampling points N = length(x); % number of sampling points f = 2*(0:N-1)/N; figure('Position', [210, 80, 950, 750]); subplot(4,4,[1,2,5,6]); M = 1024; k = 1:M / 2; wk = 2*(0:M/2-1)/M; Hz = freqz(hn, 1); plot(wk, 20*log10(abs(Hz(k)))); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('$20lg|Hg(\\omega)|$', 'Interpreter', 'latex', 'FontSize', 12); title('带通滤波器频谱特性') axis([0.2,0.9,-80,20]); set(gca,'Xtick',0:0.1:1,'Ytick',-80:20:20) grid on; subplot(4,4,[3,4,7,8]); plot(t, filter_hp_s); title('带通滤波后时域图形'); txt = xlabel('t/s', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); txt = ylabel('Amplitude', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[9, 10, 11, 12]); plot(f, X_hp_s); title('带通滤波后频域幅度特性'); txt = ylabel('Amplitude', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[13, 14, 15, 16]); plot(f, X_hp_s_angle); title('带通滤波后频域相位特性'); txt = ylabel('Phase', 'FontSize', 12); ylim([-3.5, 3.5]); xlabel('\\omega/\\pi', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); grid on; sgtitle(Alltitle, 'FontName', 'Times New Roman', 'FontSize', 14);end 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859function firlBandStop(fsl, fsu, x, fs, f1, Np, Alltitle) wsl = 2 * fsl / fs; wsu = 2 * fsu / fs; fstop = [wsl, wsu]; N = 111; hn = fir1(N-1, fstop, 'stop', hamming(N)); filter_hp_s = filter(hn, 1, x); X_hp_s = abs(fft(filter_hp_s)); X_hp_s_angle = angle(fft(filter_hp_s)); % plot the graphs period = 1/f1; % the period of analog signal(assuming f1 is the minimal) T = Np*period; % sampling time-domain window(several periods) Ts = 1 / fs; % sampling timestep t = 0: Ts : T; % samping sequence of discrete sampling points N = length(x); % number of sampling points f = 2*(0:N-1)/N; figure('Position', [210, 80, 950, 750]); subplot(4,4,[1,2,5,6]); M = 1024; k = 1:M / 2; wk = 2*(0:M/2-1)/M; Hz = freqz(hn, 1); plot(wk, 20*log10(abs(Hz(k)))); xlabel('\\omega/\\pi', 'FontSize', 12); ylabel('$20lg|Hg(\\omega)|$', 'Interpreter', 'latex', 'FontSize', 12); title('带阻滤波器频谱特性') axis([0.2,0.9,-80,20]); set(gca,'Xtick',0:0.1:1,'Ytick',-80:20:20) grid on; subplot(4,4,[3,4,7,8]); plot(t, filter_hp_s); title('带阻滤波后时域图形'); txt = xlabel('t/s', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); txt = ylabel('Amplitude', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[9, 10, 11, 12]); plot(f, X_hp_s); title('带阻滤波后频域幅度特性'); txt = ylabel('Amplitude', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); grid on; subplot(4,4,[13, 14, 15, 16]); plot(f, X_hp_s_angle); title('带阻滤波后频域相位特性'); txt = ylabel('Phase', 'FontSize', 12); ylim([-3.5, 3.5]); xlabel('\\omega/\\pi', 'FontSize', 12); set(txt, 'Interpreter', 'latex', 'FontSize', 12); grid on; sgtitle(Alltitle, 'FontName', 'Times New Roman', 'FontSize', 14);end Contributors Zhihao Li","link":"/collaboration/DigitalSignalProcessing/"},{"title":"大学物理","text":"第一章 静电场第一讲 库仑定律库仑定理——（真空静止点电荷）$$F = \\frac{1}{4\\pi\\varepsilon_0}\\frac{q_1q_2}{r^2}\\boldsymbol{r}^0\\tag 1$$ 其中真空介电常数 $\\varepsilon_0 \\approx 8.85\\times10^{-12} C^2N^{-1}m^{-2}$，令 $k=\\frac{1}{4\\pi\\varepsilon_0}$ 则 $k\\approx 9\\times 10^9Nm^2/C^2$，矢量 $\\boldsymbol{r}^0$ 由施力电荷指向受力电荷 第二讲 电场强度$E$2.1 电场强度$$E = \\frac{F}{q_0}=\\frac{1}{4\\pi\\varepsilon_0}\\frac{q}{r^2}\\boldsymbol{r}^0\\tag 2$$ 2.2 均匀带电细圆环圆环轴线上一点 $P$ 的电场强度： $$E = \\frac{1}{4\\pi\\varepsilon_0}\\frac{qx}{(R^2+x^2)^\\frac{3}{2}}\\tag 3$$ 其中，$x$ 表示 $P$ 点到圆环中心 $O$ 的距离，$R$ 表示圆环半径，$q$ 表示圆环带电量； 2.3 有限长直线段直线外一点 $P$ 电场强度： $$E_x=\\frac{\\lambda}{4\\pi\\varepsilon_0 a}(cos\\theta_1-cos\\theta_2), E_y=\\frac{\\lambda}{4\\pi\\varepsilon_0 a}(sin\\theta_2-sin\\theta_1)\\tag 4$$ 注：在建立坐标系的情况下，上式均带有方向，其中沿 $y$ 轴正向：$\\theta_1\\rightarrow \\theta_2$，$\\theta$ 为与 $y$ 轴正向夹角；其中，$a$ 表示 $P$ 点到直线的垂直距离； 2.4 均匀带电无限长直线由 $2.4$ 推得：令$\\theta_1=0,\\theta_2=\\pi$ $$E_x = \\frac{\\lambda}{2\\pi\\varepsilon_0 a}, E_y = 0\\tag 5$$ 2.5 均匀带电无限大平面$$E=\\frac{\\sigma}{2\\varepsilon_0} \\tag 6$$ 2.6 无限大均匀带异号电荷平板间$$E=\\frac{\\sigma}{\\varepsilon_0}\\tag 7$$ 其中，$\\sigma$ 表示每个平板的电荷面密度； 2.7 电偶极子电偶极矩：$\\boldsymbol{p}=q\\boldsymbol{l}$中垂线上一点$P$场强： $$E = -\\frac{\\boldsymbol{p}}{4\\pi\\varepsilon_0y^3} (y\\gg l)\\tag 8$$ 共线上一点 $P$ 场强： $$E=\\frac{2\\boldsymbol{p}}{4\\pi\\varepsilon_0x^3}(x\\gg l)\\tag 9$$ 其中 $\\boldsymbol{l}$ 方向由负电荷指向正电荷； 2.8 力偶矩电偶极子在匀强电场中得力偶矩： $\\boldsymbol{F}_+=q\\boldsymbol{E},\\boldsymbol{F}_-=-q\\boldsymbol{E}$ $$M = F_+\\cdot\\frac{1}{2}lsin\\theta+F_-\\cdot\\frac{1}{2}lsin\\theta=qlEsin\\theta\\tag{10} $$ $\\Rightarrow\\boldsymbol{M}=q\\boldsymbol{l}\\times\\boldsymbol{E}=\\boldsymbol{p}\\times\\boldsymbol{E}$ 注：电偶极子在电场的作用下总要使 $\\boldsymbol{p}$ 转向 $\\boldsymbol{E}$ 的方向； 第三讲 电通量 $\\bigstar$高斯定理3.1 电通量$$\\Phi_e=\\oint_S\\boldsymbol{E}\\cdot d\\boldsymbol{S}\\tag{11}$$ 3.2 高斯定理选定高斯面后，电通量： $$\\Phi_e=\\oint_S\\boldsymbol{E}\\cdot d\\boldsymbol{S}=\\frac{1}{\\varepsilon_0}\\sum_{(内)}q_i\\tag{12}$$ 3.3 轴对称性电场无限长均匀带电直线外一点 $P$ 场强： $$\\Phi_e=\\boldsymbol{E}\\oint_侧d\\boldsymbol{S}=2\\pi rEl=\\frac{1}{\\varepsilon_0}\\lambda l\\Rightarrow E = \\frac{\\lambda}{2\\pi\\varepsilon_0r}\\tag{13}$$ 其中，$r$表示 $P$ 距离导线垂直距离； 3.4 球面对称性电场均匀带电球面电场分布： $$\\Phi_e=\\boldsymbol{E}\\oint_S\\boldsymbol{S}=E\\cdot 4\\pi r^2=\\sum_{(内)}q_i=q$$ $$\\Rightarrow E=\\frac{1}{4\\pi \\varepsilon_0}\\frac{q}{r^2}\\boldsymbol{r}^0(r&gt;R)\\tag{14}$$ $$\\Rightarrow E=0(r&lt;R)$$ 3.5 无限大均匀带电平面选定圆柱面作为高斯面： $$\\Phi_e=\\oint_{左端面}\\boldsymbol{E}\\cdot d\\boldsymbol{S}+\\oint_{右端面}\\boldsymbol{E}\\cdot d\\boldsymbol{S}=2ES=\\frac{1}{\\varepsilon_0}\\sigma S\\tag{15}$$ $\\Rightarrow E=\\frac{\\sigma}{2\\varepsilon_0}$ 3.6 均匀带电圆盘$$E = \\frac{\\sigma}{2\\varepsilon_0}(1-\\frac{x}{\\sqrt{R^2+x^2}})\\tag{16}$$ 3.7 均匀带电球体$$E=\\frac{Q}{4\\pi\\varepsilon_0r^2}\\boldsymbol{r_0}(r&gt;R)$$ $$E=\\frac{\\rho}{3\\varepsilon_0}\\boldsymbol{r}(r&lt;R)$$ 第四讲 静电场的环路定理 电势能4.1 电场强度环流$$\\oint\\boldsymbol{E}\\cdot d\\boldsymbol{l}=0\\tag{17}$$ 环路定理表明静电场是无旋有源场； 4.2 电势能选定电势能零参考点，则点 $A$ 处的电势能： $$w_a=A_{a’0’}=\\int_a^{‘0’}q_0\\boldsymbol{E}\\cdot d\\boldsymbol{l}\\tag{18}$$ 注：电势能是标量，相对于电势能零参考点有负值； 第五讲 电势 电势差5.1 电势与电势差$A$点电势： $$u_a=\\frac{W_a}{q_0}=\\int_a^{‘0’}\\boldsymbol{E}\\cdot d\\boldsymbol{l}\\tag{19}$$ 注：电势为标量； $$U_{ab}=u_a-u_b=\\int_a^b\\boldsymbol{E}\\cdot d\\boldsymbol{l}\\tag{20}$$ 电荷$q$$a\\rightarrow b$时，静电力做功： $$A_{ab}=q(u_a-u_b)\\tag{21}$$ 5.2 电偶极子电势能在电场 $\\boldsymbol{E}$ 中： $$W=-\\boldsymbol{p}\\cdot\\boldsymbol{E}\\tag{22}$$ 当$\\boldsymbol{E}$为非均匀电场时，上式应改为积分形式；在电场中做功： 方法一 $$W_{\\theta_1\\theta_2}=-\\boldsymbol{p}\\cdot\\boldsymbol{E}(\\theta_1)-(-\\boldsymbol{p}\\cdot\\boldsymbol{E}(\\theta_2))$$ 方法二 $$W_{\\theta_1\\theta_2}=\\int_{\\theta_1}^{\\theta_2}-\\boldsymbol{p}\\times\\boldsymbol{E}d\\theta$$ 5.3 电势叠加原理对于点电荷选取无穷远处作为零电势点： $$u_a=\\int_a^{\\infty}\\boldsymbol{E}\\cdot d\\boldsymbol{l}=\\frac{1}{4\\pi\\varepsilon_0}\\frac{q}{r}\\W_a = \\frac{1}{4\\pi\\varepsilon_0}\\frac{q^2}{r}\\tag{23}$$ 叠加原理——标量叠加 $$u_a=\\sum u_i\\\\Rightarrow u_a=\\int_Q\\frac{1}{4\\pi\\varepsilon_0}\\frac{dq}{r}\\tag{24}$$ 5.4 电荷分布求电势积分形式： $$u_a=\\int_Q\\frac{1}{4\\pi\\varepsilon_0}\\frac{dq}{r}\\tag{25}$$ 电偶极子外任一点$C$的电势： $$U_C = \\frac{1}{4\\pi\\varepsilon_0}\\frac{q}{r_+}-\\frac{1}{4\\pi\\varepsilon_0}\\frac{q}{r_-}=\\frac{q}{4\\pi\\varepsilon_0}\\frac{r_–r_+}{r_-r_+}$$ $$r\\gg l\\Rightarrow r_+r_-\\approx r^2,r_–r_+\\approx lcos\\theta\\tag{26}$$ $$\\Rightarrow u_C = \\frac{1}{4\\pi\\varepsilon_0}\\frac{\\boldsymbol{p}\\cdot\\boldsymbol{r}}{r^3}$$ 5.5 电场强度求电势场强与电势关系： $$u_a=\\int_a^{\\infty}\\boldsymbol{E}\\cdot d\\boldsymbol{l}\\tag{27}$$ 带电体电荷分布具有对称性时，利用高斯定理求出场强分布进而求电势；【无限长均匀带电圆柱面】由高斯定理求得电场分布： $E = 0 (r\\leq R)$ $E=\\frac{\\lambda}{2\\pi\\varepsilon_0r}(r>R)$ 一般而言，当电荷分布延伸到无穷远时，是不能选取无穷远处为电势零参考点的； $$u_P=\\int_P^{P_0}\\boldsymbol{E}\\cdot d\\boldsymbol{l}=\\int_P^{P’}\\boldsymbol{E}\\cdot d\\boldsymbol{l}+\\int_{P’}^{P_0}\\boldsymbol{E}\\cdot d\\boldsymbol{l}$$ $$=0+\\int_r^{r_0}\\frac{\\lambda}{2\\pi\\varepsilon_0r}dr=-\\frac{\\lambda}{2\\pi\\varepsilon_0}\\ln r+\\frac{\\lambda}{2\\pi\\varepsilon_0}\\ln r_0\\tag{29}$$ $$=-\\frac{\\lambda}{2\\pi\\varepsilon_0}\\ln r+C(r&gt;R)$$ $$u_P=\\int_P^{P_0}\\boldsymbol{E}\\cdot d\\boldsymbol{l}=\\int_r^R\\boldsymbol{E}\\cdot d\\boldsymbol{l}+\\int_R^{r_0}\\boldsymbol{E}\\cdot d\\boldsymbol{l}$$ $$=0+\\int_R^{r_0}\\frac{\\lambda}{2\\pi\\varepsilon_0r}dr\\tag{30}$$ $$=-\\frac{\\lambda}{2\\pi\\varepsilon_0}\\ln R + C(r&lt;R)$$ 其中，$C=\\frac{\\lambda}{2\\pi\\varepsilon_0}\\ln r_0$ 5.6 均匀带电球面电势$V(r) = \\frac{1}{4\\pi\\varepsilon_0} \\frac{q}{R}(r \\leq R)$ $V(r) = \\frac{1}{4\\pi\\varepsilon_0}\\frac{q}{r}(r>R)$ 5.7 均匀带电球体电势球内距离球心$r$处一点$P$电势： $$u = u_1+u_2=\\frac{1}{4\\pi\\varepsilon_0}\\frac{Q}{R^3}r^2+\\int_r^R\\frac{1}{4\\pi\\varepsilon_0}\\frac{dq_2}{r’}$$ $$=\\frac{1}{4\\pi\\varepsilon_0}\\frac{Q}{R^3}r^2+\\int_r^R\\frac{3Qr’}{4\\pi\\varepsilon_0R^3}dr’\\tag{32}$$ $$=\\frac{Q(3R^2-r^2)}{8\\pi\\varepsilon_0R^3}(r&lt;R)$$ 球外距离球心 $r$ 处一点 $P$ 电势： $$u = \\frac{Q}{4\\pi\\varepsilon_0r}(r\\ge R)\\tag{33}$$ 注：在 $P$ 点的电场强度犹如电荷集中在球心处的点电荷在 $P$ 点产生的电场强度一样，故电势同理； 第六讲 电势与场强微分关系$$E = -\\frac{du}{dn}, E_l=-\\frac{du}{dl}$$ $$\\boldsymbol{E}=-(\\frac{\\partial u}{\\partial x}\\boldsymbol{i}+\\frac{\\partial u}{\\partial y}\\boldsymbol{j}+\\frac{\\partial u}{\\partial z}\\boldsymbol{k}), u(x,y,z)\\Rightarrow E(x,y,z)\\tag{34}$$ 第七讲 静电场中的导体 电容7.1 静电平衡导体表面电场强度： $$\\boldsymbol{E}=\\frac{\\sigma}{\\varepsilon_0}\\boldsymbol{n}\\tag{35}$$ 区别于无限大带电平面产生的电场(缺少静电平衡的条件)： $$\\boldsymbol{E}=\\frac{\\sigma}{2\\varepsilon_0}\\boldsymbol{n}\\tag{36}$$ 7.2 孤立导体电容$$C = \\frac{q}{u}\\tag{37}$$ 7.3 平行板电容器电容$$C = \\frac{q}{u_1-u_2}\\=\\frac{q}{Ed}=\\frac{q}{\\frac{\\sigma}{\\varepsilon_0}d}\\=\\frac{q}{\\frac{qd}{\\varepsilon_0S}}=\\frac{\\varepsilon_0S}{d}\\tag{38}$$ 7.4 球形电容器电容两球面间电场强度： $$E=\\frac{1}{4\\pi\\varepsilon_0}\\frac{q}{r^2}\\tag{39}$$ $$u_1-u_2=\\int_{R_1}^{R_2}\\boldsymbol{E}\\cdot d\\boldsymbol{l} = \\int_{R_1}^{R_2}\\frac{1}{4\\pi\\varepsilon_0}\\frac{q}{r^2}dr$$ $$=\\frac{q}{4\\pi\\varepsilon_0}\\frac{R_2-R_1}{R_1R_2}\\tag{40}$$ $$\\Rightarrow C = \\frac{q}{u_1-u_2}=\\frac{4\\pi\\varepsilon_0R_1R_2}{R_2-R_1}$$ 7.5 电容器串并联 串联 $$\\frac{1}{C}=\\frac{1}{C_1}+\\frac{1}{C_2}+\\cdot\\cdot\\cdot+\\frac{1}{C_n}$$ 并联 $$C = C_1+C_2+\\cdot\\cdot\\cdot+C_n\\tag{41}$$ 第八讲 静电能8.1 静电能公式推导$$U(t) = \\frac{q(t)}{C}, dA = U(t)dq = \\frac{q(t)}{C}dq$$ $$A = \\int dA = \\int_0^Q\\frac{q(t)}{C}dq=\\frac{Q^2}{2C}Q=CU\\tag{42}$$ $$\\Longrightarrow A = \\frac{1}{2}CU^2=\\frac{1}{2}QU\\Rightarrow W=A=\\frac{Q^2}{2C}=\\frac{1}{2}CU^2=\\frac{1}{2}QU$$ 8.2 电场能量密度推导$$U=Ed, C = \\frac{\\varepsilon_0S}{d}$$ $$\\Rightarrow W = \\frac{1}{2}\\varepsilon_0E^2Sd = \\frac{1}{2}\\varepsilon_0E^2V\\tag{43}$$ $$\\Rightarrow \\omega = \\frac{W}{V}=\\frac{1}{2}\\varepsilon_0E^2$$ 第九讲 电介质的极化 束缚电荷9.1 电介质$$C = \\varepsilon_r C_0\\tag{44}$$ 其中，$\\varepsilon_r$ 称为介质的相对介电常数（相对电容率），$C_0$ 表示真空中对应的电容；因此，除真空中 $\\varepsilon_r=1$ 外，其余 $\\varepsilon_r&gt;1$； 9.2 介质极化 有极分子 $\\Rightarrow$ 取向极化无极分子 $\\Rightarrow$ 位移极化 第十讲 电介质内的电场强度根据电介质极化原理推导： $$\\boldsymbol{E} = \\boldsymbol{E}_0+\\boldsymbol{E}’,\\ E_0=\\frac{\\sigma_0}{\\varepsilon_0},E’=\\frac{\\sigma’}{\\varepsilon_0}$$ $$\\Rightarrow E = \\frac{\\sigma_0}{\\varepsilon_0}-\\frac{\\sigma’}{\\varepsilon_0},\\ E = \\frac{E_0}{\\varepsilon_r}\\tag{45}$$ $$\\Rightarrow \\sigma’=(1-\\frac{1}{\\varepsilon_r})\\sigma_0$$ 第十一讲 $\\bigstar$电介质中的高斯定理11.1 电位移矢量推导： $$\\iint_S\\boldsymbol{E}\\cdot d\\boldsymbol{S}=\\frac{1}{\\varepsilon_0}(\\sigma_0-\\sigma’)S$$ 由式(45)得: $$\\frac{1}{\\varepsilon_0}(\\sigma_0-\\sigma’)=\\frac{\\sigma_0}{\\varepsilon_0\\varepsilon_r}$$ $$\\iint_S\\varepsilon_0\\varepsilon_r\\boldsymbol{E}\\cdot d\\boldsymbol{S}=\\varepsilon_0S=q_0$$ 令 $\\boldsymbol{D} = \\varepsilon\\boldsymbol{E} = \\varepsilon_0\\varepsilon_r\\boldsymbol{E}$ 得: $$\\iint_S\\boldsymbol{D}\\cdot d\\boldsymbol{S} = q_0\\tag{46}$$ 其中，$D$ 称为电位移矢量或电通密度，$\\varepsilon = \\varepsilon_0\\varepsilon_r$ 称为电介质的介电常数； 11.2 电介质中的能量密度$$\\omega = \\frac{1}{2}\\boldsymbol{D}\\cdot\\boldsymbol{E}\\\\varepsilon_r = 1\\Rightarrow \\omega = \\frac{1}{2}\\varepsilon_0E^2\\tag{47}$$ 第十二讲 经典习题 第二章 恒定电流的磁场第一讲 磁感应强度$B$电流元 $Idl$ 所受磁场力： $$d\\boldsymbol{F} = Id\\boldsymbol{l}\\times\\boldsymbol{B}\\tag{1}$$ 第二讲 毕奥-萨伐尔定律2.1 电流元的磁场$$d\\boldsymbol{B} = \\frac{\\mu_0}{4\\pi}\\frac{Id\\boldsymbol{l}\\times\\boldsymbol{r}^0}{r^2}\\tag{2}$$ 其中，$\\mu_0=4\\pi\\times10^{-7}N/A^2$称为真空磁导率，$\\boldsymbol{r}_0$ 表示到 $P$ 点的单位矢量，$r$ 表示到 $P$ 点的距离； 2.2 运动电荷的磁场$$\\boldsymbol{B} = \\frac{d\\boldsymbol{B}}{dN}=\\frac{\\mu_0}{4\\pi}\\frac{q\\boldsymbol{v}\\times\\boldsymbol{r}^0}{r^2}\\tag{3}$$ 2.3 载流直导线的磁场$$dB = \\frac{\\mu_0}{4\\pi}\\frac{Idlsin\\theta}{r^2}$$ $$\\Rightarrow B = \\frac{\\mu_0I}{4\\pi r}\\int_{\\theta_1}^{\\theta_2}sin\\theta d\\theta=\\frac{\\mu_0I}{4\\pi r}(cos\\theta_1-cos\\theta_2)\\tag{4}$$ $$\\theta_1\\approx 0,\\theta_2\\approx\\pi\\Rightarrow B = \\frac{\\mu_0I}{2\\pi r}$$ 式中，$r$ 表示到载流导线的距离； 2.4 载流圆环的磁场$$B = \\int dB_x = \\int dBcos\\theta = \\frac{\\mu_0}{4\\pi}\\int \\frac{Idl}{r^2}cos\\theta$$ $$cos\\theta = \\frac{R}{r}=\\frac{R}{(R^2+x^2)^{1/2}}\\tag{5}$$ $$\\Rightarrow B = \\frac{\\mu_0IR^2}{2(R^2+x^2)^{3/2}}$$ 【$N$匝线圈】 $$B = \\frac{\\mu_0IR^2N}{2(R^2+x^2)^{3/2}}\\tag{6}$$ 【圆弧磁场】由式 (5) 令 $x=0$ 得圆心处磁感应强度: $B = \\frac{\\mu_0I}{2R}$ $$B = \\frac{\\mu_0I}{2R}\\cdot\\frac{\\varphi}{2\\pi}=\\frac{\\mu_0I\\varphi}{4\\pi R}\\tag{7}$$ 2.5 载流线圈的磁矩由式 (5) 令 $x\\gg R$ 则得: $(x^2+R^2)\\approx x^2$ $$\\Rightarrow B\\approx \\frac{\\mu_0IR^2}{2x^3} = \\frac{\\mu_0I\\pi R^2}{2\\pi x^3}=\\frac{\\mu_0 IS}{2\\pi x^3}$$ $$\\Rightarrow Define:\\ \\ \\ \\ \\boldsymbol{p}_m = IS\\boldsymbol{n}\\tag{8}$$ $$\\boldsymbol{B} = \\frac{\\mu_0}{2\\pi}\\frac{\\boldsymbol{p}_m}{x^3}$$ 其中，$\\boldsymbol{n}$表示线圈平面正法线方向上的单位矢量；圆心处的磁感应强度： $$\\boldsymbol{B} = \\frac{\\mu_0}{2\\pi}\\frac{\\boldsymbol{p}_m}{R^3}\\tag{9}$$ 2.6 无限大均匀载流平面$$dB = \\frac{\\mu_0\\alpha dx}{2\\pi\\sqrt{r^2+x^2}}$$ 由对称性得：$B_x = \\int dB_x, \\ \\ \\ B_y = \\int dB_y = 0$ $$B = B_x = \\int \\frac{r}{\\sqrt{r^2+x^2}}\\cdot \\frac{\\mu_0 \\alpha dx}{2\\pi\\sqrt{r^2+x^2}} =\\int \\frac{\\mu_0 \\alpha r dx}{2\\pi (r^2+x^2)}$$ $$=\\frac{\\mu_0 \\alpha r}{2\\pi}\\int_{-\\infty}^{+\\infty}\\frac{1}{r^2+x^2}dx=\\frac{\\mu_0 \\alpha}{2}\\tag{10}$$ $$\\Longrightarrow B = \\frac{1}{2}\\mu_0\\alpha$$ 式中，$r$ 表示$P$点距到无限大载流平面的距离，$\\alpha$ 表示流过单位长度的电流； 2.7 均匀密绕直螺线管$$dB = \\frac{\\mu_0R^2dI’}{2(R^2+l^2)^{3/2}} = \\frac{\\mu_0R^2Indl}{2(R^2+l^2)^{3/2}}$$ $$l = Rcot\\beta\\ ,\\ \\ \\ dl = -Rcsc^2\\beta d\\beta\\ , \\ \\ \\ R^2+l^2 = R^2csc^2\\beta$$ $$\\Rightarrow dB= -\\frac{\\mu_0}{2}nIsin\\beta d\\beta\\tag{11}$$ $$\\Rightarrow B =\\int_{\\beta_1}^{\\beta_2}-\\frac{\\mu_0}{2}nIsin\\beta d\\beta = \\frac{\\mu_0nI}{2}(cos\\beta_2-cos\\beta_1)$$ 【无限长】 $$L\\gg R,\\ \\ \\beta_1\\rightarrow\\pi, \\ \\ \\beta_2\\rightarrow 0 \\Rightarrow B = \\mu_0nI\\tag{12}$$ 【半无限长】端点处： $$\\beta_1 = \\frac{\\pi}{2}, \\ \\ \\beta_2\\rightarrow 0\\ , or \\ \\ \\beta_1\\rightarrow \\pi, \\ \\ \\beta_2=\\frac{\\pi}{2}\\Rightarrow B = \\frac{\\mu_0nI}{2}\\tag{13}$$ 式中，$n$ 表示单位长度上的线圈匝数； 2.8 均匀密绕圆环螺线管$$B = n\\mu I$$ 第三讲 磁通量 磁场的高斯定理3.1 磁通量$$\\Phi_m = \\int_S \\boldsymbol{B}\\cdot d\\boldsymbol{S}\\tag{14}$$ 3.2 高斯定理$$\\oint_S \\boldsymbol{B}\\cdot d\\boldsymbol{S}=0\\tag{15}$$ 第四讲 $\\bigstar$安培环路定理$$\\oint_L\\boldsymbol{B}\\cdot d\\boldsymbol{l}=\\mu_0\\sum_{(内)}I_i\\tag{16}$$ 式中，$I_i$ 的正（负）取决于电流方向与闭合路径 $L$ 绕行方向满足（不满足）右螺旋法则；$B$ 表示闭合路径 $L$ 内外所有电流产生的总磁感应强度；【无限大载流平面】 $$\\oint_L\\boldsymbol{B}\\cdot d\\boldsymbol{l} = \\int_{PQ}\\boldsymbol{B}\\cdot d\\boldsymbol{l}+\\int_{QR}\\boldsymbol{B}\\cdot d\\boldsymbol{l}+\\int_{RS}\\boldsymbol{B}\\cdot d\\boldsymbol{l}+\\int_{SP}\\boldsymbol{B}\\cdot d\\boldsymbol{l}$$ $$=Bx+0+Bx+0 = 2Bx = \\mu_0 \\alpha x\\tag{17}$$ $$\\Rightarrow B = \\frac{1}{2}\\mu_0\\alpha$$ 第五讲 磁场对电流作用5.1 载流导线所受安培力： $$\\boldsymbol{F} = \\int_LId\\boldsymbol{l}\\times\\boldsymbol{B}\\tag{18}$$ 5.2 载流线圈所受磁力矩： $$M = F_{ab}l_1sin\\varphi=BIl_1l_2sin\\varphi=BISsin\\varphi$$ $$\\boldsymbol{p}_m = IS\\boldsymbol{n}\\tag{19}$$ $$\\Rightarrow \\boldsymbol{M}=\\boldsymbol{p}_m\\times\\boldsymbol{B}$$ 式中，$\\boldsymbol{n}$ 的方向按电流方向用右螺旋法则确定； 5.3 磁力的功$$A = F\\overline{aa’} = BIl\\overline{aa’}=BI\\vartriangle S = I\\vartriangle\\Phi$$ $$\\Rightarrow A = \\int_{\\Phi_1}^{\\Phi_2}Id\\Phi = I(\\Phi_2-\\Phi_1)=I\\vartriangle\\Phi\\tag{20}$$ 5.4 磁偶极子势能载流线圈相当于磁偶极子，因此载流线圈同理；当 $\\varphi = \\frac{\\pi}{2}$ 时, $W = 0$ (零势能点) $$W = -A = -\\int_\\varphi^{\\pi/2}Md\\varphi = -p_mB\\int_\\varphi^{\\pi/2}sin\\varphi d\\varphi = -p_mBcos\\varphi$$ $$\\Rightarrow W = -\\boldsymbol{p}_m\\cdot \\boldsymbol{B}\\tag{21}$$ 第六讲 带电粒子在电场和磁场中的运动6.1 洛伦兹力$$\\boldsymbol{F} = q\\boldsymbol{v}\\times\\boldsymbol{B}\\tag{22}$$ 式中，$q$ 包含电荷正负特性符号； 6.2 霍尔效应$$q\\overline{v}B=qE\\Rightarrow E = \\overline{v}B\\Rightarrow U = El = vBl$$ $$I = nqS\\overline{v}\\Rightarrow U = \\frac{IB}{nqd} = K\\frac{IB}{d}\\tag{23}$$ $$K = \\frac{1}{nq}$$ 式中，$d$ 和 $l$ 分别表示沿电流方向上导体截面的宽度和高度；$n$ 表示单位体积的载流子数；【载流子种类】 p(positive)型半导体 $\\Rightarrow$ 空穴 $\\Rightarrow$ 空穴导电n(negative)型半导体 $\\Rightarrow$ 电子 $\\Rightarrow$ 电子导电金属导体(大多数) $\\Rightarrow$ 电子 $\\Rightarrow$ 电子导电 第七讲 磁介质7.1 相对磁导率$$\\mu_r = \\frac{B}{B_0}\\tag{24}$$ 式中，$B_0$ 表示真空磁感应强度，$\\mu_r$ 表示磁介质的相对磁导率，$B$ 表示磁介质的磁感应强度； $\\mu_r &gt; 1\\Rightarrow$ 顺磁质(弱/非磁性物质)$\\mu_r&lt;1\\Rightarrow$ 抗磁质(弱/非磁性物质)$\\mu_r\\gg 1 \\Rightarrow$ 铁磁质(强磁性物质) 7.2 $\\bigstar$ 磁介质的安培环路定理$$\\oint_L\\boldsymbol{B}\\cdot d\\boldsymbol{l} = \\mu_0\\mu_r\\sum_{(内)}I$$ 令 $\\mu = \\mu_0\\mu_r$ 得: $$\\oint_L \\frac{\\boldsymbol{B}}{\\mu}\\cdot d\\boldsymbol{l}=\\sum_{(内)}I$$ 令 $\\boldsymbol{H} = \\frac{\\boldsymbol{B}}{\\mu}$ 得: $$\\oint_L\\boldsymbol{H}\\cdot d\\boldsymbol{l}=\\sum_{(内)}I\\tag{25}$$ 式中，$\\mu$ 表示磁介质的磁导率，$\\boldsymbol{H}$ 表示磁场强度，对有介质存在的环路定理的处理可以参考电位移矢量 $\\boldsymbol{D}$； 第八讲 经典习题 第三章 电磁感应与电磁场第一讲 电磁感应的基本规律1.1 电动势闭合回路上： $$\\xi = \\oint\\boldsymbol{E}_k\\cdot d\\boldsymbol{l}\\tag{1}$$ 对于一段电路$ab$： $$\\xi = \\int_a^b\\boldsymbol{E}_k\\cdot d\\boldsymbol{l}\\tag{2}$$ 其中，$\\boldsymbol{E}_k$表示非静电性电场强度； 1.2 法拉第电磁感应定律$$\\xi_i=-\\frac{d\\Phi}{dt}\\tag{3}$$ 由楞次定律确定方向$\\Rightarrow$方向相反； 1.3 多匝串联线圈$$\\xi_i=-\\frac{d}{dt}(\\sum_{k=1}^N\\Phi_k)=-\\frac{d\\Psi}{dt}\\tag{4}$$ $$\\xi_i=-\\frac{d\\Psi}{dt}=-N\\frac{d\\Phi}{dt}(\\Phi_i=\\Phi_j, 1 \\leq i,j \\leq N)\\tag{5}$$ 其中，$\\Psi=\\sum_{k=1}^N\\Phi_k$表示穿过各线圈的总磁通量，称为磁通链数； 1.4 长直螺线管在长直螺线管外套一 $N$ 匝，总内阻为 $R$ 的圆线圈，$S$ 表示螺线管截面积： $$B=\\mu_0nI\\Rightarrow \\Phi = \\boldsymbol{B}\\cdot\\boldsymbol{S}=\\mu_0nIS$$ 当通电电流均匀变化时，螺线管内的感应电动势: $$\\xi_i=-\\frac{d\\Psi}{dt}=-N\\frac{d\\Phi}{dt}=-\\mu_0nNS\\frac{dI}{dt}$$ 【螺线管内磁感应强度】感应电流 $I_i=\\frac{\\xi_i}{R}=-\\frac{N}{R}\\frac{d\\Phi}{dt}$ $$\\Delta_{q_i}=\\int_{t_1}^{t_2}I_idt=-\\frac{N}{R}\\int_{\\Phi_1}^{\\Phi_2}d\\Phi=-\\frac{N}{R}(\\Phi_2-\\Phi_1)$$ $$\\Longrightarrow \\Phi_1-\\Phi_2=\\frac{\\Delta_{q_i}R}{N}$$ 当 $\\Phi_1=0\\vert_{t=t_1},\\Phi_2=BS\\vert_{t=t_2\\rightarrow+\\infty}$ 时，推出 $B=\\frac{\\Delta_{q_i}R}{NS}$ 第二讲 动生电动势 感生电动势2.1 动生电动势导体棒 $ab$ 产生的动生电动势： $$\\xi_i=\\int_a^b\\boldsymbol{E}_k\\cdot d\\boldsymbol{l}=\\int_a^b(\\boldsymbol{v}\\times\\boldsymbol{B})\\cdot d\\boldsymbol{l}\\tag{6}$$ 闭合回路产生的动生电动势： $$\\xi_i=\\oint_Ld\\xi_i=\\oint_L(\\boldsymbol{v}\\times\\boldsymbol{B})\\cdot d\\boldsymbol{l}\\tag{7}$$ 动生电动势方向由 $\\boldsymbol{v}\\times\\boldsymbol{B}\\cdot d\\boldsymbol{l}$ 判定： $\\xi_i>0\\Rightarrow u_a\\leq u_b$ $\\xi_iu_b$ 注：积分路径：$a\\rightarrow b$，在电源内部非静电性电场强度从负极指向正极， $\\boldsymbol{E}_k$ 与积分方向一致时积分值为正，否则为负； 2.2 感生电动势 感生电场假说$\\Longrightarrow$ 有旋电场 【回路固定不动】 $$\\xi_i=\\oint_L\\boldsymbol{E}_V\\cdot d\\boldsymbol{l}=-\\iint_S\\frac{\\partial \\boldsymbol{B}}{\\partial t}\\cdot d\\boldsymbol{S}\\tag{9}$$ 感生电动势方向由楞次定律判定；有旋电场度 $E_V$ 的方向判定：闭合回路由右螺旋法则指向磁场方向选定回路绕行正方向，由式 $(9)$ 代入符号计算，$E_V$ 正负与回路绕行方向保持一致；当 $E_V$ 相等，磁场均匀变化时， $$\\xi_i=E_V\\oint_Ldl=-\\frac{\\partial{B}}{\\partial{t}}\\iint_SdS=-\\frac{\\partial{B}}{\\partial{t}}S\\tag{10}$$ $\\Longrightarrow$ 计算某一闭合回路上的有旋电场强度($S$ 表示磁场面积) 第三讲 自感与互感3.1 自感电动势$$\\Psi=LI\\Rightarrow \\xi_L=-\\frac{d\\Psi}{dt}=-L\\frac{dI}{dt}\\tag{11}$$ 式中 $L$ 表示自感系数为常量，与 $I$ 无关(存在铁磁质时与 $I$ 有关)，仅有回路的匝数、几何形状、大小以及周围介质磁导率决定； 3.2 长直螺线管自感系数【空心自感线圈】 $$B = \\mu_0nI=\\mu_0\\frac{N}{l}I\\Rightarrow \\Psi=NBS=\\mu_0\\frac{N^2}{l}\\pi R^2I$$ $$\\Longrightarrow L=\\frac{\\Psi}{I}=\\frac{\\mu_0N^2\\pi R^2}{l}=\\mu_0n^2V\\ (V=\\pi R^2l)\\tag{12}$$ 3.3 传输线的分布电感两长直平行导线电流 $I$，半径 $r_0$，轴线间距 $d$，且 $r_0\\leq d$；导线微元: $d\\Phi_1=BdS=\\frac{\\mu_0I}{2\\pi r}ldr$ $$\\Rightarrow \\Phi_1=\\int_{r_0}^{d-r_0}\\frac{\\mu_0Il}{2\\pi}\\frac{dr}{r}=\\frac{\\mu_0Il}{2\\pi}\\ln(\\frac{d-r_0}{r_0})$$ $\\Phi=\\Phi_1+\\Phi_2=2\\Phi_1$(电流反向) $\\Phi=\\Phi_1+\\Phi_2=0$(电流同向) $$ L=\\frac{\\Phi}{I}=\\frac{\\mu_0}{\\pi}l\\ln(\\frac{d-r_0}{r_0})\\approx \\frac{\\mu_0}{\\pi}l\\ln\\frac{d}{r_0}\\tag{13} $$ 3.4 互感电动势 回路 $1$ 对回路 $2$: $\\Psi_{21}=M_{21}I_1$ 回路 $2$ 对回路 $1$: $\\Psi_{12}=M_{12}I_2$ $$M_{21}=M_{12}=M\\Longrightarrow \\xi_M=-M\\frac{dI}{dt}\\tag{14}$$ 式中 $M_{21}$ 表示回路 $1$ 对回路 $2$ 的互感系数，$M_{12}$ 表示回路 $2$ 对回路 $1$ 的互感系数；$M$ 表示两个回路间的互感系数，与 $I$ 无关(存在铁磁质时与 $I$ 有关)，由回路的匝数、几何形状、尺寸、周围介质磁导率以及回路的相对位置决定； $M_{12}=M_{21}=M\\Rightarrow$ 转换研究对象简化计算互感系数$\\Rightarrow$ 互感电动势 第四讲 磁能4.1 自感磁能$$dA=-\\xi_Lidt,\\ \\xi_L=-L\\frac{di}{dt}$$ $$\\Longrightarrow dA=Lidi$$ $$\\Longrightarrow A=\\int_0^ILidi=\\frac{1}{2}LI^2\\tag{15}$$ 即$W_m=\\frac{1}{2}LI^2$ (自感磁能) 当有磁场能量时可以利用$L=\\frac{2W_m}{I^2}$ 计算自感系数 式中，$L$ 表示线圈自感，$I$ 表示线圈所通电流； 4.2 长直螺线管磁能$$由式(12)\\Rightarrow L=\\mu n^2V\\Rightarrow W_m=\\frac{1}{2}LI^2=\\frac{1}{2}\\mu n^2I^2V$$ $$B=\\mu nI\\Longrightarrow H=\\frac{B}{\\mu}=nI\\tag{16}$$ $W_m=\\frac{1}{2}BHV$ 磁能密度$\\omega_m=\\frac{W_m}{V}=\\frac{1}{2}BH=\\frac{1}{2}\\frac{B^2}{\\mu_0\\mu_r}$ 4.3 有限体积内的磁能$$W_m=\\int_VdW_m=\\frac{1}{2}\\int_VBHdV\\tag{17}$$ 第五讲 麦克斯韦电磁场理论5.1 位移电流 传导电流 $\\Leftarrow$ 电荷定向移动形成的电流位移电流 $\\Leftarrow$ 电位移通量的变化率(变化的电场) $$\\Phi_D=DS=\\varepsilon ES=\\varepsilon\\cdot\\frac{\\sigma}{\\varepsilon}S=\\sigma S$$ 传导电流 $$\\Longrightarrow \\frac{d\\Phi}{dt}=\\frac{d}{dt}(\\sigma S)=\\frac{dq}{dt}=I\\tag{18}$$ 位移电流 $$\\Longrightarrow I_D = \\frac{d\\Phi_D}{dt}=\\frac{dD}{dt}S=\\varepsilon\\frac{dE}{dt}S$$ 全电流 $\\Longrightarrow$ 全电流 = $I+I_D$ 非恒定电路中传导电流不连续但全电流保持连续 5.2 全电流安培环路定理$$\\oint_L\\boldsymbol{H}\\cdot d\\boldsymbol{l}=I+I_D,\\ I_D =\\frac{d\\Phi_D}{dt}= \\int_S\\frac{d\\boldsymbol{D}}{dt}\\cdot\\boldsymbol{S}\\tag{19}$$ 5.3 麦克斯韦方程组 电场 $\\boldsymbol{E}, \\boldsymbol{D}$ 自由电荷产生的静电场 $\\boldsymbol{E_1}$、$\\boldsymbol{D_1}$ $\\Rightarrow\\boldsymbol{E}=\\boldsymbol{E_1}+\\boldsymbol{E_2}$ 变化磁场产生的有旋电场 $\\boldsymbol{E_2}$、$\\boldsymbol{D_2}$ $\\Rightarrow\\boldsymbol{D}=\\boldsymbol{D_1}+\\boldsymbol{D_2}$ 磁场 $\\boldsymbol{B}, \\boldsymbol{H}$ 传导电流产生的磁场 $\\boldsymbol{B_1}$、$\\boldsymbol{H_1}$ $\\Rightarrow\\boldsymbol{B}=\\boldsymbol{B_1}+\\boldsymbol{B_2}$ 位移电流产生的磁场 $\\boldsymbol{B_2}$、$\\boldsymbol{H_2}$ $\\Rightarrow\\boldsymbol{H}=\\boldsymbol{H_1}+\\boldsymbol{H_2}$ 电场的高斯定理 $\\oint_S\\boldsymbol{D}\\cdot d\\boldsymbol{S}=\\sum_iq_i$ 电场是有源场 法拉第电磁感应定律 $\\oint_L\\boldsymbol{E}\\cdot d\\boldsymbol{l}=-\\iint_S\\frac{\\partial{\\boldsymbol{B}}}{\\partial{t}}\\cdot d\\boldsymbol{S}$ 静电场是保守(无旋、有势)场 磁场的高斯定理 $\\oint_S\\boldsymbol{B}\\cdot d\\boldsymbol{S}=0$ 磁场是无源场 全电流安培环路定理 $\\oint_L\\boldsymbol{H}\\cdot d\\boldsymbol{l}=\\sum(I_D+I)$ 磁场是有旋(非保守)场 位移电流 $I_d$ $$\\Phi_D = \\iint\\boldsymbol{D}\\cdot d\\boldsymbol{S}\\I_d = \\frac{d\\Phi_D}{dt}$$ $\\Rightarrow$ 位移电流密度 $j_d = \\frac{I_d}{S}$ + 位移电流激发的磁场 $B$ $$ \\oint_L\\boldsymbol{H}\\cdot d\\boldsymbol{l}=j_dS $$ 第四章 狭义相对论力学基础第一讲 力学相对性原理1.1 经典力学相对性原理 力学相对性原理 对于描述力学现象的规律而言，所有惯性系都是等价的 力学规律的数学表达式应具有伽利略坐标变换的不变性(协变性) | 1.2 伽利略坐标变化式根据 $\\lambda_{PS’}+\\lambda_{S’S}=\\lambda_{PS}$ 推出: $$\\lambda’ = \\lambda-\\mu t\\ \\ (\\lambda=x,y,z,\\boldsymbol{v},\\boldsymbol{a},\\mu=u),\\ t’=t\\tag{1}$$ 第二讲 狭义相对论基本假设 狭义相对论的相对性原理 在所有惯性系中，一切物理学定理都相同，即具有相同的数学表达式 对于描述一切物理现象的规律而言，所有惯性系都是等价的 | 光速不变原理 在所有惯性系中，真空中光沿各个方向传播的速率都等于同一个恒量 $c$，与光源和观察者的运动状态无关 第三讲 狭义相对论的时空观3.1 同时性的相对性 异地发生的两个同时事件，同时性具有相对性(对任意参考系)同地发生的两个同时事件，同时性具有绝对性(对任意参考系) 3.2 时间延缓 时间间隔具有相对性 $$\\tau=\\frac{\\tau_0}{\\sqrt{1-(\\frac{u}{c})^2}}=\\gamma\\tau_0\\tag{2}$$ 式中，$\\gamma = \\frac{1}{\\sqrt{1-(\\frac{u}{c})^2}}$，$\\tau_0$ 表示同地不同时的两事件的时间间隔称为原时，且在不同参考系中测得的时间间隔以原时最短； 3.3 长度收缩 长度测量具有相对性 $$L’=L\\sqrt{1-(\\frac{u}{c})^2}\\tag{3}$$ 式中，$L$ 表示观测者静止时测得的长度(原长)，$L’$ 表示在沿尺长度方向运动速度为 $u$ 时测得的长度，且在不同参考系中测得的长度以原长最长； 第四讲 洛伦兹变换4.1 时空坐标变换$P$ 在 $S$ 中的时空坐标 $(x,y,z,t)$,在 $S'$ 中的时空坐标 $(x',y',z',t')$ $S$ 系中测得 $S'$ 中坐标 $x''= x'\\sqrt{1-(\\frac{u}{c})^2}$ (长度收缩) $\\Longrightarrow$ 在 $S$ 系中 $P$ 坐标 $x = ut + x''=ut+x'\\sqrt{1-(\\frac{u}{c})^2}$ $S'$ 系中测得 $S$ 中坐标 $x_1= x\\sqrt{1-(\\frac{u}{c})^2}$ (长度收缩) $\\Longrightarrow$ 在 $S'$ 系中 $P$ 坐标 $x' = x_1 -ut' = x\\sqrt{1-(\\frac{u}{c})^2} - ut'$ $$ \\Longrightarrow x' = \\frac{x-ut}{\\sqrt{1-(\\frac{u}{c})^2}},t' = \\frac{t-\\frac{u}{c^2}x}{\\sqrt{1-(\\frac{u}{c})^2}}\\tag{4} $$ 式中，$u$ 表示 $S’$ 相对于 $S$ 的速度(相对速度)，$x’$ 表示待求坐标系中参量； 推导时间变换式: 由 $x’ = \\frac{x-ut}{\\sqrt{1-(\\frac{u}{c})^2}}$ 及逆变换 $x = \\frac{x’+ut’}{\\sqrt{1-(\\frac{u}{c})^2}}$ 联立消去 $x’$ 解 $t’$ 4.2 时空间隔变换$P_1,P_2$ 在 $S$ 中的时空坐标 $(x_1,y_1,z_1,t_1),(x_2,y_2,z_2,t_2)$,在 $S'$ 中的时空坐标 $(x_1',y_1',z_1',t_1'),(x_2',y_2',z_2',t_2')$ 由 $S\\rightarrow S'$ 得: $$ \\Delta t'=\\frac{\\Delta t-\\frac{u}{c^2}\\Delta x}{\\sqrt{1-\\beta^2}},\\Delta x'=\\frac{\\Delta x-u\\Delta t}{\\sqrt{1-\\beta^2}}\\ (\\beta = \\frac{u}{c}) $$ 由 $S'\\rightarrow S$ 得: $$ \\Delta t=\\frac{\\Delta t'+\\frac{u}{c^2}\\Delta x'}{\\sqrt{1-\\beta^2}},\\Delta x=\\frac{\\Delta x'+u\\Delta t'}{\\sqrt{1-\\beta^2}}\\ (\\beta = \\frac{u}{c})\\tag{5} $$ 式中，$u$ 关联于坐标轴选取的正方向，一般选定 $S$ 系运动方向为坐标轴正方向； 4.3 爱因斯坦速度相加定律由式 $(4)$ 求微分得: $$dx’=\\frac{(v_x-u)}{\\sqrt{1-\\beta^2}}dt,\\ dy’= dy,\\ dz’= dz,\\ dt’ = \\frac{(1-\\frac{u}{c^2}v_x)}{\\sqrt{1-\\beta^2}}dt$$ $$\\Rightarrow v_x’=\\frac{dx’}{dt’}=\\frac{v_x-u}{1-\\frac{u}{c^2}v_x},\\ v_y’=\\frac{dy’}{dt’}=\\frac{v_y\\sqrt{1-\\beta^2}}{1-\\frac{u}{c^2}v_x},\\ v_z’=\\frac{dz’}{dt’}=\\frac{v_z\\sqrt{1-\\beta^2}}{1-\\frac{u}{c^2}v_x}\\tag{6}$$ 第五讲 狭义相对论质点动力学5.1 相对论动量和质量 质速关系式 $$m(v) = \\frac{m_0}{\\sqrt{1-(\\frac{u}{c})^2}}$$ $$\\Longrightarrow \\boldsymbol{p}=m\\boldsymbol{v}=\\frac{m_0}{\\sqrt{1-(\\frac{u}{c})^2}}\\boldsymbol{v},\\ \\boldsymbol{F}=\\frac{d\\boldsymbol{p}}{dt}=\\frac{d}{dt}(\\frac{m_0}{\\sqrt{1-(\\frac{u}{c})^2}}\\boldsymbol{v})\\tag{7}$$ 式中，$m_0$ 表示物体静止质量； 5.2 相对论动能$$E_k = \\int \\boldsymbol{F}\\cdot d\\boldsymbol{r}=\\int \\frac{d(m\\boldsymbol{v})}{dt}\\cdot d\\boldsymbol{r}=\\int d(m\\boldsymbol{v})\\cdot \\frac{d\\boldsymbol{r}}{dt}=\\int d(m\\boldsymbol{v})\\cdot\\boldsymbol{v}$$ $m\\propto\\boldsymbol{v}\\Longrightarrow d(m\\boldsymbol{v})\\cdot\\boldsymbol{v}=(\\boldsymbol{v}dm+md\\boldsymbol{v})\\cdot \\boldsymbol{v}=v^2dm+mvdv$ 由式 $(7)$ 得: $m^2v^2=m^2c^2-m_0^2c^2\\Rightarrow v^2dm+mvdv=c^2dm$ $$ E_k=\\int_{m_0}^mc^2dm=mc^2-m_0c^2\\tag{8} $$ 5.3 质能方程 运动能量: $E=mc^2$静止能量: $E_0=m_0c^2$ 5.4 光子质量 爱因斯坦光子假说 光子能量: $E= h\\nu$ $$\\Longrightarrow m_\\varphi=\\frac{E}{c^2}=\\frac{h\\nu}{c^2}=\\frac{h}{c\\lambda}\\tag{10}$$ 光子、中微子在真空中速率为$c$，不可能静止因此静止能量等于零 5.5 相对论能量与动量关系由式 $(7)$ 得: $m^2(1-\\frac{v^2}{c^2})=m_0^2$ $\\Longrightarrow m^2c^4=m^2v^2c^2+m_0^2c^4$ $p=mv\\Rightarrow E^2=p^2c^2+E_0^2$ 由于光子 $m_0=0$ 故得: $E_0=0\\Rightarrow E^2=p^2c^2$ $$ \\Longrightarrow p=\\frac{h\\nu}{c}=\\frac{h}{\\lambda}\\tag{11} $$ 第五章 量子物理基础第一讲 普朗克量子假设1.1 基本概念 热辐射 物体由其温度所决定的电磁辐射(温度越高，单位时间内辐射的能量越高) 平衡热辐射 当辐射和吸收达到平衡时，物体的温度不再发生变化而处于热平衡状态时的热辐射 单色辐射出射度(单色辐出度) 物体单位表面积在单位时间内发射的，波长在$\\lambda\\rightarrow\\lambda+d\\lambda$ 范围内的辐射能 $dM_\\lambda$与波长间隔 $d\\lambda$ 的比值 绝对黑体(黑体) 能够全部吸收各种波长的辐射能而不发生发射和透射的物体 1.2 单色辐出度$$M_\\lambda(T)=\\frac{dM_\\lambda}{d\\lambda}\\tag{1}$$ 【单色辐出度图】 温度越高 单色辐出度越大，峰值波长越短 1.3 普朗克量子假设$$\\varepsilon=nh\\nu\\tag{2}$$ 式中，$\\varepsilon$ 表示腔壁中带电谐振子离散变化的能量，振子的频率为 $\\nu$，$n$ 表示量子数，$h\\nu$ 表示能量子——谐振子能量的最小单位(不是物质而是能量单位)； 第二讲 爱因斯坦光子理论2.1 光电效应 金属及其化合物在光的照射下发射电子的现象 【光电效应伏安特性曲线】 照射光光强越大，饱和光电流越大 光电子最大初动能与照射光强度无关，而与频率成线性关系 2.2 光电效应方程 遏止电压: $\\frac{1}{2}mv_m^2=eU_a$光电效应方程: $h\\nu = A + \\frac{1}{2}mv_m^2$截止频率: $\\nu_0=\\frac{A}{h}$ $$\\Longrightarrow U_a = \\frac{h}{e}\\nu-\\frac{A}{e}\\tag{3}$$ 第三讲 康普顿效应及光子理论解释3.1 康普顿效应 单色$X$ 射线被物质散射时，散射光两种波长中有一种波长比入射线长的散射现象 3.2 光子理论解释【微观机制】——等价于微观粒子的弹性碰撞入射光子频率 $\\nu_0$,散射角为 $\\theta$ 的光子频率为 $\\nu$,电子沿着与入射线成 $\\varphi$ 角的方向运动,静质量 $m_0$,动质量 $m$由动量守恒定律得到: $\\frac{h\\nu_0}{c}=\\frac{h\\nu}{c}\\cos\\theta+mv\\cos\\varphi$ $\\frac{h\\nu}{c}\\sin\\theta=mv\\sin\\varphi$ $\\Longrightarrow m^2v^2c^2=h^2(\\nu_0^2-\\nu^2-2\\nu_0\\nu\\cos\\theta)\\tag{4}$ 由能量守恒定律得到: $hv_0+m_0c^2=hv+mc^2\\tag{5}$ 进一步式 $(5)$ 平方 $-$ 式 $(4)$ 且 $m^2(1-\\frac{v^2}{c^2})=m_0^2$ 得到: $$ m_0c^2(\\nu_0-\\nu)=h\\nu_0\\nu(1-\\cos\\theta) $$ $$ \\Longrightarrow \\Delta\\lambda = \\lambda - \\lambda_0=\\frac{c}{\\nu}-\\frac{c}{\\nu_0}=\\frac{h}{m_0c}(1-\\cos\\theta) =\\frac{2h}{m_0c}\\sin^2\\frac{\\theta}{2}=2\\lambda_C\\sin^2\\frac{\\theta}{2}>0\\tag{6} $$ 式中，$\\lambda_C=\\frac{h}{m_0c}$ 称为电子的康普顿波长； 第四讲 氢原子光谱 玻尔氢原子理论4.1 氢原子光谱实验规律 氢原子光谱——线状光谱【里德伯-里兹合并原则】 光谱线波数 $\\widetilde{\\nu}=\\frac{1}{\\lambda}=T(k)-T(n)=R_H(\\frac{1}{k^2}-\\frac{1}{n^2})$ ($k、n\\in Z$ 且 $n&gt;k$) $k = 1\\ (n=2,3,4,\\cdots)$ ——赖曼系 $k = 2\\ (n=3,4,5,\\cdots)$——巴耳末系 4.2 玻尔氢原子理论——氢原子或类氢原子【辐射频率公式】——辐射或吸收一个频率为 $\\nu_{kn}$ 的光子 $$\\nu_{kn} = \\frac{|E_k-E_n|}{h}\\tag{7}$$ 【角动量量子化条件】——轨道角动量不能连续变化 $$L=mvr=n\\frac{h}{2\\pi}=n\\overline{h},\\ \\ n = 1,2,3,\\cdots\\tag{8}$$ 式中，$\\overline{h}=\\frac{h}{2\\pi}$ 表示约化普朗克常数；【电子轨道半径】——电子轨道半径不能连续变化 $$m\\frac{v^2}{r}=\\frac{1}{4\\pi\\varepsilon_0}\\frac{e^2}{r^2}$$ 又由式 $(8)$ 得: $$\\Rightarrow r_n=n^2(\\frac{\\varepsilon_0h^2}{\\pi me^2})=n^2r_1\\ (n=1,2,3,\\cdots)\\tag{9}$$ 式中 $r_1$ 表示氢原子中电子的最小轨道半径，称为玻尔半径； $n=1$ 的定态——基态$n=2,3,4,\\cdots$ 各态——受激态 氢原子能量=电子动能+电子电势能 量子数为 $n$ 的定态时氢原子能量： $$E=\\frac{1}{2}mv^2-\\frac{1}{4\\pi\\varepsilon_0}\\frac{e^2}{r}=-\\frac{1}{8\\pi\\varepsilon_0}\\frac{e^2}{r}$$ $$\\Longrightarrow E_n=-\\frac{1}{8\\pi\\varepsilon_0}\\frac{e^2}{r_n}=-\\frac{1}{n^2}(\\frac{me^4}{8\\varepsilon_0^2h^2})\\ \\ \\ (n=1,2,3,\\cdots)\\tag{10}$$ 当 $n\\rightarrow \\infty$ 时,$r_n\\rightarrow \\infty$，$\\ E_n\\rightarrow0$，能级趋于连续，原子趋于电离；$E&gt;0$ 时，原子处于电离状态，能量可连续变化。 电离能: 使原子或分子电离所需要的能量原子电离电势:电子使原子刚好电离所需的加速电势差 【氢原子跃迁】高能态跃迁到低能态发射一个光子其频率和波数： $$\\nu_{nk}=\\frac{E_n-E_k}{h}\\ \\ \\ (n&gt;k)$$ $$\\widetilde{\\nu_{nk}}=\\frac{1}{\\lambda_{nk}}=\\frac{\\nu_{nk}}{c}=\\frac{1}{hc}(E_n-E_k)\\tag{11}$$ 常用物理常数 物理常数 物理符号 取值 普朗克常数 $h$ $6.62607015\\times 10^{-34} \\ \\ J\\cdot s$ 电子电量 $e$ $1.6\\times 10^{-19}\\ \\ C$ 光速 $c$ $3\\times 10^8\\ \\ m/s$ 真空电容率/真空介电常量 $\\varepsilon_0$ $8.85\\times 10^{-12}\\ \\ C^2\\cdot N^{-1}\\cdot m^{-2}$ Contributors Zhihao Li","link":"/collaboration/Physics/"},{"title":"关于 ARM 指令体系中立即数范围的扩散机制","text":"ARM 指令体系特点ARM作为一款嵌入式微处理器或者一种嵌入式微处理架构，具有非常规整的指令体系，其精简指令集中共计 $30$ 条指令，并且每条指令均为 $32$ 位宽。 ARM 立即数的表示ARM 中一条指令有 $32$ 比特，但是立即数不能占用 $32$ 位指令编码空间的全部比特位，留给立即数的编码空间只有 $12$ 位。此外，因为 ARM 指令为单周期指令，当遇到操作数非常大的情形时，也不可能再取指一次。因此，不得不对指令中立即数表示进行特殊的设计。 扩散数据表示范围ARM 处理器指令系统将 $12$ 比特位分成 $8$ 比特位的常数和 $4$ 比特位的循环右移偶数位两部分。我们知道，$8$ 位比特位能够表示的数据范围为 (无符号数) Immed_$8=0\\sim255$，而 $4$ 位循环移位能够表示范围为 Rotate_Imm$=0\\sim15$, 因此设计立即数表示为: Immed = Immed_$8$ >> ($2\\times$Rotate_Imm) 循环右移偶数位数 $2\\times(0\\sim15)=${$0,2,4,\\cdots,30$} 因此，一个 $8$ 比特的数据，有 $16$ 种不同的循环右移方式，那么可以表示的数据范围为： $$2^8\\times16=256\\times16=4096B=4KB=2^{12}$$ 上式表明，循环右移偶数位并没有使得数据表示个数有任何改变; I.有效数据位在进一步阐述数据表示的扩散机制前，先要明确有效数据位的含义。在循环移位中，有效数据位指的是最长非零二进制串，即 $000101000100$ 中有效数据位为 $1010001$, 长度为 $7$。 II.扩散机制循环右移偶数位使得立即数在数据表示上有效数据位范围从原来的 $0\\sim12$ 位变为了 $0\\sim8$ 位，并且可以表示在第 $12\\sim32$ 比特位上。注：最高移位是 $8$ 位常数位，循环右移 $30$ 位，最长可以表示为 $38$ 位的长度，为什么不尽最大长度扩散数据范围呢？可以考虑指令的执行过程，ARM 指令中的立即数会被送往 ALU 中，而 ARM 的 ALU 最高处理 $32$ 位数据，因此只扩散到 $32$ 位编码空间。【扩散效应】——数据本源并没有发生变化，而是将数据扩展散开分布。 概括来讲，循环右移偶数位不能改变原有数据表示个数，只是将数据离散分布到 $32$ 位编码空间，因此称为扩散数据表示。但是这种方式，就要保证立即数是合法的，即满足有效位数小于等于 $8$ 位，能够通过循环右移偶数位后表示该数据。","link":"/blog/ARMImmediateNumber/"},{"title":"微型计算机原理与系统设计","text":"第一章 绪论：微型计算机概述第一讲 微型计算机概述基本概念 微型计算机系统的三个层次：微处理器、微型计算机、微型计算机系统 微处理器：由 1 片或几片大规模集成电路组成的中央处理器，也即微型计算机中的 CPU (中央处理单元），具体包括控制器、运算器、寄存器以及连接三者的片内总线； 微型计算机：微处理器、内存、I/O 接口以及连接三者的系统总线或芯片组的集合，也即裸机； 微型计算机系统：以微型计算机为中心，配以相应的外围设备以及控制微型计算机工作的软件，简称为微机； 软件：系统软件和应用软件； 外设：外存和 I/O 设备； 单片机：CPU、内存、I/O接口以及使三者互连的总线在一个芯片上的集成，也即微型计算机在一个芯片上的集成； 单片机系统：由单片机、专用软件和 I/O设备组成的系统，常用于特定任务的控制或处理； 单片机系统具有专用性，微型计算机系统具有通用性 微处理器概述 Intel微处理器发展 时间 型号 位宽 主频 制造工艺 1971年 4004 4位 108KHz 10$\\mu$ 1972年 8008 8位 500-800KHz 10$\\mu$ 1974年 8080 8位 2MHz 6$\\mu$ 1978年 8086 16位 5MHz 3$\\mu$ 1979年 8088 16位 5MHz 3$\\mu$ 1982年 80286 16位 6MHz 1.5$\\mu$ 1985年 80386 32位 16MHz 1.5$\\mu$ 1989年 80486 32位 25MHz 1$\\mu$ 1993年 Pentium I 32位 66MHz 0.8$\\mu$ 2001年 Itanium 64位 66MHz 0.8$\\mu$ 第二章 Intel 单核/多核处理器第一讲 单核处理器（Intel 8086处理器）8086/8088 处理器功能特性 16 位微处理器（内部数据总线） 引进指令级流水 引入分段管理机制，扩大寻址空间 只有整数运算能力，配套数值协处理器 8087、输入/输出协处理器 8089，具备较强大计算能力和 I/O处理能力 8086：16位；8088：8位（外部数据总线） 8086处理器的体系架构 【组成部分】 8086处理器分为两个部分：执行单元 (Execution Unit, EU) 和总线接口单元 (Bus Interface Unit, BIU) 执行单元 EU负责指令的执行，包括ALU（运算器）、通用寄存器组和状态寄存器 执行的运算：算术、逻辑、移位运算及段内偏移地址（即有效地址）的计算 总线接口单元 BIU负责与主存和I/O设备的接口，由段寄存器、指令指针、地址加法器和指令队列缓冲器等组成 主要操作：取指令、与主存或I/O设备交换数据 【工作机理】 EU与BIU并行工作 BIU在指令队列缓冲器有2个以上字节时就不断从主存连续地址单元中取得指令送入指令队列缓冲器中，EU则不断从指令队列缓冲器中取出指令加以译码执行 异常情况: 当6个字节的指令队列缓冲器满，且EU没有主存或I/O访问请求时，BIU进入空闲状态； 当EU执行访存或I/O指令时，BIU在执行完当前周期后，暂停取指令操作，在下一总线周期执行EU所要求的主存或I/O读写操作，之后再继续BIU的取指操作 当EU执行转移、调用、返回等程序跳转类指令时，BIU会清除之前读入指令队列缓冲器的无效指令，并根据EU提供的跳转指令，重新获取跳转后的程序段指令 【创新特点】 8086处理器引入指令队列缓冲器使得预取指令变为现实，取指令和执行指令可以并行执行，从而加速了程序的运行. 8086处理器的寄存器 8086处理器有14个寄存器：8个通用寄存器（4个数据寄存器、2个指针寄存器和2个变址寄存器）、2个控制寄存器和4个段寄存器. 【数据寄存器】 16位数据寄存器：AX、BX、CX和DX，可以存放16位的源操作数或目的操作数 为了支持字节操作，每个寄存器又分为两个8位的高、低字节寄存器：AH、AL、BH、BL、CH、CL、DH、DL 【指针寄存器】 SP(Stack Pointer)堆栈指针寄存器用于存放主存中堆栈区的偏移地址，指示堆栈的当前操作位置 BP(Base Pointer)基数指针寄存器用于存放主存的基本偏移地址 【变址寄存器】 SI(Source Index)源变址寄存器，指向源操作数，具有自动修改内容的功能 DI(Destination Index)目的变址寄存器，指向目的操作数，具有自动修改内容的功能 【控制寄存器】 IP(Instruction Pointer)指令指针寄存器，指示当前指令所在存储单元的段内偏移地址；当8086 CPU根据CS和IP取得一个指令字节后，IP便自动加1(顺序执行)，指向下一条待读取指令 分析8086处理器采用分段存储管理机制，CS段寄存器存放当前指令所在内存段的首地址，IP存放当前指令所在内存段内偏移地址，以此来获取指令的地址；操作系统执行程序时，将首地址加载至CS和IP中，转移类指令执行时：如果目标地址与程序首地址所在同一段，就用目标地址修改IP；如果目标地址与程序首地址在不同段，就用目标地址同时修改CS和IP. PSW(Program Status Word)程序状态字，也称程序寄存器或标志寄存器，存放CPU工作过程中的状态信息 【标志位】——16位暂定义了9个标志位 C——进位标志位，加减运算时标识最高位出现进位或借位，受逻辑运算、位移和循环指令的影响 P——奇偶标志位，标识运算结果低8位中1的个数是否为偶数 A——半加进位标志位，加减运算时标识低4位向高4位是否进位或借位，用于对BCD结果的校正 Z——零标志位，标识运算结果是否为全0 S——符号标志位，标识运算结果是否为负数，当运算结果最高位为1时，该标志位置为1 T——单步标志位（陷阱标志位），标志位置为1时，8086处理器进入单步执行指令方式；每条指令执行完毕时，CPU测试T标志位：若T=1则在当前指令执行后产生单步中断（陷阱中断），CPU执行陷阱中断处理程序：该程序能够显示当前指令执行结果，为程序调试提供必要的信息 I——中断允许标志位，该标志位为1时，CPU可响应可屏蔽中断请求，否则不响应可屏蔽中断请求 D——方向标志位，若该标志位为1，则SI和DI在串操作指令执行中自动减量，即从高地址到低地址处理字符串，否则在串操作指令执行中自动增量 O——溢出标志位，标识带符号数运算结果是否超出8位或16位表示范围 【段寄存器】 组成：代码段寄存器 CS(Code Segment)、数据段寄存器 DS(Data Segment)、堆栈段寄存器 SS(Stack Segment)和附加段寄存器 ES(Extra Segment) 作用：存储不同属性段的段地址，与有效的段内偏移地址一起确定主存的物理地址; CS指示程序区，DS和ES指示数据区，SS指示堆栈区 8086 主存储器和 I/O结构【主存分段存储管理机制】 8086处理器的主存物理地址由段地址和段内偏移地址确定，且主存空间: $1MB$，每段大小: $64KB$，则可以划分的虚拟段数为: $$\\frac{1MB}{64KB}=\\frac{2^{20}B}{2^{16}B}=2^4$$ 因此 1MB 主存可以划分为$2^4$ 个不重叠的存储段，对于段起始地址（有效高四位）采用低位对齐策略即段起始地址的低4位全为 0，可以使得 $20$ 位段地址降低为 $16$ 位，调整段起始地址又可以指定不同存储段因此又可以划分为 $2^{16}$ 个重叠的存储段；段内偏移地址（有效低16位）可以决定段内的地址.$BIU$ 中有一个地址加法器，作用是将 $16$ 位段地址左移 $4$ 位，然后与 $16$ 位段内偏移地址相加，生成 $20$ 位的物理地址: $$Memory\\ Address=CS\\times16+IP$$ 每个存储单元的地址标识：$20$ 位物理地址/逻辑地址（段地址 $16$ 位：段内偏移地址 $16$ 位）由上式可以看出，8086处理器可以提供20位的地址，对主存单元寻址使用全部20位地址，对I/O设备端口寻址使用其低16位地址：1M的主存存储空间、64K的I/O设备端口空间【主存结构设计】 支持字节操作：8086处理器采用字节编址方式，即主存或I/O的一个地址单元内存储一个字节，实现根据地址进行1字节的存储器或I/O设备的读/写操作 支持16位数据操作：字节编址方式下使用两个连续地址来访问16位数据，并按小段模式存储，实现16位存储器或I/O操作 【主存和I/O系统的分体结构】 主存的存储空间和I/O的端口空间均采用奇、偶双体存储器，偶地址对应低字节的访问，通过低字节允许信号 $A_0=0$ 选择；奇地址空间对应高字节的访问，通过高字节允许信号 $\\overline{BHE}=0$ 选择； $\\overline{BHE}$ $A_0$ 操作说明 $0$ $0$ 高字节体与低字节体同时有效，给定地址$n^*$，从主存或 I/O 空间读写16位数据 $0$ $1$ 高字节体有效，给定奇地址$n$，从主存或 I/O 空间读写8位数据 $1$ $0$ 低字节体有效，给定偶地址$n$，从主存或 I/O 空间读写8位数据 $1$ $1$ 高字节体和低字节体无效，不能访问主存或 I/O 空间 注 $^*$:当 $n$ 为偶地址时，仅需一个总线周期就可以完成 $2$ 字节的读/写，其中 地址 $n$ 读/写数据低 $8$ 位，地址 $n+1$ 读/写数据高 $8$ 位（主存按字节编址）；当 $n$ 为奇地址时，则需要两个总线周期才可以完成 $2$ 字节的读/写，其中第一个总线周期从地址 $n$ 读/写数据低 $8$ 位，第二个总线周期从地址 $n+1$ 读/写数据高 $8$ 位（主存按字节编址）；（总线周期：CPU通过系统总线对主存或 I/O 设备进行一次读/写访问所需的时间） 【数据按属性分段存储】8086处理器设置了 $4$ 个属性的存储段：程序段（代码段）、数据段、堆栈段和附加段，并用段寄存器 $CS$、$DS$、$SS$ 和 $ES$ 分别为 $4$ 个属性段提供段地址. 在访存操作中，段地址由“默认”或“指定”的段寄存器提供： 段寄存器的默认使用情况 段寄存器的指定使用情况 通过在指令中增加一个字节的段超越前缀指令来实现：$MOV\\ AL,ES:[BX]$(从存储单元 ($ES$:$BX$) 中读取数据) 8086处理器芯片引脚 关注方面 作用 引脚功能 引脚信号的定义、作用 信号的流向 输出、输入、双向 有效电平 高电平有效、低电平有效、上升沿有效、下降沿有效 三态能力 低电平、高电平、高阻 【8086 CPU 引脚分析——工作条件】 名称 方向 有效电平 功能 $VCC$ $Input$ $+5V$ 工作电源 $GND$ $Input$ $1$ 接地端 $CLK$ $Input$ $5MHz$ 时钟信号 $RESET$ $Input$ $1$ 复位信号 $MN$ $Input$ $1$ 最小工作模式 $MX$ $Input$ $0$ 最大工作模式 $AD_0\\sim AD_{15}$ $Input/Output$ 三态 地址或数据总线 $A_{16}\\sim A_{19}$ $Output$ 三态 地址总线 $BHE$ $Output$ 三态 高字节允许信号 $INTR$ $Input$ $1$ 可屏蔽中断信号 $INTA$ $Output$ 三态 对$INTR$ 请求信号的响应信号 $READY$ $Input$ $1$ 准备就绪信号 $TEST$ $Input$ $0$ 测试信号 $DT/\\overline{R}$ $Output$ 三态 数据发送/接受控制信号 $DEN$ $Output$ 三态 数据有效信号 $ALE$ $Output$ $1$ 地址锁存信号 I. 两种工作模式下的共用信号【$A_{16}\\sim A_{19}/S_3\\sim S_6$】$A_{16}\\sim A_{19}$ 与 $S_3\\sim S_6$ 分时复用信号，$S_6$ 始终为0，$S_5$ 表示中断允许标志的状态，$S_4$、$S_3$ 指示 CPU 正在使用的寄存器；【$\\overline{BHE}/S_7$】分时复用信号，$\\overline{BHE}$ 在总线周期的 $T_1$ 时钟周期起作用；$S_7$ 为备用状态；【$RESET$】复位信号，当 $RESET$ 返回低电平时，CPU重新启动；【$READY$】CPU读写主存或 I/O 设备时，在总线周期的 $T_3$ 时钟周期采样 $READY$ 信号；当为低电平时，需要在 $T_3$ 周期之后插入等待周期 $T_{WAIT}$ ；【$INTR$】CPU 在每条指令执行的最后一个时钟周期采样该信号，以决定是否进入中断响应周期；【$NMI$】非屏蔽中断请求信号，上升沿有效，中断不可被屏蔽，$NMI$ 请求的优先级高于 $INTR$ 请求；【$\\overline{INTA}$】在响应中断过程中，由 $\\overline{INTA}$ 送出两个负脉冲，在第二个 $INTA$ 周期 CPU 获得外部中断源的中断向量码；【$DT/\\overline{R}$】数据发送/接受控制信号，高电平控制数据发送；低电平控制数据接受；【$\\overline{DEN}$】数据有效信号，表示 $D_0\\sim D_{15}$ 的数据是否有效；该信号在最小模式下由 8086 提供，低电平有效，在最大模式下由 8288 提供，高电平有效；【$ALE$】地址锁存信号，高电平有效，表示 $A_0\\sim A_{19}$ 上的地址有效； II. 最小工作模式当$MN/\\overline{MX}=1$ 时， 8086 CPU工作在最小模式，微机中只有一个处理器，系统总线仅由CPU信号形成； 【$M/\\overline{IO}$(输出、三态)】低电平访问 I/O 设备；高电平访问主存；【$\\overline{RD}$(输出、三态)】低电平有效，表示CPU正在读主存或 I/O 接口；【$\\overline{WR}$(输出、三态)】低电平有效，表示CPU正太写主存或 I/O 接口；【$HOLD$(输入)】保持请求信号，高电平有效，表示某总线设备请求使用系统总线；【$HLDA$(输出、三态)】保持允许信号，高电平有效，表示 CPU 对 HOLD 请求的响应信号； III. 最大工作模式当$MN/\\overline{MX}=0$ 时， 8086 CPU工作在最大模式，微机中除了主处理器8086，还允许接入其他协处理器（运算处理器8087、I/O 处理器8089）构成多微处理器系统，系统总线由 8086 和总线控制器 8288 提供的信号共同形成； 【$\\overline{S_0}$、$\\overline{S_1}$、$\\overline{S_2}$(输出、三态)】表示该总线周期存取哪种设备的状态信号，是 $8288$ 产生控制信号的依据； 【$8288$ 不同之处】 8288 的 $DEN$ 信号为高电平有效； 8288 通过 $\\overline{MRDC}$(存储器读信号)，$\\overline{MWTC}$(存储器写信号)、$\\overline{IORC}$(I/O 读信号)、$\\overline{IOWC}$(I/O 写信号)来控制对主存或I/O的访问； 【$\\overline{LOCK}$(输出、三态)】总线索存信号，低电平有效，信号有效期间，总线请求信号被封锁； 【$QS_0, QS_1$(输出)】表示指令队列缓冲器存取的状态信号: QS1 QS0 性能 0 0 无操作 0 1 队列中操作码的第一个字节 1 0 队列空 1 1 队列中非第一个操作码字节 8086处理器工作时序【指令周期】在冯诺依曼计算机中，将CPU取得并执行一条指令所花的时间定义为一个指令周期。【总线周期】CPU通过系统总线对主存或I/O设备进行一次读/写访问所需的时间。8086 CPU的一个总线周期由 4 个时钟周期（T1、T2、T3、T4）组成。【基本总线时序】等待周期 $T_{WAIT}$ 一般插入到 $T_3$ 后面 I. 写总线周期(Concise Mode)$T_1:\\ Load\\ Address$ $A_{16}\\sim A_{19}/S_3\\sim S_6\\Longrightarrow A_{16}\\sim A_{19}$ $\\overline{BHE}/S_7\\Longrightarrow \\overline{BHE}$ $AD_0\\sim AD_{15}\\Longrightarrow A_0\\sim A_{15}$ $A_0\\sim A_{19}, \\overline{BHE}\\stackrel{ALE}\\Longrightarrow$ 锁存器 $T_1:\\ Select\\ Interface$ $M/\\overline{IO}==1\\Longrightarrow A_0\\sim A_{19}(Memory\\ Unit)$ $M/\\overline{IO}==0\\Longrightarrow A_0\\sim A_{15}\\ with\\ A_{16}\\sim A_{19}=0(IO\\ Interface)$ $T_2:\\ Load\\ Data$$AD_0\\sim AD_{15}\\Longrightarrow D_0\\sim D_{15}$ $\\overline{WR},\\ M/\\overline{IO},\\ A_0\\sim A_{19}\\Longrightarrow Memory/IO(Minimal\\ Mode)$ $\\overline{MWTC},\\ \\overline{IOWC},\\ A_0\\sim A_{19}\\Longrightarrow Memory/IO(Maximal\\ Mode)$ $T_3:\\ Continue\\ T_2$READY信号：解决主存或I/O接口实际写入时间长于CPU提供的时间的问题 $(T_3$ 开始时刻即下降沿 $):\\ Test\\ READY==0$ $\\Longrightarrow Insert\\ T_{WAIT}$ $(T_{WAIT}$ 开始时刻即下降沿 $):\\ Test\\ READY==0$ $\\Longrightarrow Insert\\ T_{WAIT}\\Rightarrow Continue\\ Loop$ $Test\\ READY==1\\Longrightarrow T_4$ $T_4:\\ Restore\\ States$ II. 读总线周期(Concise Mode)——对比写总线周期$DT/\\overline{R}$ $DT/\\overline{R}==0,Bus\\stackrel{data}\\Longrightarrow CPU(Read\\ Mode)$ $DT/\\overline{R}==1,CPU\\stackrel{data}\\Longrightarrow Bus(Write\\ Mode)$ III. 中断响应周期(Concise Mode)$Response\\ Conditions:$ $INTR == 1\\ and\\ IF == 1\\ (Request\\ and\\ permission)$ $INTA\\ Period:\\ from\\ T_2\\ to\\ T_4$$First\\ INTA\\ Period$: 通知提出 $INTA$ 请求的中断源，请求已得到响应，并封锁总线 $Second\\ INTA\\ Period$: 总线封锁信号 $\\overline{LOCK}$ 无效，中断源 $\\stackrel{中断向量码}\\Longrightarrow D_0\\sim D_7$ $D_0\\sim D_7\\stackrel{中断向量码}\\Longrightarrow CPU$ 8086系统总线的形成系统总线：地址总线(AB)、数据总线(DB)、控制总线(CB) I. 最小模式的系统总线8086 CPU 工作在最小模式下，系统总线信号全部来自CPU 地址总线 AB: $A_0\\sim A_{19}$、$\\overline{BHE}/S_7$ (ALE信号由 8086 CPU 提供) 双向数据总线 DB: $D_0\\sim D_{15}$ 控制总线 CB: 8086 CPU 在最小模式下提供的所有控制信号 II. 最大模式的系统总线8086 CPU 工作在最大模式下，系统总线信号来自 CPU 和总线控制器 8288 地址总线 AB: $A_0\\sim A_{19}$、$\\overline{BHE}/S_7$ (ALE信号由总线控制器 8288 提供) 双向数据总线 DB: $D_0\\sim D_{15}$ 控制总线 CB: 最大模式下的控制总线信号由 8086 CPU 和总线控制器 8288 共同提供 III. 常用芯片引脚分布【74LS373——8位锁存器】 【74LS245——双向驱动器】 【74LS244——单向驱动器】 第三章 Intel 处理器指令系统及汇编语言第一讲 指令寻址方式操作数的寻址方式指令格式：操作码+操作数（操作数本身/操作数地址/地址的一部分/操作数地址的指针/其他操作数信息） 立即寻址 Example: MOV AX, im 指令格式：操作数包含在指令中存储形式：操作码 $[n]+imL[n+1]+imH[n+2]$（小端存储）存储位置：与操作码一起存放在代码段区域中指令用途：主要用来给寄存器/存储器赋初值 Operand = AH &lt;&lt; 8 + AL = imH &lt;&lt; 8 + imL 直接寻址 Example: MOV AX, DS:[offset] 指令格式：16位段内偏移地址包含在指令中存储形式：操作码 $[n]+offsetL[n+1]+offsetH[n+2]$（小端存储）存储位置：与操作码一起存放在代码段区域中 Address = DS &lt;&lt; 4 + offsetOperand = AH &lt;&lt; 8 + AL = Mem[Address] &lt;&lt; 8 + Mem[Address+1] 寄存器寻址 Example: MOV DS, AX 指令格式：操作数包含在CPU的内部寄存器中存储位置：存放在寄存器中 Operand = (Reg) 寄存器间接寻址 Example: MOV AX, [SI] 指令格式：操作数的16位段内偏移地址存放在 $SI/DI/BP/BX$ 中, 其中 $SI/DI/BX$ 间接寻址时操作数通常存放在现行数据段中; $BP$ 间接寻址时操作数存放在堆栈段中存储位置：操作数存放在存储器中 Address = (DS) &lt;&lt; 4 + (SI/DI/BX) / (SS) &lt;&lt; 4 + (BP)Operand = AH &lt;&lt; 8 + AL = Mem[Address] &lt;&lt; 8 + Mem[Address+1] 寄存器相对寻址 Example: MOV AX, DISP[SI] 指令格式：指令中存放段内偏移地址的寄存器 $SI/DI/BX/BP$ + $8/16$ 位带符号相对地址偏移量 $DISP$存储位置：操作数存放在存储器中 Address = (DS) &lt;&lt; 4 + (SI/DI/BX) + DISP / (SS) &lt;&lt; 4 + (BP) + DISPOperand = AH &lt;&lt; 8 + AL = Mem[Address] &lt;&lt; 8 + Mem[Address+1] 基址、变址寻址 Example: MOV AX, [BX][SI] 指令格式：指令指定一个基址寄存器 $BX/BP$ 和一个变址寄存器 $SI/DI$存储位置：操作数存放在存储器中 Address = (DS) &lt;&lt; 4 + (BX) + (SI/DI) / (SS) &lt;&lt; 4 + (BP) + (SI/DI)Operand = AH &lt;&lt; 8 + AL = Mem[Address] &lt;&lt; 8 + Mem[Address+1] 基址、变址、相对寻址 Example: MOV AX, [BX][SI] 指令格式：指令指定一个基址寄存器 $BX/BP$、一个变址寄存器 $SI/DI$ 和相对偏移地址 $DISP$存储位置：操作数存放在内存中 Address = (DS) &lt;&lt; 4 + (BX) + (SI/DI) + DISP / (SS) &lt;&lt; 4 + (BP) + (SI/DI) + DISPOperand = AH &lt;&lt; 8 + AL = Mem[Address] &lt;&lt; 8 + Mem[Address+1] 隐含寻址 Example: 乘法指令 MUL BL 十进制调整指令 DAA 串传送指令 MOVSW 指令格式：操作数的地址隐含在指令操作码中 转移地址的寻址方式 段内直接寻址（相对寻址） Example: JMP DSP1 指令格式：指令指明8位或16位带符号相对地址偏移量 $DISP$(补码表示)指令说明：相对寻址指的是在程序计数器的基础上加上偏移量进行相对位移 Jump Address = (CS) &lt;&lt; 4 + ((IP) + DISP) 段内间接寻址 Example: JMP CX 指令格式：指令指明存放段内偏移地址的寄存器或存储器单元地址，按指令码中规定的寻址方式取得转移地址指令说明：间接寻址指的是通过寄存器或存储器单元获取得到新的段内偏移地址赋值给程序计数器而段地址不变 Jump Address = (CS) &lt;&lt; 4 + (CX) 段间直接寻址 Example: JMP FAR PTR OPRD PTR 属性运算符: 给指令中的操作数指定一个临时属性，暂时忽略当前属性 作用于操作数时，重载操作数的类型(字节或字)或属性(NEAR或FAR) NEAR, SHORT, FAR 表示操作数的属性 SHORT 表示段内短转移, IP 偏移量: $-128\\sim127$ 字节($8$ 位，一个字节) NEAR 表示段内近转移, IP 偏移量: $-32768\\sim32767$ 字节($16$ 位，两个字节) FAR 表示段间转移 指令格式：指令码指明段地址和偏移地址所在存储单元的首地址: OPRD (标号或立即数, 标号会在编译时由 CPU 转换为对应的地址)指令说明：操作码后连续四个字节，低字表示段内转移地址，高字表示段地址，直接赋值给 IP, CS (直接寻址) Jump Address = (CS) &lt;&lt; 4 + (IP) 段间间接寻址 Example: JMP DWORD PTR [BP][DI] 指令格式：依据指令码寻址方式确定存储单元的首地址，前两个单元：段内偏移地址，后两个单元：段地址指令说明：需要确定段地址 ($16$位) 和段内偏移地址 ($16$位) $32$ 位信息，只适用于存储器寻址方式 Address = SS + BP + DICS = Mem[Address+3] &lt;&lt; 8 + Mem[Address+2], IP = Mem[Address+1] &lt;&lt; 8 + Mem[Address]Jump Address = (CS) &lt;&lt; 4 + (IP) 第二讲 汇编语言 引言：汇编语言的用途 嵌入式系统中，程序大小和运行速度需要高度优化 设计驱动程序、操作系统内核以及编译程序 高级语言中嵌入汇编 汇编语言的访问层次 OS函数 BIOS功能（一组固化到计算机内主板上一个ROM芯片上的程序，保存着计算机最重要的基本输入输出的程序、开机后自检程序和系统自启动程序） 硬件 汇编语言程序结构 操作系统装入程序：初始化CS为正确的代码段地址，初始化SS为正确的堆栈段地址 ASSUME 伪指令：指明段与段寄存器的对应关系 编译、链接和运行程序 汇编语言基本元素 […] 中的参数可选，{…|…} 多选一（由’|’隔开） 整数常量 digits[radix] radix 进制 radix 进制 radix 进制 radix 进制 radix 进制 h 十六进制 b 二进制 q/o 八进制 d 十进制 r 编码实数 保留字 指令助记符: 标识特定的指令，Example: MOV, ADD 伪指令: 指明 MASM 如何编译程序 属性: 变量和操作数的尺寸以及使用方式的说明 运算符: 常量表达式中 预定义符号: Example: @data (编译时返回整数常量) 定义数据伪指令 符号常量伪指令不占用任何实际的存储空间 等号伪指令 Example:COUNT = 500mov cx COUNT EQU伪指令 TESTEQU伪指令 操作符 OFFSET 返回数据标号的偏移地址（标号距数据段开始的距离，以字节为单位） 保护模式下偏移: $32$ 位，实模式下偏移: $16$ 位 Example: Address[bVal] = $00300$h 12345.databVal DB ?wVal DW ?mov si, OFFSET bVal ; SI=0000Hmov si, OFFSET wVal ; SI=0001H SEG 返回变量或数据标号的段基址 Example: 1234.databuffer DB ?mov ax, SER buffermov ds, ax PTR 重载操作数的默认尺寸 标准数据类型：DB, SDB, WORD, SWORD, DWORD, SDWORD, FWORD, QWORD, TDB Example: 1234567.datamyDouble DD 12345678h ; Data Double Words: 32位数据.codemov ax, myDouble ; ax为16位寄存器，数据长度不一致引发报错mov ax, DW PTR myDouble ; Data Word: 16位数据, ax=5678hmov ax, DW PTR [myDouble+2] ; ax=1234hmov bl, DB PTR myDouble ; Data Byte: 8位数据, bl=78h TYPE 返回按字节计算的变量单个元素的大小 Example: 12345.datavar1 DB ? ; Data Bytevar2 DW ? ; Data Wordvar3 DD ? ; Data Double Wordsvar4 DQ ? ; Data Quanter Words 表达式 值 TYPE var$1$ $1$ TYPE var$2$ $2$ TYPE var$3$ $4$ TYPE var$4$ $8$ LENGTHOF 计算数组中元素个数 Example: 123456.dataDB1 DB 10, 20, 30array1 DW 30 DUP(?), 0, 0array2 DW 5 DUP(3, DUP(?))array3 DD 1, 2, 3, 4digitStr DB &quot;12345678&quot;, 0 表达式 值 LENGTHOF DB$1$ $3$ LENGTHOF array$1$ $30+2$ LENGTHOF array$2$ $5\\times3$ LENGTHOF array$3$ $4$ LENGTHOF digitStr $9$ SIZEOF SIZEOF返回值=LENGTHOF返回值 $\\times$ TYPR返回值 Example: 12.dataintArray DW 32 DUP(0) 表达式 值 TYPE intArray $2$ LENGTHOF intArray $32$ SIZEOF intArray $64$ 第三讲 指令系统数据传送指令 操作数类型 立即数 (Immediate) Label imm: $8$、$16$、$32$ 位立即数 imm8: $8$ 位立即数 imm16: $16$ 位立即数 imm32: $32$ 位立即数 立即数只能用作源操作数 立即数类型 $8$ 位 $16$ 位 无符号数 $00H\\sim FFH(0\\sim 255)$ $0000H\\sim FFFFH(0\\sim 65535)$ 有符号数 $80H\\sim 7FH(-128\\sim 127)$ $8000H\\sim 7FFFH(-32768\\sim 32767)$ 寄存器操作数 (Register) Label reg: 任意的通用寄存器 sreg: $16$ 位段寄存器 (CS, DS, SS, ES, FS, GS) r8: AH, AL, BH, BL, CH, CL, DH, DL r16: AX, BX, CX, DX r32: EAX, EBX, ECX, EDX, ESI, EDI, EBP, ESP SI, DI, SP, BP 只能存放字操作数 不允许将立即数传送到段寄存器 内存操作数 (Memory) Label mem: $8$、$16$ 或 $32$ 内存操作数 不允许两个操作数同时为存储器操作数 Example: 12345.codemov [8000H], [1000H] ; Errormov buff1, buff2 ; Errormov al, [buff2] ; 编译器自动将名称转换为地址偏移量mov al, [buff1+1] ; 直接偏移操作数 数据传送指令 MOV 指令格式：mov desination, source Special Rules: 两个操作数类型尺寸必须一致 目的操作数不能是 CS 和 IP (保护程序执行) 特殊用法 (AX作桥梁) 123456789; 存储器 -&gt; 存储器MOV AX, MEM1MOV MEM2, AX; 段寄存器 -&gt; 段寄存器MOV AX, DSMOV ES, AX; 立即数 -&gt; 段寄存器MOV AX, 1000HMOV DS, AX MOVZX (move with zero-extend, 高位填充 $0$) 指令格式: movzx r32, r/m8 movzx r32, r/m16 movzx r16, r/m8 Example: 1234.codemov bx, 0A69Bhmovzx eax, bxmovzx ax, bl MOVSX (move with sign-extend, 高位填充符号位) 指令格式: movzx r32, r/m8 movzx r32, r/m16 movzx r16, r/m8 Example: 12345.codemov bx, 0A69hmovsx eax, bx ; eax = FFFFA69Bhmovsx edx, bl ; edx = FFFFFF9Bhmovsx ax, bl ; ax = FF9Bh 字节-字转换命令 指令格式: CBW: 把 AL 的符号位复制到 AH CWD: 把 AX 的符号位复制到 DX 用途: 用于有符号数的除法 XCHG (exchange data) 指令格式: xchg reg, reg xchg reg, mem xchg mem, reg Rules: 两个操作数至少有一个在寄存器中 操作数不能是段寄存器和立即数 操作数类型一致 Example: 1234.codemov ax, val1xchg ax, val2mov val1, ax 堆栈操作指令概述 以字 ($16$ 位) 为单位进行数据压入和弹出操作，但是存储单元仍是 $8$ 位 SS 指示堆栈段的段基址，SP 始终指向堆栈的顶部，即最后一个存储单元 进栈方向从高地址向低地址发展 (SP 的初值决定了所用堆栈区的大小) 堆栈用途 作为临时保存区域，保存局部变量 备份寄存器状态，以便恢复其原始值 CALL 指令执行时，CPU 用堆栈保存当前过程的返回地址 调用过程中，通过堆栈传递参数 PUSH 指令格式: PUSH SRC (SRC 为 $16/32$ 位操作数，对应 SP 以 $2/4$ 递减) Example: 12345.codepush ax; (SP-1) &lt;- AH; (SP-2) &lt;- AL; (SP) &lt;- (SP)-2 小端存储，注意括号寻址 POP 指令格式: POP DEST (DEST 为 $16/32$ 位操作数，对应 SP 以 $2/4$ 递增) 指令功能: 从堆栈顶部连续取两个单元的内容送到 DEST 指定的位置 Example: 1234.codepop ax ; 寄存器pop ds ; 数据段寄存器pop [bx] ; 内存单元 PUSHFD ($32$ 位程序) 在堆栈上压入 $32$ 位 EFLAGS 寄存器的值 Example: 1234567.datasaveFlags DWORD ?.codepushfd ; 标志入栈，不需指定pop saveFlags ; 拷贝到变量push saveFlags ; 将保存的标志入栈popfd ; 恢复标志，不需指定 POPFD ($32$ 位程序) 将堆栈顶部的值弹出并送至 EFLAGS 寄存器 PUSHF (实地址模式) 在堆栈上压入 $16$ 位 FLAGS 寄存器的值 POPF (实地址模式) 将堆栈顶部的值弹出 $16$ 位的值并送至 FLAGS 寄存器 PUSHAD (Push All Data) 在堆栈上按顺序压入所有 $32$ 位通用寄存器: EAX, ECX, EDX, EBX, ESP, EBP, ESI, EDI POPAD (Pop All Data) 在堆栈上按相反的顺序弹出所有 $32$ 位通用寄存器: EDI, ESI, EBP, ESP, EBX, EDX, ECX, EAX PUSHA ($80286$ 处理器) 在堆栈上按顺序压入所有 $16$ 位通用寄存器: AX, CX, DX, BX, SP, BP, SI, DI POPA ($80286$ 处理器) 在堆栈上按相反的顺序弹出所有 $16$ 位通用寄存器: DI, SI, BP, SP, BX, DX, CX, AX LEA (Load Effective Address) 指令格式: LEA reg, mem 指令功能: 将指定存储器的 $16$ 位偏移地址送到指定的寄存器 Example: 123LEA BX, BUFFER ; 符号地址为 BUFFER 的存储单元的偏移地址取到 BX 中MOV BX, BUFFER ; 符号地址为 BUFFER 的存储单元中内容送至 BX 中LEA BX, BUFFER = MOV BX, OFFSET BUFFER ; LEA 可以取动态的地址, OFFSET 只能取静态的地址 LDS (Load DS) 指令格式: LDS reg, mem32 指令功能: $mem32$ 开始的四个内存单元 $\\rightarrow$ DS:reg (高 $16$ 位 $\\rightarrow$ DS, 低 $16$ 位 $\\rightarrow$ reg) LES (Load ES) 指令格式: LES reg, mem32 指令功能: $mem32$ 开始的四个内存单元 $\\rightarrow$ ES:reg (高 $16$ 位 $\\rightarrow$ ES, 低 $16$ 位 $\\rightarrow$ reg) Special Rules(LEA, LDS, LES): 源操作数必须是一个内存操作数 目的操作数必须是一个 $16$ 位的通用寄存器 算术运算类指令 加法和减法指令 INC 指令格式: INC OPRD 指令功能: 操作数 + $1$ $\\rightarrow$ 目的操作数 DEC 指令格式: DEC OPRD 指令功能: 操作数 - $1$ $\\rightarrow$ 目的操作数 Special Rules(INC, DEC): 不影响 CF, 影响 OF, SF, ZF, AF, PF ADD 指令格式: ADD DEST, SRC 指令功能: 源操作数+目的操作数 $\\rightarrow$ 目的操作数 ADC 指令格式: ADC DEST, SRC 指令功能: 源操作数+目的操作数+ CF $\\rightarrow$ 目的操作数 指令用途: 多用于多字节加法运算中 Example: 有两个 $4$ 字节的数分别存放在 FIRST 和 SECOND 开始的两个存储区中，试将两数相加并将结果放回 FIRST 存储区中 1234567.codemov ax, FIRST ; 第一个数的低 16 位送 axadd ax, SECOND ; 两数的低 16 位相加送 axmov FIRST, ax ; 低 16 位相加结果送 FIRST 和 FIRST+1 单元 (小端存储)mov ax, FIRST+2 ; 第一个数的高 16 位送 axadc ax, SECOND+2 ; 高 16 位同低位进位相加送 ax mov FIRST+2, ax ; 高 16 位相加的结果存入 FIRST+2 和 FIRST+3 单元 SUB 指令格式: SUB DEST, SRC 指令功能: 目的操作数 - 源操作数 $\\rightarrow$ 目的操作数 SBB 指令格式: SBB DEST, SRC 指令功能: 目的操作数 - 源操作数 - CF $\\rightarrow$ 目的操作数 指令用途: 多用于多字节减法运算中 影响标志位: CF, ZF, SF, OF, AF, PF NEG 指令格式: NEG OPRD 指令功能: 0 - (OPRD) $\\rightarrow$ OPRD (OPRD 可以为 REG/MEM) 指令说明: 将操作数按位求反、末位加一 (求补运算) 影响标志位: CF, ZF, SF, OF, AF, PF Example: 12345.dataval DB FCH.codemov al, valneg al ; (al) = 04H, CF = 1 乘法和除法指令 MUL 指令格式: MUL r/m$8$/m$16$/m$32$ 指令功能: 无符号乘法 被乘数 乘数 积 CF=$1$的条件 AL r/m$8$ AX AH$\\neq0$ AX r/m$16$ DX:AX DX$\\neq0$ EAX r/m$32$ EDX:EAX EDX$\\neq0$ IMUL 指令格式: IMUL r/m$8$/m$16$/m$32$ 指令功能: 有符号乘法 指令说明: 积的高半部分不是第半部分的符号扩展，则设置 CF 和 OF DIV 指令格式: DIV r/m8/m16/m32) 指令功能: 无符号除法，影响 OF 被除数 除数 商 余数 AX r/m8 AL AH DX:AX r/m16 AX DX EDX:EAX r/m32 EAX EDX IDIV 指令格式: IDIV r/m$8$(/$16$/$32$) 指令功能: 有符号除法，影响 OF 被除数 除数 商 余数 AX r/m8 AL AH DX:AX r/m16 AX DX EDX:EAX r/m32 EAX EDX CBW 将AL中的符号位扩展到AH CWD 将AX中的符号位扩展到DX CDQ 将EAX中的符号位扩展到EDX 溢出 商太大，目的操作数无法容纳 -&gt; 使用32位除数 除数 = 0 $\\rightarrow$ 跳过，不执行 逻辑运算和移位指令 逻辑运算类指令 AND 指令格式: AND DEST, SRC 指令功能: DEST &amp; SRC $\\rightarrow$ DEST (对特定位清零，同时保留其他位) 影响的标志位: 清除 OF 和 CF, 修改 SF, ZF, PF Example: 大小写字母的转换(大写转小写方法：将位 $5$ 设置为 $1$, 加32得到小写字母) ‘a’: 61h, 即 $01\\boldsymbol{1}00001$ ‘A’: 41h, 即 $01\\boldsymbol{0}00001$ 123456789.data array DB 50 DUP(?).code mov cx, LENGTHOF array mov si, OFFSET arrrayL1: and byte PTR [si], 11011111b inc si loop L1 OR 指令格式: OR DEST, SRC 指令功能: DEST | SRC $\\rightarrow$ DEST (对特定位设 $1$，同时保留其他位) Example: $0\\sim9$ 数字转换为 ASCII 码数字 (方法：将位 $4$ 和位 $5$ 设置为 $1$, 数字加48得到数字字符) $05$h: 00000101b $30h$: 00$\\boldsymbol{11}$0000b 123.code mov dl, 05h or dl, 30h XOR 指令格式: XOR DEST, SRC 指令功能: DEST ^ SRC $rightarrow$ DEST (对特定位取反，同时保留其他位) Example: 判断 $16$ 位或 $32$ 位值奇偶性 简单数据加密 $(X\\oplus Y)\\oplus Y=X$ 对 Reg 清零 (自身异或) 把 Reg/Mem 得某几位取反 (与 $1$ 异或) 123.code mov ax, 64C1h xor ah, al 寄存器清零 MOV AX, $0$ XOR AX, AX AND AX, $0$ SUB AX, AX NOT 指令格式: NOT Reg/Mem 指令功能: 对数据位取反，结果为反码 (不影响任何状态标志) 比较测试指令 CMP 指令格式: CMP DEST, SRC 指令功能: 目的操作数 - 源操作数, 但不回送结果只影响标志位 影响标志位: CF, ZF, SF, OF, AF, PF 判读比较结果 两个无符号数 CMP Result ZF CF DEST &lt; SRC 0 1 DEST &gt; SRC 0 0 DEST = SRC 1 0 两个有符号数 CMP Result Sign DEST &lt; SRC SF$\\neq$ OF DEST &gt; SRC SF=OF DEST = SRC ZF=1 TEST 指令格式: TEST DEST, SRC 指令功能: DEST &amp; SRC, 根据结果设置标志位但不回送结果 影响标志位: 清除 OF, CF；修改 SF, ZF, PF Example: 测试操作数的某一位是 $0$ 或 $1$ 123.codetest al, 80H ; 检查 al 操作数是否是负数 (D7=1)jnz MINUS ; 条件跳转指令, 转到 MINUS 12.codetest al, 00001001H ; 检查 al 操作数第 0 位和第 3 位是否同时为 0 控制转移指令 转移指令概述 实质: 改变 IP(或 CS) 的内容 所有转移指令不会影响标志位 无条件转移指令 JMP JMP disp (段内直接转移) CS 保持不变, 指令中 $8/16$ 位偏移量加到 IP JMP SHORT OPRD $8$ 位偏移地址: $-128\\sim +127$ JMP NEAR PTR BUF $16$ 位偏移地址: $-32768\\sim +32767$ Example: 1234.codeJMP 0120H ; 直接转向JMP SHORT LR ; 8 位偏移量JMP NEAR PTR BUF ; 16 位偏移量 JMP reg/mem (段内间接转移) CS 保持不变, reg/mem 中的 $16$ 位偏移地址送到 IP $16$ 位偏移地址指的是段内偏移地址，而不是相对于 IP 的偏移量 123.codeJMP [BX + DI] ; (DS) = 3000H, (BX) = 1300H, (DI) = 1200H, (32500H) = 2350H; (IP) = 2350H JMP segment:offset (段间直接转移) 指令中的 $16$ 位段地址和 $16$ 位偏移地址送到 CS 和 IP (立即数) 123.codeJMP 2000H:1000H; (CS) = 2000H, (IP) = 1000H 注：直接地址为符号地址时，段间直接转移指令中的符号地址前应加操作符 FAR PTR JMP mem$32$ (段间间接转移) mem$32$ 中的 $16$ 位段地址和 $16$ 位偏移地址送到 CS 和 IP (两个相邻字) 1234.codeJMP DWORD PTR[SI] ; 表示转移地址为一个双字; (DS) = 4000H, (SI) = 1212H, (41212H) = 1000H, (41214H) = 4A00H; (IP) = 1000H, (CS) = 4A00H 条件跳转指令 声明全局变量 (变量后面紧跟 “::”) 基于特定 CPU 标志值的跳转指令 助记符 标志位 标志值/跳转条件 JZ ZF(零标志位) 1 JNZ ZF(零标志位) 0 JC CF(进位标志位) 1 JNC CF(进位标志位) 0 JO OF(溢出标志位) 1 JNO OF(溢出标志位) 0 JS SF(符号标志位) 1 JNS SF(符号标志位) 0 JP PF(奇偶标志位) 1 JNP PF(奇偶标志位) 0 依据相等比较的跳转指令 指令格式: CMP LeftOp, RightOp Jxx Label 助记符 跳转条件 JE LeftOp = RightOp JNE LeftOp$\\neq$ RightOp JCXZ CX=0 JECXZ ECX=0 基于无符号整数比较结果的跳转指令 指令格式: CMP LeftOp, RightOp Jxx Label 指令助记: Z:zero, E:equal, A:above, B:below 助记符 跳转条件 JA LeftOp &gt; RightOp JAE LeftOp &gt;= RightOp JB LeftOp &lt; RightOp JBE LeftOp &lt;= RightOp JNA LeftOp &lt;= RightOp JNAE LeftOp &lt;= RightOp JNB LeftOp &gt;= RightOp JNBE LeftOp &gt;= RightOp 基于有符号整数比较结果的跳转指令 指令格式: CMP LeftOp, RightOp Jxx Label 指令助记: Z:zero, E:equal, G:greater, L:less 助记符 跳转条件 JG LeftOp &gt; RightOp JGE LeftOp &gt;= RightOp JL LeftOp &lt; RightOp JLE LeftOp &lt;= RightOp JNG LeftOp &lt;= RightOp JNGE LeftOp &lt;= RightOp JNL LeftOp &gt;= RightOp JNLE LeftOp &gt;= RightOp 123MOV al, statusTEST al, 00100000b ; 测试 bit5=1jnz OK 123MOV al, statusTEST al, 00010011b ; 测试 bit0, bit1, bit4至少一个1jnz OK 1234MOV al, statusAND al, 10001100b ; 取出 bit2, bit3, bit7CMP al, 10001100b ; 测试 bit2, bit3, bit7不全为 1jnz OK 循环指令 LOOP LOOP Label 执行操作: (CX)$-1$ $\\rightarrow$ CX 循环条件: (CX) $\\neq0$ 转至 Label 处循环执行 等价指令: DEC CX, JNZ Label 循环目的地址与当前地址相距范围：$-128\\sim+127$字节，机器指令平均 $3$ 字节, 单词循环平均最多包含约 $42$ 条指令 1234567891011.datacount DW ?.code mov cx, 100L1: mov count, cx mov cx, 20L2: . . loop L2 mov cx, count loop L1 LOOPZ (Loop if zero, Loop if equal) 指令等价：LOOPE 指令格式: LOOPZ Label 执行操作: (CX) $-1\\rightarrow$ CX 循环条件: (CX) $\\neq0 \\wedge$ ZF $=1$ LOOPNZ (Loop if not zero, Loop if not equal) 指令等价：LOOPNE 指令格式: LOOPNZ Label 执行操作: (CX) $-1\\rightarrow$ CX 循环条件: (CX) $\\neq0 \\wedge$ ZF $=0$ 移位和循环移位指令 助记符 指令含义 影响标志位 助记符 指令含义 影响标志位 助记符 指令含义 影响标志位 SHL 逻辑左移 CF,OF,PF,SF,ZF ROL 循环左移 CF,OF SHLD 双精度左移 CF,OF SHR 逻辑右移 CF,OF,PF,SF,ZF ROR 循环右移 CF,OF SHRD 双精度右移 CF,OF SAL 算术左移 CF,OF,PF,SF,ZF RCL 带进位的循环左移 CF,OF SAR 算术右移 CF,OF,PF,SF,ZF RCR 带进位的循环右移 CF,OF 算术移位——把操作数看做是有符号数 逻辑移位——把操作数看做无符号数 $8086$/$8088$: imm8=1, $80286$ 以上: imm8=任意整数 SHL/SAL 指令格式: SHL/SAL mem/reg, imm8/CL 指令功能: 逻辑左移/算术左移 Example 12345SAL al, 1 ; 2xMOV ah, alSAL al, 1 ; 4xSAL al, 1 ; 8xADD al, ah ROL 指令格式: ROL mem/reg, imm8/CL 指令功能: 循环左移 指令特点: 循环左移出的最高位同时给 CF Example 123MOV al, 10000000bROL al, 1 ; 10000000b -&gt; 00000001b, CF = 1ROL al, 1 ; 00000001b -&gt; 00000010b, CF = 0 123MOV al, 26HMOV cl, 4ROL al, cl ; AL=62H ROR 指令格式: ROR mem/reg, imm8/CL 指令功能: 循环右移 指令特点: 循环右移出的最低位同时给 CF Example 123MOV al, 00000001bROR al, 1 ; 00000001b -&gt; 10000000b, CF = 1ROR al, 1 ; 10000000b -&gt; 01000000b, CF = 0 123MOV al, 26HMOV cl, 4ROL al, cl ; AL=62H 12345678.data ArraySize = 3 array DWORD ArraySize DUP(99999999H).code MOV esi, 0 SHR array[esi+8], 1 ; 高位的最低位移进 CF RCR array[esi+4], 1 ; 低位移位必须带 CF RCR array[esi], 1 SHLD 至少是Intel $386$处理器 指令格式: SHLD DEST, SRC, CL/imm8 功能: 将目的操作数左移指定的位数，低位空出来的位用源操作数的高位填充 影响标志: SF, ZF, AF, PF, CF SHRD 至少是Intel $386$处理器 指令格式: SHRD DEST, SRC, CL/imm8 功能: 将目的操作数右移指定的位数，高位空出来的位用源操作数的低位填充 影响标志: SF, ZF, AF, PF, CF ASCII和十进制调整指令 ASCII AAA 加法之后进行ASCII码调整 例题 AAS 减法之后进行ASCII码调整 例题 AAM 乘法之后进行ASCII码调整 例题 AAD 除法之后进行ASCII码调整 例题 调整算法 十进制 DAA 将ADD或ADC指令执行后AL中的结果转换成压缩的十进制格式 DAS 将SUB或SBB指令执行后AL中的结果转换成压缩的十进制格式 例题 调整算法 字符串操作指令 寻址方式 源操作数指针 DS:SI 目的操作数指针 ES:DI SI是DS段中的偏移 DI是ES段中的偏移 ES ES通常开始时设为同样的段值 SI DI的值会自动修改（方向标志位DF=0 增；DF=1 减；CLD 清除方向标志位，STD 设置方向标志位） 基本字符串指令 MOVSB(/SW/SD)移动 拷贝DS:(E)SI寻址的内存操作数至ES:(E)DI 例题 CMPSB(/SW/SD)比较 SCASB(/SW/SD)扫描比较内存中由DS:(E)SI寻址和ES:(E)DI寻址的字符串。源﹣目的 例题 SCASB(/SW/SD)扫描 扫描ES:(E)DI指向的内存字符串查找与累加器匹配的值 例题 STOSB(/SW/SD)存储 将累加器内容存储到由ES:(E)DI寻址的内存中 例题 LODSB(/SW/SD)装入 将由DS:(E)SI寻址的内存单元装入累加器中 例题 使用重复前缀 REP 当CX&gt;0时重复 REPZ, REPE 当ZF=1且CX&gt;0时重复 REONZ, REPNE 当ZF=0且CX&gt;0时重复 实现用一条指令处理整个数组 端口、过程与逻辑运算指令 使用I/O端口控制硬件 端口范围：0~FFFFh 端口作用：传送数据，返回状态，控制 指令 IN 累加器，端口地址 累加器：AL\\AX\\EAX 端口地址 0~FFh之间的常量 包含0~FFFFh 之间值的DX寄存器 OUT 端口地址，累加器 例题 过程的定义和使用 PROC伪指令 CALL与RET指令 CALL指令执行时，处理器自动完成压栈；RET指令执行时，处理器自动完成出栈 过程可以嵌套调用 局部标号(L1:)和全局标号(L1::) 例题 逻辑运算指令 设置和清除单个CPU标志 12stc ;设置进位标志clc ;清除进位标志 12and al，0 ;设置零标志or al,1 ;清除零标志 12or al,80h ;设置符号标志and al,7Fh ;清除符号标志 123mov al,7Fh ;AL=+127inc al ;AL=-128, OF=1or eax,0 ;清除溢出标志 程序设计举例 顺序程序 例题 分支程序 例题1 例题2 循环程序 两种结构两种结构循环次数CX的设置会不同 例题 子程序 编写时应注意 如何调用和返回 入口条件和出口条件 寄存器、保护、影响哪些标志位、出错如何处理 参数传递 利用寄存器 利用内存单元 利用堆栈 例题 功能调用 高级功能调用（DOS功能调用） 123MOV AH,功能号对各寄存器调用参数INT 21H 低级功能调用（BIOS功能调用) 123MOV AH,功能类型对各寄存器调用参数INT 中断类型 例题 例1 例2 例3 例4 例5 Examples123456789101112131415TITLE Summing an Array; This program sums an array of 16-bit integers..dataintarray DW 100H, 200H, 300H, 400H.codemain PROC MOV di, OFFSET intarray MOV cx, LENGTHOF intarray MOV ax, 0 ; clear the accumulatorL1: ADD ax, [di] ADD di, TYPE intarray LOOP L1main ENDPEND main 123456789101112131415TITLE Copying a String; This program copies a string..datasource DB &quot;This is the source string.&quot;, 0 ; 字符串末尾加上结束符便于输出，'\\0'的 ASICC 码值为 0target DB SIZEOF source DUP(0), 0.codemain PROC MOV si, 0 MOV cx, SIZEOF sourceL1: MOV al, source[si] MOV target[si], al LOOP L1main ENDPEND main 123456789101112131415161718192021222324252627TITLE Displaying Register Contents; This program displays the contents of the general-purpose registers..codemain PROC MOV bx, 1234H MOV ch, 4 ; 循环计数器ROT: MOV cl, 4 ; 移位计数器 ROL bx, cl ; 循环移位，将高4位移到低4位 MOV al, bl AND al, 0FH ; 取低4位 ADD al, 30H ; 将数字转换为ASCII码 CMP al, 39H JBE DISP ; 如果不大于 9 就显示 ADD al, 7 ; 如果大于 9 则转 A-FDISP: MOV dl, al MOV ah, 2 ; 显示 INT 21H DEC CH ; 计数4个十六进制数 JNZ ROT MOV dl, 48H ; 显示 H MOV ah, 2 INT 21Hmain ENDPEND main 123456789101112131415161718192021222324252627TITLE Scanning an Array for Non-Zero Values; This program scans an array for non-zero values..data intArray SWORD 0, 0, 0, 0, 1, 20, 35 ; single word nonMsg DB &quot;A non-zero value was not found.&quot;, 0.codemain PROC MOV ebx, OFFSET intArray MOV ecx, SIZEOF intArray ; Loop counterL1: CMP WORD PTR[ebx], 0 JNZ Found ; Found a value ADD ebx, 2 Loop L1 JMP NotFoundFound: MOVSX eax, WORD PTR[ebx] call Writeint ; Display the value in EAX JMP QuitNotFound: MOV edx, OFFSET nonMsg call WriteStringQuit: call crlf ; Carriage Return and linefeed exitmain ENDPEND main 123456789101112131415161718192021222324TITLE Scanning an Array; This program scans an array until it finds a positive value..data intArray SWORD -3, -6, -1, 0, 1, 20, 35 ; single word sentinel SWORD 0.codemain PROC MOV esi, OFFSET intArray MOV ecx, LENGTHOF intArray ; Loop counterNext: TEST WORD PTR[esi], 80H ; Testing the highest bit PUSHFD ; Pushing flags on stack ADD esi, TPYE intArray POPFD ; Poping flags from stack LOOPNZ Next ; if not found JNZ Quit ; finished scanning and not zero that is not found SUB esi, TYPE intArray ; moving to the former elementQuit: call crlf ; Carriage Return and linefeed exitmain ENDPEND main 1234567891011121314151617181920212223242526TITLE Compress BCD to ASCII; This program converts a BCD number to ASCII..codemain PROC MOV si, 1000H ; SI &lt;- BCD 首地址 MOV di, 2000H ; DI &lt;- ASCII 首地址 MOV BX, 4 ; 4 个 BCD 码L1: MOV al, [si] ; 取 BCD 码 AND al, 0FH ; 屏蔽高 4 位 OR al, 30H ; 转换为 ASCII STOSB ; 将AL寄存器中的值存储到DI地址指向的内存单元 LODSB ; SI指向的存储单元读入读入AL MOV cl, 4 SHR al, cl ; 逻辑右移 4 位 OR al, 30H ; 得到高 4 位 ASCII 码 STOSB INC si DEC bx JNZ L1 Quit: call crlf ; Carriage Return and linefeed exitmain ENDPEND main 第四章 总线技术第一讲 总线概述 定义: 连接两个以上数字系统元件的公共的信息通路 总线分类 按连接的层次 片内总线：连接CPU内部各功能部件的总线，例如内部的运算器、寄存器等元件级总线：连接CPU、内存以及总线控制逻辑的板内总线系统总线(内总线)：主机内用于连接主板、网卡、显卡等高速功能部件的总线通信总线(外总线)：连接主机与主机、主机与外设之间的总线 按数据传输的位数 I. 并行总线 多条数据线对数据各位进行同时传输 仅适宜计算机内部高速部件近距离传输 有时钟偏移和串扰 II. 串行总线 采用一条数据线逐位传输各位数据 常用于长距离通信及计算机网络，在短距离应用中性能也超过并行总线 无时钟偏移和串扰 内总线 PC机的内总线 ISA总线(Industry Standard Architecture) I. 特点 16位数据总线，支持 8 位、16 位数据操作 地址、数据非多路复用 多主控设备总线 II. 信号定义 数据总线(System Data Bus) 16位 $SD_0$ - $SD_{15}$, 8位 $SD_0$ - $SD_7$ 提速: 同步准备就绪信号 $\\overline{SRDY}$(Synchronous Ready)又零等待状态信号 $\\overline{NOWS}$(No Wait State)，表示无需额外等待即可完成一个总线周期 升位: 片选信号$\\overline{MEMCS16}$(Memory Chip Select 16), $\\overline{IOCS16}$(I/O Chip Select 16)，指示进行16位数据操作 位数可选: 系统高字节允许信号 $\\overline{SBHE}$(System Byte High Enable) 注: 当 $\\overline{SBHE}$ 被主控设备置为低电平，ISA插卡必须及时将 $\\overline{MEMCS16}$ 和 $\\overline{IOCS16}$ 置为有效作为回应(操作16位数据) 地址总线(System Address Bus) 主存地址空间: $SA_0\\sim SA_{19}$ 寻址 $1$MB，配合 $LA_{17}\\sim LA_{23}$ 寻址能力达到16M = $2^{24}$ I/O地址空间: $SA_0\\sim SA_{15}$ 寻址 $64$K，实际寻址 $1$K 未锁定地址信号 $LA_{17}\\sim LA_{23}$(Unlatched Address) 不锁存时: $LA_{17}\\sim LA_{19}$ (不锁存)与 $SA_{17}$ - $SA_{19}$ (锁存)重复 索存时: $LA_{17}\\sim LA_{23}$ 扩展地址总线 中断请求 11个：$IRQ_3$ - $IRQ_7$、$IRQ_9$ - $IRQ_{12}$、$IRQ_{14}$ - $IRQ_{15}$ $IRQ_0$ 定时器，$IRQ_1$ 键盘，$IRQ_2$ 级联，$IRQ_8$ 定时器8254，$IRQ_{13}$ 协处理器 DMA请求 7个 $DRQ_0$ - $DRQ_3$ 、 $DRQ_5$ - $DRQ_7$、 $\\overline{DACK_0}$ - $\\overline{DACK_3}$ 、 $\\overline{DACK_5}$ - $\\overline{DACK_7}$ 优先级：$DRQ_0$ - $DRQ_7$递减 $DRQ_0$ - $DRQ_3$ （8位传输）， $DRQ_5$ - $DRQ_7$（16位传输） 多主控制总线 $\\overline{MASTER}$ 系统控制权信号 速度 CLK PCI总线(Peripheral Component Interconnect Local Bus) I. 特点 不依赖处理器（PCI桥） 扩充性好（多PCI总线结构） 自动配置，即插即用 数据、地址奇偶校验功能 数据宽度32位，可扩展为64位 信号复用，支持无限读写突发操作 适应性广 并行总线操作II. 总线命令 总线命令 命令类型 0000 中断应答 0001 特殊周期 0010 I/O读 0011 I/O写 0100 MEM读 0100 MEM写 0100 读配置 0100 写配置 III. 突发成组数据传输：一个分组= 一个地址节拍 + 一个/多个数据节拍 工控机的内总线 STD总线 外总线 RS232C I. 特点 传输信号线少 传输距离较远 采用不归零编码NRZ和负逻辑 单端通信 传输速率较低 II. 电气特性、引脚功能 DB25和DB9，相同功能针号不同 信号 传送信息信号：TXD RXD（逻辑1电压为负） 联络信号：RTS CTSS DTR DSR DCD RI（逻辑1电压为正） 全双工和半双工 与TTL电平转换 应用 使用modem连接 软硬件系统调试 直接连接 交叉连接方式（全双工） 三线连接方式（软件无需检测CTS、DSR的状态） RS423、RS422 RS423：单端输出、差分接收 RS422：差分输出、差分接收 SCSI 使用逻辑地址而非物理地址寻址数据 特点： 适用范围广 传输速率高 提高了CPU效率，CPU占有率低 支持多任务 智能化 SCSI-1和SCSI-2 通用接口，设备无关 主机适配器 + 外设控制器 ≤ 8 两种工作方式 同步数据传输 异步数据传输（速率低于同步） SCSI总线上的设备无主从之分，任何设备可做启动设备也可做目标设备 驱动方式（三种方式不能共存） 单端 差分 LVD 总线信号 $DB_0$ - $DB_7$,DBP:命令、数据、状态、信息、SCSI-ID REQ 、 ACK：握手信号 C/D I/O SEL MGS ATN BSY RST 工作过程 启动设备选择一个目标设备，发送一条命令 目标设备被选中并接受命令（获得总线控制权），命令传到LUN（物理外设）去执行 目标设备释放总线 USB总线 特点 单一接口类型 127个外设 整个USB系统只用一个端口、一个中断，节省系统资源 热插拔 高速、全速、低速 设备供电 控制传输、同步传输、中断传输、批量传输 构成 硬件 USB主机 USB主集线器 根集线器 USB设备 集线器 功能部件 软件 USB设备驱动程序 USB驱动程序 USB主控制器驱动程序 第二讲 总线的驱动与控制总线竞争 同一总线上，同一时刻，有两个或以上的器件输出状态 防止总线竞争：用三态电路，严格控制逻辑 总线负载 直流负载 驱动器的高电平输出电流应不小于所有负载所需高电平输入电流之和 驱动器的低电平输出电流应不小于所有负载所需低电平输入电流之和 扇出数：驱动同门的个数，用 $I_{OH}$ / $I_{IH}$ 和 $I_{OL}$ / $I_{IL}$，取二者的较小值 交流负载 对MOS电路，主要考虑电容负载 扇出数：输出门的负载电容 $C_p$ / $C_{li}$ 总扇出数：$I_{OH}$ / $I_{IH}$ 、 $I_{OL}$ / $I_{IL}$ 、 $C_p$ / $C_{li}$，三者取最小值（理想情况） 总线驱动设计 克服总线负载效应：用驱动器和缓冲器 扇出能力大 延时可忽略 噪声容限较高 几种常用芯片（防止总线竞争） 单向驱动器（三态输出）（224） 双向驱动器（三态输出）（245） DIR = 0，读；DIR = 1，写 锁存器（三态输出）（373） 总线驱动设计 内存板：20位地址（$\\overline{MEMR}$ 、 $\\overline{MEMW}$），接口板：16位地址（$\\overline{IOR}$ 、 $\\overline{IOW}$） 防止总线竞争的原则：只有当CPU读本电路板内的内存地址/接口地址时，才允许双向驱动器指向系统总线的三态门是导通的 步骤 分析板内内存地址，找出地址特征 设置译码电路，用来控制双向数据总线驱动器，使之满足防止总线竞争的原则 译码方式 基本门电路 译码器（74LS138） 译码ROM 比较器（74LS688） PLA CPLD FPGA 第三讲 总线的工程设计问题 设计总线要考虑 不发生总线竞争 总线负载 总线交叉串扰 总线延时 总线信号的反射 总线交叉串扰 产生原因 总线间的寄生电容 总线本身可看做一个小电感 解决：减少总线间的寄生电容 减少总线长度 增加总线间距离 降低总线上的负载 两条信号线间加一条地线 减少总线的平行走向 总线优化器DS36662 采用双绞线 总线延时 解决方法 减少总线长度 选用延时小、输入输出电容小、驱动能力强的元器件 总线信号的反射 产生原因： 信号沿总线传播到达总线终端时，若总线终端负载阻抗与总线特性阻抗不匹配，信号的一部分会被反射 反射回来的信号到达信号源时，若源的内部阻抗与总线的特性阻抗不匹配，又会有一部分被反射回去 此过程有时需要多次才能在负载上建立所需的波形 危害 反射使波形变坏、延时增加 克服方法 降低传输信号的频率 尽量使 信号源内阻、总线的特性阻抗、负载阻抗 相匹配 总线匹配 末端匹配 源端匹配 限制总线长度 来自作者的忠告 从本章开始，默认读者已经掌握大部分微机原理与接口技术的基础内容，后续着重关注于较难的问题或者不易记忆的模块，此部分基于作者本身的直觉认为应当是计算机技术专业学生必须牢记的；由于作者仍在深入学习，知识储存量不能做到非常全面，所以如果您觉得部分内容有冗余，作者建议您可以选择直接略过。 第五章 存储技术第一讲 概述半导体存储器的基本概念 RAM: SRAM [异步 SRAM, 同步 SRAM], DRAM ROM 可一次编程 ROM: PROM(Programmable ROM); 可擦写的 PROM: [EPROM(Erasable Programmable ROM),E2PROM/EEPROM(Electrically Erasable Programmable ROM)[传统 E2PROM, FLASH]] 三级存储结构: 高速缓冲存储器、主存储器、辅助存储器 第二讲 常用存储器芯片及接口设计 来自作者的忠告 本讲重点关注于常用存储器芯片的特性以及接口设计，在主存接口设计中必须牢记常用存储器芯片特点 SRAM 及接口设计 异步 SRAM: 访存独立于时钟，控制信号不需要时钟同步 典型传统异步性SRAM芯片–6264芯片 数据线: $D0\\sim D7$ (8位存储器) 地址线: $A0\\sim A12\\Longrightarrow 2^{13}\\times 8=8K\\times 8$ (存储容量) 片选信号: $\\overline{CS1}, CS2$ 使能信号: $\\overline{OE}$ (输出使能), $\\overline{WE}$ (写使能) 时序分析 写入时序地址 $\\rightarrow$ 片选 $\\rightarrow$ 数据 $\\rightarrow$ 写信号 $\\rightarrow\\cdots\\rightarrow$ 撤写信号 $\\rightarrow$ 撤其他信号 读出时序地址 $\\rightarrow$ 片选 $\\rightarrow$ 读信号 $\\rightarrow$ 数据有效 $\\rightarrow$ 撤读信号 $\\rightarrow$ 撤其他信号 逻辑分析传送地址、加载片选信号以及加载读写信号顺序保持一定，主要是使得存储器读写信号的充分有效性 译码电路 译码方式 全地址译码: 全部的高位地址信号(除了存储单元地址以及片内地址)作为译码信号，编码存储器芯片的所有存储单元 部分地址译码/部分高位地址: 部分高位地址信号进行译码，存储器芯片占据几组不同的地址范围 译码电路的选择 利用译码芯片(74LS139(2-4译码器), 74LS138(3-8 译码器), 74LS154(4-16 译码器))、门电路 利用数字比较器芯片 74LS688 利用 PROM 译码器 芯片结构 同步 SRAM: 访存依赖于时钟，控制信号需要时钟同步 第三讲 Intel16/32/64 位微机系统的主存设计 8088 系统存储器: 8 位数据总线，单体存储器 8086/186/286 系统: 16 位数据总线，双体存储器 80386/80486 系统: 32 位数据总线，四体存储器 Intel 16位微机系统的主存设计 存储器的字、位扩展 芯片数量计算 存储容量 = (尾地址 - 首地址 + 1)$\\times$ 位宽 芯片数量 = 存储容量/芯片容量 Intel 32位微机系统的主存设计 $M/\\overline{IO}$ $D/\\overline{C}$ $W/\\overline{R}$ 总线周期 0 0 0 中断响应 0 0 1 停机 0 1 0 I/O读 0 1 1 I/O写 1 0 0 取指令操作码 1 0 1 保留 1 1 0 存储器读 1 1 1 存储器写 Intel 64位微机系统的主存设计 体选择信号: $\\overline{BE0}$ ~ $\\overline{BE7}$ 内存由 8 个体构成，每个体对应一个体选择信号 第四讲 只读存储器 (ROM) 及接口设计 外存平均访问时间 ms 级; 内存平均访问时间 ns 级 EPROM–2764芯片 地址总线: $A12\\sim A0$ (8K$\\times$ 8bit) 数据总线: $D7\\sim D0$ 片选信号: $\\overline{CE}$ 输出使能信号: $\\overline{OE}$ PGM: 编程时脉冲输入，读时为 “1” 芯片结构 EEPROM–98C64A芯片 地址总线: $A12\\sim A0$ (8K$\\times$ 8bit 并行) 数据总线: $D7\\sim D0$ 片选信号: $\\overline{CE}$ 输出使能信号: $\\overline{OE}$ 写入使能信号: $\\overline{WE}$ $Ready/\\overline{Busy}$: 漏极开路 芯片结构 EPROM 需要进行擦除 EEPROM 可单字节随机读写 (不需擦除，直接改写数据) 存储密度小，单位成本高 第六章 输入/输出技术第一讲 I/O概述主机和外设I/O 方式 程序控制方式 无条件传送方式 查询方式 不要求CPU效率 外设慢速 中断方式 要求CPU效率 外设中慢速 DMA 外设高速从无条件传送方式到DMA，控制越来越复杂，效率越来越高 I/O 接口 作用 信息传递（利用端口——接口里的寄存器） 注意区分“接口”“端口” 数据格式转换 CPU与外设速度匹配 负载匹配、时序匹配 总线隔离 提供中断、DMA功能 信息传递 编址方式 统一编制 独立编制（有IN OUT） I/O 端口地址译码 全地址译码 部分地址译码 基本的并行输入/输出接口 输入：三态门（防止总线冲突） 输出：锁存器（CPU和外设速度匹配） 第二讲 程序查询 I/O 方式 无条件传送方式（查询方式的特例） 外设时刻处于就绪状态 硬件：数据端口，软件：输入输出指令 查询方式 外设准备就绪之后才能与微机系统进行信息交换 硬件：数据端口、状态端口，软件：不断查询 ；硬件简单，软件开销大 时序 多外设的查询控制 一定要服务优先级高的——用于优先级差别很大的 服务完高优先级的再从头查询——用于优先级有一定差距的 机会均等——用于优先级差别很小的 第三讲 中断方式中断概述 中断源 内部：内中断（软件中断） 外部：外中断（硬件中断） NMI（不可屏蔽） INTR（可屏蔽） 中断过程 中断源发出中断请求 满足中断条件，进行中断响应 断点保护（硬件完成） PSW压栈，关中断，CS压栈，IP压栈 中断判优 硬件判优 软件判优 中断源识别 软件查询 中断矢量法 获得中断服务子程序首地址 固定入口法 中断向量法 中断处理——中断服务子程序 FAR类型，用IRET返回 保护现场 → 开中断STI → 中断处理 → 关中断CLI → 恢复现场 → 中断返回IRET RISC寄存器分页，中断服务程序无需PUSH POP 中断返回 IRET IRET指令使CPU把堆栈内保存的断电信息弹出到IP、CS、FLAG中 中断优先级嵌套 优先级原则 速度快＞速度慢 输入设备＞输出设备 解决办法 软件查询 硬件链式优先级排队电路 硬件优先级编码比较电路 利用可编中断控制器PIC 实现中需注意 中断处理程序：STI开中断指令 堆栈足够大 正确使用堆栈（关中断之后恢复现场） Intel 16位中断系统8086/8088中断系统 中断源类型 中断向量表IVT 存放中断服务程序的入口地址 00000H ~ 003FFH，1KB = 4B/入口 × 256个入口 中断向量在IVT中的存放地址 = 4 × 中断类型号n 4n : IP 4n+2 : CS 中断的响应过程 注意，外部中断INTA有两次 优先级从高到低：内部中断、NMI 、INTR、单步中断 可编程中断控制器（PIC）8259 内部结构 IRR 请求（=1表示有请求） ISR 服务（=1表示正被服务） IMR 屏蔽（=1表示被屏蔽） 中断优先权判别电路 引脚 工作方式 级联 单片8259A可支持8个中断源，多片级联最多支持64个中断源，n片可支持7n+1个中断源（n≤9） $\\overline{\\text{SP}}$ / $\\overline{\\text{EN}}$ :低-从片，高-主片 从片中断结束，中断服务程序需发送两个EOI命令 编程使用 内部寄存器的寻址方法 命令字 中断方式实现方法 8259连接（硬件） 编写初始化程序 8259初始化（初始化命令字） 设置中断向量表 直接写IVT 利用都是功能调用：功能号25H是写IVT 编写中断处理程序第四讲 直接存取方式DMA工作过程 特点 高速外设，纯硬件控制，外设直接与存储器进行数据交换，无需CPU，传输速率高 内存/外设的地址和读写控制均由DMACA提供 时序 AEN=0 : CPU AEN=1 : DMA 工作过程 外设准备好，向DMA发出DRQ DMA收到请求，向CPU发出HOLD CPU完成当前总线周期且非总线封锁，响应HOLD信号 将数据总线、地址总线、控制信号线置高阻态，放弃总线控制权 向DMA发出HLDA DMA收到HLDA，开始控制总线，向外设发出DACK 外设与内存 或 内存与内存 直接数据传送 DMA自动修改地址和字节计数器，传完后，撤HOLD CPU撤HLDA，下一时钟周期控制总线DMA控制器8237（DMAC） 特点 4个独立DMA通道，级联扩展为最多16个 引脚及功能 工作时序 Si：空闲状态 S0请求状态 S1 ~ S4传送状态 S1只在A15 ~ A18更新时才执行 可在S3 S4之间插入Sw 正常时序S2、S3、S4 压缩时序S2、S48237工作方式 工作方式 空闲周期 工作周期 传输方式 单字节传送（传完一个字节后，DREQ无效，总线控制权还给CPU） 数据块传送 （传完块数据后才释放总线，期间无论DREQ是否有效都传送） 请求传送（猝发传送）（只要DREQ有效或I/O接口的数据缓冲可用，DMA一直传送数据） 连接方式 级联方式 在级联方式下，当第二层8237的请求得到响应时，第一层8237仅向微处理器发出HRQ信号、对第二层的HRQ作出响应DACK而不能输出地址及控制信号，第二层的8237才是真正的主控制器 传送类型：存储器 → 接口，接口 → 存储器，存储器 → 存储器 优先级 固定优先级 循环优先级 传输速率 正常时序（1个DMA总线周期需4个时钟周期） 压缩时序（1个DMA总线周期需2个时钟周期） 内部寄存器 第七章 常用接口器件第一讲 8255: 8位通用可编程并行接口计算机与外设之间通过接口传送数据 无条件输入(三态门: 74LS244) 无条件输出(锁存器: 74LS273) 中断方式，单向输入/输出 中断方式，双向传输(I/O) 端口地址信号 $A_1$ $A_0$ 选择 0 0 A口($PA_0\\sim PA_7$) 0 1 B口($PB_0\\sim PB_7$) 1 0 C口($PC_0\\sim PC_7$) 1 1 控制寄存器 A口、B口的输入和输出具有锁存能力，C口的输出有锁存能力，输入没有锁存能力 A组 B组 $PA_0\\sim PA_7, PC_4\\sim PC_7$ $PB_0\\sim PB_7, PC_0\\sim PC_3$ 控制字 控制字的部分独立性 A口($PA_0\\sim PA_7$)、B口($PB_0\\sim PB_7$)、C口低四位($PC_0\\sim PC_3$)、C口高四位($PC_4\\sim PC_7$)可单独定义 状态字当8255的A口、B口工作在方式1或A口工作在方式2时，通过读C口的状态，可以检测A口和B口的状态 工作方式 工作方式 0(基本输入/输出方式)A口($PA_0\\sim PA_7$)、B口($PB_0\\sim PB_7$)、C口低四位($PC_0\\sim PC_3$)、C口高四位($PC_4\\sim PC_7$)独立定义，均可做为输入或输出接口 工作方式 1(选通输入/输出方式) A、B口均输出 固定 C 口线A 口使用 $PC_3(INTR_A),PC_6(\\overline{ACK_A}),PC_7(\\overline{OBF_A})$, B 口使用 $PC_0(INTR_B),PC_1(\\overline{OBF_B}),PC_2(\\overline{ACK_B})$ $\\overline{OBF}$(Ouput Buffer): (8255端口$\\rightarrow$ 外设) 输出缓冲器满信号，通知外设在规定端口上取数据: $CPU\\overset{data}{\\rightarrow}Buffer\\overset{\\overline{OBF}}{\\rightarrow}Device\\overset{Fetch}{\\rightarrow}Data$ $\\overline{ACK}$: (外设$\\rightarrow$ 8255端口) 外设响应信号：$Device\\overset{Fetch}{\\rightarrow}Data\\rightarrow \\overline{OBF}=1$ $INTR$: (8255端口$\\rightarrow$ CPU) 中断请求信号，$CPU\\overset{data}{\\rightarrow}Buffer\\rightarrow INTR\\rightarrow Newdata$ $INTE$: 中断允许状态，A口由$PC_6$控制，B口由$PC_2$控制 A、B口均输入 固定 C 口线A 口使用 $PC_3(INTR_A),PC_5(IBF_A),PC_4(\\overline{STB_A})$, B 口使用 $PC_0(INTR_B),PC_1(IBF_B),PC_2(\\overline{STB_B})$ $\\overline{STB}$: (外设$\\rightarrow$ 8255端口) 输入选通信号，将外设数据锁存于输入锁存器中 $\\overline{IBF}$: (8255端口$\\rightarrow$ 外设) 输入缓冲器满信号 $INTR$: (8255端口$\\rightarrow$ CPU) 中断请求信号 $INTE$: 中断允许状态，A口由$PC_4$控制，B口由$PC_2$控制 工作方式 2(双向输入/输出方式，仅A口) A口控制线: $PC_0\\sim PC_7$ 芯片结构连接图 初始化程序 8255端口地址：380H$\\sim$383H 方式0——手动设置外设选通信号1234567891011121314151617181920212223242526272829303132333435INIT_8255: MOV DX, 0383H ; 控制寄存器 MOV AL, 10000011B ; 方式选择 OUT DX, AL MOV AL, 00001101B ; C口使得STROBE=1, 初始化控制信号 OUT DX, ALPRINT: MOV AL, BLAK MOV Cl, AL ; 字符串长度 MOV SI, OFFSET DATAGOOD: MOV DX, 0382H ; C口 PWAIT: IN AL, DX ; Test busy signal AND AL, 02H JNZ PWAIT ; Wait until ready MOV AL, [SI] ; load char data MOV DX, 0380H ; A口 OUT DX， AL ; CPU -&gt; A口 MOV DX, 0382H ; C口 MOV AL, 00H OUT DX, AL ; PC6=0 -&gt; STROBE=0 CALL Delay_1us ; A 口 -&gt; 外设 MOV AL, 40H OUT DX, AL ; PC6=1 -&gt; STROBE=1 Restore high level INC SI DEC CL JNZ GOOD RET 方式1——自动握手12345678910111213141516171819202122232425262728INIT_8255: MOV DX, 0383H ; 控制寄存器 MOV AL, 10100000B ; 方式选择 OUT DX, AL MOV AL, 00001101B ; C口 PC6=1 初始无握手信号 OUT DX, ALPOLLPRINT: MOV AL, BLAK MOV Cl, AL ; 字符串长度 MOV SI, OFFSET DATAGOOD: MOV DX, 0382H ; C口PWAIT: IN AL, DX ; Test busy signal AND AL, 80H ; Test PC7 = 1 首先测试 PC7 表示 CPU 已经响应中断 JZ PWAIT ; Wait until ready MOV AL, [SI] ; load char data MOV DX, 0380H ; A口 OUT DX， AL ; CPU -&gt; A口 ; Make a short summary ; A 口自动传送给外设: CPU响应中断 -&gt; data -&gt; A口 -&gt; 有效的OBF -&gt; 选通外设接受数据 ; 外设接受数据 -&gt; 有效 ACK 握手信号 -&gt; OBF 无效 INC SI DEC CL JNZ GOOD RET 第二讲 8253: 可编程定时器端口地址信号 $A_1$ $A_0$ 选择 0 0 计数器0 0 1 计数器1 1 0 计数器2 1 1 控制寄存器 芯片结构 计数方式 8253的计数模式 计数初值寄存器与减一计数器的关系: 计数初值装入初值寄存器，减一计数器每次都加载初值寄存器中的初值，启动计数。这种关系适用于所有计数模式，主要便于循环计数，启动新一轮计数.减一计数器与计数锁存器的关系：CPU发锁存命令，使得减一计数器中的当前计数值会锁存到计数锁存器中, 可用于在计数过程中读取当前计数值. 减1计数器 二进制计数：0000H$\\sim$ FFFFH(65535) 初值0000H为最大计数值，FFFFH为计数最小值 减一运算采用补码加法： FFFFH-1H = FFFFH + 0001H=0000H 0000H-1H = 0000H + 0001H=0001H 0001H-1H = 0001H + 0001H=0010H 方式0：计数结束产生中断 Condition: GATE 高电平，允许计数 OUT: 输出低电平 First CLK: 计数初值 N -&gt; 初值寄存器 N CLKs: 减一计数 N+1 CLKs: 事件计数 方式1：可编程单稳 Condition: GATE 上升沿触发计数 OUT: 输出低电平 N CLKs: 利用 GATE 编程计数 方式2：频率发生器 Condition: GATE 高电平，允许计数 OUT: 周期输出负脉冲 N CLKs: 减到1时送出负脉冲 方式3：方波发生器 Condition: GATE 高电平，对称方波 OUT: 前 N/2 | (N+1)/2 CLKs 高电平，后 N/2 | (N-1)/2 CLKs 低电平 方式4：软件触发选通 Condition: GATE 高电平，允许计数 OUT: 计数结束输出负脉冲 N CLKs: 技术结束下一个 CLK, 送出负脉冲 方式5：硬件触发选通 Condition: GATE 上升沿触发计数 OUT: 计数结束输出负脉冲 N CLKs: 技术结束下一个 CLK, 送出负脉冲 Summary 所有 GATE 上升沿触发计数均在下一个 CLK 启动计数 0为计数最大值，1为计数最小值，计数为减一计数器 从初值寄存器装入新的计数值，除了可编程单稳，频率发生器都需要重新开始计数 所有 GATE 上升沿触发计数(可编程单稳，频率发生器)在新的上升沿到来时，均会重新开始计数 控制字 芯片结构连接图 初始化程序及其应用 利用计数器模式设计电路，输出指定频率的信号 12345678910MOV AL, 36H ; 控制字，计数器0，双字节，方式3，二进制计数OUT 43H, AL ; 送入控制寄存器MOV AL, 0OUT 40H, AL ; 先写低字节OUT 40H, AL ; 后高字节MOV AL, 54H ; 计数器1，低字节，方式2，二进制计数OUT 43H, ALMOV AL, 18OUT 41H, AL 计数模式1：GATE周期信号频率: $F_1$Hz, CLK输入频率: $F_2$Hz，GATE上升沿刷新计数的最大计数值为:$$N = \\frac{F_2}{F_1}$$ 题型分析 仅适用于重点范围的题型分析，是个人觉得出题意义或概率相对较大的一类题目或知识模块; 设定的 Todo List 仅是基于我个人的情况，不提供参考； 第二章 8086/8088 CPU 8086/8088 最小/最大模式下系统总线形成 第三章 8086汇编语言程序设计 常用汇编指令 基本汇编程序编写 基本表达式计算 数据段中数组数据进行排序 第四章 总线与驱动控制 常用数据总线特点(ISA, PCI, USB) 总线驱动控制电路设计与分析 单向驱动器: 74LS224 双向驱动器: 74LS245 数据锁存器: 74LS373 总线驱动与控制参数计算 第五章 存储器设计 SRAM, DRAM, EPROM, EEPROM 特点 数字比较器、PROM 作为译码电路的实现机制 基于 SRAM 芯片的8086/88系统主存电路设计与分析 位扩展+字扩展设计方法(8086系统16位存储设计) 8086系统8位读写以及16位读写 总线驱动设计: 单向驱动与双向驱动 基于 EEPROM 芯片的 IO 接口设计(Busy的处理) 存储器设计中总线驱动器件使能端设计 第六章 输入/输出技术 中断处理响应过程 8259工作方式(状态字) 级联: 特殊全嵌套与一般嵌套 中断结束: 自动EOI, 特殊/指定EOI, 一般/非指定EOI 优先级: 固定优先级, 自动循环优先级, 指定循环优先级 必要细节处理… Extend Points… Contributors Zhihao Li Xiaoyue Li References 微型计算机原理及接口技术(第三版)","link":"/collaboration/Microcomputer/"},{"title":"Install Anaconda On the Linux Server","text":"Download the Anaconda PackageFirstly, we need to get the anaconda3 package and there is some mirrors website providing the faster speed of downloading. There, we chosen the tsinghua mirror and the version of 2023.09 with x86 architecture. 1wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2023.09-0-MacOSX-x86_64.sh After running, we can get this package at the home directory. Install the PackageIn the linux server, we can install the anaconda package by the following command. 1bash Anaconda3-2023.09-0-Linux-x86_64.sh Running the above command we can see this welcoming page, Next, we need to accept the license agreement to continue following steps. Then, we need to choose the installation path, and we can choose the default path by inputting ENTER. Automatically initializing conda will be convenient for us. So in this step, it is recommended to input the yes for automatically activating base environment. After implementing the above steps, we can successfully install the anaconda on a linux server. There is two ways to check whether the anaconda is installed successfully. The first one is to check the version of anaconda. The second one is to check the environment variable of anaconda. Create the New EnvironmentIn this section, a new conda virtual environment will be create: 1conda create -n LabelNoise python=3.11.5 We can also specify the installation path of the new environment. For example, we can create a new environment in the path of /home/zhli/anaconda3/envs/Pytorch by the following command: 1conda create --prefix=/home/zhli/anaconda3/envs/Pytorch python=3.11.5 Activate the New EnvironmentIf we want use the new environment, we need to activate it first. 1conda activate LabelNoise Remove the New EnvironmentIf we donn’t need the new environment any more, we can remove it by the following command: 1conda remove --n Pytorch --all","link":"/blog/AnacondaOnLinuxServer/"},{"title":"Chapter 5 to 8","text":"Chapter 5 Logistic RegressionFormal definitions: for some data samples ${&lt;\\boldsymbol{x}^{(n)}, y^{(n)}&gt;}_{n=1}^N$, we can get a predictive model by statistical analysis. The model we have trained will output a preferable prediction value: $$y=f(\\boldsymbol{x})$$ for a given test data $\\boldsymbol{x}={x_1, x_2, \\cdots, x_n}$. Linear RegressionDefine: $f(\\cdot)$ is regression model in linear combinations $$y = f(\\boldsymbol{x})=\\boldsymbol{w}_T\\boldsymbol{x}+b\\=w_1x_1+\\cdots+w_nx_n+b$$ Optimization goal： $$\\min\\limits_{w,\\ b} \\ |f(\\boldsymbol{x}^{(n)})-y^{(n)}|$$ Loss Function( Mean squared error)： $$E = \\sum\\limits_n[y^{(n)}-(\\boldsymbol{w}^T\\boldsymbol{x}^{(n)}+b)]^2$$ Assumption of the selection error function: The data sample points satisfy the normal distribution with the predicted values of the selected linear regression model as the mean, which is converted to a parametric estimation problem.Define the conditional probability, which represents the probability value of the model prediction output as $y$ given the independent variables: $$p(y|\\boldsymbol{x})\\thicksim N(\\boldsymbol{w}^T\\boldsymbol{x}+b, \\sigma^2)$$ Derive the likelihood function of the conditional probability: $$p(y|\\boldsymbol{x}) :\\ \\ \\boldsymbol{L}(\\boldsymbol{w}, b)=\\prod\\limits_n\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-\\frac{1}{2\\sigma^2}(y^{(n)}-\\boldsymbol{w}^T\\boldsymbol{x}^{(n)}-b)^2)$$ $$\\Rightarrow \\ln\\boldsymbol{L}(\\boldsymbol{w}, b)=n\\ln(\\frac{1}{\\sqrt{2\\pi}\\sigma})+\\ln(\\prod\\limits_n\\exp(-\\frac{1}{2\\sigma^2}(y^{(n)}-\\boldsymbol{w}^T\\boldsymbol{x}^{(n)}-b)^2)$$ $$=n\\ln(\\frac{1}{\\sqrt{2\\pi}\\sigma})-\\frac{1}{2\\sigma^2}\\sum\\limits_n(y^{(n)}-\\boldsymbol{w}^T\\boldsymbol{x}^{(n)}-b)^2$$ $$\\frac{\\partial{\\ln\\boldsymbol{L}(\\boldsymbol{w}, b)}}{\\partial{\\boldsymbol{w}}}=0, \\ \\ \\frac{\\partial{\\ln\\boldsymbol{L}(\\boldsymbol{w}, b)}}{\\partial{b}}=0$$ $$\\Rightarrow \\boldsymbol{w}, b=\\underset{\\boldsymbol{w},\\ b}{\\arg \\max}\\ \\ \\boldsymbol{L}(\\boldsymbol{w}, b)$$ $$=\\underset{\\boldsymbol{w},\\ b}{\\arg \\min}\\ \\ \\sum\\limits_n(y^{(n)}-\\boldsymbol{w}^T\\boldsymbol{x}^{(n)}-b)^2$$ If matrix operations are used, the error function is represented in vector form: $$\\boldsymbol{Y}=\\left[y^{(1)},y^{(2)},\\cdots,y^{(n)}\\right]’,\\ \\boldsymbol{w}=(w_1, w_2, \\cdots, w_n)$$ $$\\boldsymbol{X}=\\left[\\boldsymbol{x}_1, \\boldsymbol{x}_2,\\cdots, \\boldsymbol{x}_m\\right],\\ \\boldsymbol{x_i}=(x_i^{(1)}, x_i^{(2)}, \\cdots, x_i^{(n)})’$$ $$\\boldsymbol{b}=\\left[b_1, b_2, \\cdots, b_n\\right]’, \\ \\ b_1=b_2=\\cdots=b_n$$ $$\\Longrightarrow \\boldsymbol{E}=(\\boldsymbol{Y}-\\boldsymbol{X}\\boldsymbol{w}^T-\\boldsymbol{b})^T(\\boldsymbol{Y}-\\boldsymbol{X}\\boldsymbol{w}^T-\\boldsymbol{b})$$ if we combine data samples weights $w$ with parameters $b$, that is: $$\\boldsymbol{w}=(w_1, w_2, \\cdots, w_n, b)$$ $$\\boldsymbol{X}=\\left[\\boldsymbol{x}_1, \\boldsymbol{x}_2,\\cdots, \\boldsymbol{x}_m,\\boldsymbol{1}\\right],\\ \\boldsymbol{x_i}=(x_i^{(1)}, x_i^{(2)}, \\cdots, x_i^{(n)})’,\\ \\boldsymbol{1} = [1,1,\\cdots,1]’$$ $$\\Longrightarrow \\boldsymbol{E}=(\\boldsymbol{Y}-\\boldsymbol{X}\\boldsymbol{w}^T)^T(\\boldsymbol{Y}-\\boldsymbol{X}\\boldsymbol{w}^T)$$ $$\\Longrightarrow\\frac{\\partial{E}}{\\partial{\\boldsymbol{w}}}=2\\boldsymbol{X}^T(\\boldsymbol{X}\\boldsymbol{w}^T-\\boldsymbol{Y})=0$$ $$\\Longrightarrow\\boldsymbol{w}^T=(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{Y}$$ $$\\Longrightarrow y=\\boldsymbol{x}\\boldsymbol{w}^T=\\boldsymbol{x}(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{Y}$$ Logistic RegressionIf there exists a nonlinear relationship between random variables ${x_i}(i=1,2,\\cdots,n)$ and $y$, a generalized linear regression model can be defined by a nonlinear transformation $g(\\cdot)$: $$y\\mathop{\\longrightarrow}\\limits^{g(\\cdot)} g(y);\\ \\boldsymbol{x}\\mathop{\\longrightarrow}\\limits^{f(\\cdot)} g(y)$$ $$f(\\boldsymbol{x})=g(y)\\Rightarrow y=g^{-1}(f(\\boldsymbol{x}))$$ 【Example of binary classification】In binary classification tasks, the goal is to fit a separating hyperplane. $$y=g^{-1}(f(\\boldsymbol{x}))=0,\\ f(\\boldsymbol{x})&lt;0\\ \\ |\\ \\1,\\ f(\\boldsymbol{x})&gt;0.$$ To ensure differentiability of $g(\\cdot)$, a function $\\sigma(\\cdot)$ is chosen to approximate the discontinuous step function, such as the logistic function: $$y=\\sigma(f(\\boldsymbol{x}))=\\frac{1}{1+\\exp(-\\boldsymbol{w}^T\\boldsymbol{x}-b)}$$ $$p(y=1|\\boldsymbol{x})=\\sigma(f(\\boldsymbol{x}))$$ $$p(y=0|\\boldsymbol{x})=1-\\sigma(f(\\boldsymbol{x}))$$ $$g^{-1}(x)=\\sigma(x)\\Rightarrow \\sigma(g(y))=y=\\frac{1}{1+e^{-f(\\boldsymbol{x})}}=\\frac{1}{1+e^{-g(y)}}$$ $$\\Longrightarrow g(y)=\\log\\frac{y}{1-y}=\\log\\frac{p(y=1|\\boldsymbol{x})}{p(y=0|\\boldsymbol{x})}=f(\\boldsymbol{x})=\\boldsymbol{w}^T\\boldsymbol{x}+b$$ Parameter estimation: Maximum likelihood estimation is used to estimate the conditional probability $p(y|\\boldsymbol{x};\\boldsymbol{w},b)$.Likelihood Function： $$\\boldsymbol{L}(\\boldsymbol{w}, b)=\\prod\\limits_n[\\sigma(f(\\boldsymbol{x}^{(n)}))]^{y^{(n)}}[1-\\sigma(f(\\boldsymbol{x}^{(n)}))]^{1-y^{(n)}}$$ $$\\Longrightarrow \\ln\\boldsymbol{L}(\\boldsymbol{w}, b)=\\ln(\\prod\\limits_n[\\sigma(f(\\boldsymbol{x}^{(n)}))]^{y^{(n)}}[1-\\sigma(f(\\boldsymbol{x}^{(n)}))]^{1-y^{(n)}})$$ $$=\\sum\\limits_n[y^{(n)}\\log(\\sigma(f(\\boldsymbol{x}^{(n)})))+(1-y^{(n)})\\log(1-\\sigma(f(\\boldsymbol{x}^{(n)})))]$$ Pytorch Logistic RegressionThis is a demo for Logistic Regression Learning. Firstly, we need to prepare corresponding environment by importing some libraries. 12345import numpy as npfrom torch.distributions import MultivariateNormalimport torchimport torch.nn as nnimport matplotlib.pyplot as plt 【Prepare Data】We set Normal Distribution’s mean vector and covariance matrix. 1234567891011121314151617181920212223242526mu1 = torch.ones(2) * -3 # two variablesmu2 = torch.ones(2) * 3sigma1 = torch.eye(2) * 0.5sigma2 = torch.eye(2) * 2# set Normal Distributions Objectm1 = MultivariateNormal(mu1, sigma1)m2 = MultivariateNormal(mu2, sigma2)# samplingx1 = m1.sample((100,))x2 = m2.sample((100,))# set label valuey = torch.zeros((200, 1))y[100:] = 1# combine and scramble samplesx = torch.cat([x1, x2], dim=0)idx = np.random.permutation(len(x))x = x[idx]y = y[idx]# visualizationplt.scatter(x1.numpy()[:,0], x1.numpy()[:,1])plt.scatter(x2.numpy()[:,0], x2.numpy()[:,1])plt.show() 【Prepare Pytorch Linear Model】In the class torch.nn, Linear object achieves $y=x\\boldsymbol{x}^T+b$. 123456# set features numberD_in, D_out = 2, 1linear = nn.Linear(D_in, D_out, bias=True)output = linear(x)print(x.shape, linear.weight.shape, linear.bias.shape, output.shape) 【Pytorch Activation Function】Logistic Regression used for binary classification problem will ultilize torch.nn.Sigmoid() fanction to map the result which linear model have calculated to $0\\thicksim 1$. 12sigmoid = nn.Sigmoid()scores = sigmoid(output) 【Loss Function】Logistic Regression uses cross-entropy as its loss function. Torch.nn provides many standard loss function and we can directly use torch.nn.BCELoss to calculate binary cross-entropy loss. 12loss = nn.BCELoss()print(loss(sigmoid(output), y)) 【Reconstruct our model】In Pytorch, we can inherit nn.Module to build our own model, but what we need to notice is that forward() method must be overwritten by subclasses. 12345678910111213class LogisticRegression(nn.Module): def __init__(self, D_in): super(LogisticRegression, self).__init__() self.linear = nn.Linear(D_in, 1) self.sigmoid = nn.Sigmoid() def format(self, x): x = self.linear(x) output = self.sigmoid(x) return outputlr_model = LogisticRegression(2)loss = nn.BCELoss()loss(lr_model(x), y) 【Optimization Algorithm】Logistic regression typically optimizes the objective function using gradient descent. Pytorch’s torch.optim package implements most commonly used optimization algorithms. 123from torch import optimoptimizer = optim.SGD(lr_model.parameters(), lr=0.03) After constructing optimizer, we can train the model iteratively. There are two main steps here, one is calling the backward() method of the loss function to calculate the model gradient, the other is calling the step() method of the optimizer to update the model parameters. It should be noted that we need to call the zero_grad() method of the optimizer to clear out parameters’ gradient firstly. 1234567891011batch_size = 10iters = 10for _ in range(iters): for i in range(int(lex(x)/batch_size)): input = x[i*batch_size:(i+1)*batch_size] target = y[i*batch_size:(i+1)*batch_size] optimizer.zero_grad() output = lr_model(input) l = loss(output, target) l.backward() optimizer.step() Chapter 6 Neural Network BasicsI. Basic characteristics of neural networks Connectionist model based on statistics Have the basic unit for processing signals Processing units are connected in parallel with each other There are the connection weights between processing units 【Neuron】$$out=f(w_1x_1+w_2x_2+\\cdots+w_nx_n+b)$$ A neuron receives a set of tensors as input: $\\boldsymbol{x}={x_1,x_2,\\cdots,x_n}^T$, connection weights $\\boldsymbol{w}={w_1,w_2,\\cdots,w_n}$, then performs a weighted summation： $$sum=\\sum_iw_ix_i=\\boldsymbol{w}\\boldsymbol{x}$$ Sometimes, the weighted summation of neurons is also accompanied by a constant term $b$ as a bias: $$sum=\\boldsymbol{w}\\boldsymbol{x}+b$$ Activation Function $f(\\cdot)$ is applied to the input weighted $sum$ to produce the output of the neuron; if the order of $sum$ is greater than 1, then $f(\\cdot)$ is applied to each element of $sum$ . 【Activation Function】 $softmax()\\ \\ function$It is suitable for multivariate classification problems, and the function is to normalize n scalars representing n classes respectively to obtain the probability distribution of these n classes. $$softmax(x_i)=\\frac{\\exp(x_i)}{\\sum_j\\exp(x_j)}$$ $sigmoid()\\ \\ function$It is also usually expressed as $logitic()\\ \\ function$, suitable for binary classification problems, and is a binary version of $softmax$. $$\\sigma(x) = \\frac{1}{1+\\exp(-x)}$$ that is, for two classes: $x_1, x_2$, there is the equation: $$softmax(x_1)=\\frac{\\exp(x_1)}{\\exp(x_1)+\\exp(x_2)}=\\frac{1}{1+\\exp(x_2-x_1)}$$ $$P(x_1=1)=sigmoid(x_1)=\\frac{1}{1+\\exp(-x_1)}(x_2=0)$$ $Tanh()\\ \\ function$It is a variant of $logistic()\\ \\ function$. $$tanh(x) = \\frac{2\\sigma(x)-1}{2\\sigma^2(x)-2\\sigma(x)+1}$$ $ReLU()\\ \\ function(Rectified\\ \\ linear\\ \\ unit)$This function only has the half of the range which is active and it can effectively avoid the problem of vanishing gradient. $$ReLU(x) = \\max(0,x)$$ 【Output Layer】The output of the activation function is just the output of the neuron. A neuron can have multiple outputs $o_1, o_2,\\cdots,o_m$, but corresponding to different activation functions $f_1,f_2,\\cdots,f_m$. 【Neural Network】A neural network is a directed graph and a computational graph. II. Perceptron【Monolayer Perceptron】Consider there is a neuron, which has two inputs $x_1,x_2$ with weights $w_1,w_2$. And we use symbolic function as activation function: $$f(x) = sgn(x)=-1,\\ x&lt;0\\ |\\ 1,\\ x\\geq 0$$ In training process, weights will be updated according to the following method: $$w’\\leftarrow + \\alpha \\cdot (y-o)\\cdot x$$ About the above parameters, $o$ means activation value, $y$ is expected objective value, $\\alpha$ is learning rate. 【Multi-Layer Perceptron, MLP】We can take an example to specifically understand how different the MLP is. Supposing that we choose XOR function as model’s activation function: $$f(x_1,x_2)=0,\\ x_1=x_2\\ \\ |\\ \\ 1,\\ x_1\\neq x_2$$ And in this case, as the equation described, we just need two inputs to get our activate value. But how can we achieve XOR operation by using some neurons? As mentioned above, Monolayer Perception can fit a hyperplane $y=ax_1+bx_2$. And this is suitable for linearly separable problems. That’s because our mapping relationship is linear. So if we use one more layer, there are maybe good changes in mapping relationship between inputs and outputs.Now we set two hidden layer neurons $h_1, h_2$, and then we can get new equations: $$h_1=w_{11}x_1+w_{21}x_2,\\ \\ h_2=w_{12}x_1+w_{22}x_2$$ $$y=0,\\ h_1=h_2\\ \\ |\\ \\ 1,\\ h_1\\neq h_2$$ III. BP Neural NetworkDefine: Back Propagation(BP) algorithm propagates the error backward from the output layer to the front layer, and uses the error of the latter layer to estimate the error of the previous layer. 【Gradient Descent】In order to ensure the error back propagation, we utilize gradient descent algorithm to search in the weight space int the direction of the fastest error decline. $$w\\leftarrow w+\\Delta w$$ $$\\Delta w=-\\alpha\\triangledown Loss(w)=-\\alpha\\frac{\\partial{Loss}}{\\partial{w}}$$ Commonly used loss functions Mean Squared Error, MSE $$Loss(o, y)=\\frac{1}{n}\\sum\\limits_{i=1}^n|o_i-y_i|^2$$ Cross Entropy, CE $$Loss(x_i)=-\\log(\\frac{\\exp(x_i)}{\\sum_j\\exp(x_j)})$$ 【Back Propagation】The key to backpropagating the error is to use the chain rule for partial derivatives. Take the following neural network as example to demonstrate. $$o=f_3(w_6\\cdot f_2(w_5\\cdot f_1(w_1\\cdot i_1+w_2\\cdot i_2)+w_3\\cdot i_3)+w_4\\cdot i_4)$$ In gradient descent, in order to calculate $\\Delta w_k$, we need to utilize the chain rule to get $\\frac{\\partial{Loss}}{\\partial{w_k}}$. For example, to calculate $\\frac{\\partial{Loss}}{\\partial{w_1}}$: $$\\frac{\\partial{Loss}}{\\partial{w_1}}=\\frac{\\partial{Loss}}{\\partial{f_3}}\\frac{\\partial{f_3}}{\\partial{f_2}}\\frac{\\partial{f_2}}{\\partial{f_1}}\\frac{\\partial{f_1}}{\\partial{w_1}}$$ 【Dropout Regularization】As a kind of regularization method, dropout will reduce overfitting in neural network by avoiding features co-adaptations. 1234567891011121314151617181920212223242526import torch.nn as nnimport torch# set coding environmentsp, count, iters, shape = 0.5, 0., 50, (5, 5)dropout = nn.Dropout(p=p)dropout.train()for _ in range(iters): activations = torch.rand(shape) + 1e-5 output = dropout(activations) count += torch.sum(output == activations *( 1/(1-p)))print(&quot;In training mode Dropout influenced {} neurons&quot;.format(1-float(count)/(activations.nelement() * iters)))count = 0dropout.eval()for _ in range(iters): activations = torch.rand(shape) + 1e-5 output = dropout(activations) count += torch.sum(output == activations)print(&quot;In evaling mode Dropout influenced {} neurons&quot;.format(1-float(count)/(activations.nelement() * iters)))&gt;&gt;&gt; In training mode Dropout influenced 0.4952 neurons&gt;&gt;&gt; In evaling mode Dropout influenced 0.0 neurons 【Batch Normalization】 Internal Covariate ShiftWhen training neural network, we always need to normalize input data to speed up the training process. However, learning algorithms for example SGD will continuously change the parameters in network and thus the distribution of activations in hidden layer will also change. Batch Normalization Functions Accelerate training High learning rate ability Regularization Batch Normalization AchievementFor a specific activation $x^{(k)}$, we can use it to demonstrate how Batch Normalization works.Now define: current batch has $m$ activations that is $\\beta$: $$\\beta = (x_1,x_2,\\cdots,x_m)$$ Firstly, we need to calculate mean and variance of $\\beta$ $$\\mu_{\\beta}=\\frac{1}{m}\\sum\\limits_{i=1}^mx_i,\\ \\delta^2_{\\beta} = \\frac{1}{m}\\sum\\limits_{i=1}^m(x_i-\\mu_{\\beta})^2$$ Secondly, it’s necessary to normalize $\\beta$ using calculated mean $\\mu_{\\beta}$ and variance $\\delta^2_{\\beta}$: $$\\widehat{x_i}=\\frac{x_i-\\mu_{\\beta}}{\\delta_{\\beta}^2+\\xi}\\sim N(0, 1)$$ And $\\xi = 1\\times10^{-5}$ is used to avoid zero division. Batch Normalization Affine MappingSome hidden layers need data that is not standardized distribution, so Batch Normalization provides affine mapping $y_i=\\gamma \\widehat{x}_i+\\beta$ for standard variables $x_i$ to restore expression ability in neural network. And these parameters will be trained with network original weights.In training process, calculates the mean and variance of the moving average: $$running_{mean} = (1-momentum)\\times running_{mean} + momentum\\times \\mu_{\\beta}$$ $$running_{var} = (1-momentum)\\times running_{var} + momentum\\times \\delta^2_{\\beta}$$ After model training, we can get trained two parameters $\\beta$ and $\\gamma$ and two variables running_mean and running_var. If we use this model to do inference, we need to do the following transformation: $$y = \\frac{\\gamma}{\\sqrt{running_{var}}+\\xi}\\cdot x+(\\beta-\\frac{\\gamma}{\\sqrt{running_{var}}+\\xi}\\cdot running_{mean})$$ Batch Normalization UsageIn Pytorch, torch.nn.BatchNorm1d achieved Batch Normalization function, also used as a typical layer in neural network. It has two critical parameters: num_features determines the number of features and affine determines whether Batch Normalization uses affine mapping or not. 1234567891011121314151617181920212223242526272829303132333435import torchfrom torch import nn# prepare running environmentsm = nn.BatchNorm1d(num_features=5, affine=False)print(&quot;BEFORE:&quot;)print(&quot;running_mean:&quot;, m.running_mean)print(&quot;running_var:&quot;, m.running_var)for _ in range(100): input = torch.randn(20, 5) output = m(input)print(&quot;AFTER:&quot;)print(&quot;running_mean:&quot;, m.running_mean)print(&quot;running_var:&quot;, m.running_var)m.eval()for _ in range(100): input = torch.randn(20, 5) output = m(input)print(&quot;EVAL:&quot;)print(&quot;running_mean:&quot;, m.running_mean)print(&quot;running_var:&quot;, m.running_var)BEFORE:running_mean: tensor([0., 0., 0., 0., 0.])running_var: tensor([1., 1., 1., 1., 1.])AFTER:running_mean: tensor([-0.0585, -0.0522, -0.0177, 0.0318, 0.0267])running_var: tensor([1.0350, 1.0406, 0.9752, 1.0325, 0.9902])EVAL:running_mean: tensor([-0.0585, -0.0522, -0.0177, 0.0318, 0.0267])running_var: tensor([1.0350, 1.0406, 0.9752, 1.0325, 0.9902]) In Batch Normalization, two parameters of affine mapping $\\gamma$ and $\\beta$ are called weight and bias. When not using affine, these parameters will be set None. 12345678910111213141516171819202122print(&quot;no affine, gamma:&quot;, m.weight)print(&quot;no affine, beta:&quot;, m.bias)m_affine = nn.BatchNorm1d(num_features=5, affine=True)print(&quot;with affine, gamma:&quot;, m_affine.weight, type(m_affine.weight))print(&quot;with affine, beta:&quot;, m_affine.bias, type(m_affine.bias))BEFORE:running_mean: tensor([0., 0., 0., 0., 0.])running_var: tensor([1., 1., 1., 1., 1.])AFTER:running_mean: tensor([-0.0585, -0.0522, -0.0177, 0.0318, 0.0267])running_var: tensor([1.0350, 1.0406, 0.9752, 1.0325, 0.9902])EVAL:running_mean: tensor([-0.0585, -0.0522, -0.0177, 0.0318, 0.0267])running_var: tensor([1.0350, 1.0406, 0.9752, 1.0325, 0.9902])&gt;&gt;&gt; no affine, gamma: None&gt;&gt;&gt; no affine, beta: None&gt;&gt;&gt; with affine, gamma: Parameter containing:tensor([1., 1., 1., 1., 1.], requires_grad=True) &lt;class 'torch.nn.parameter.Parameter'&gt;&gt;&gt;&gt; with affine, beta: Parameter containing:tensor([0., 0., 0., 0., 0.], requires_grad=True) &lt;class 'torch.nn.parameter.Parameter'&gt; As the results described, the types of m_affine.weight and m_affine.bias are both Parameter. That means they will be particapated in model training process but the types of running_mean and running_var are both Tensor which is called buffer updated and saved as intermediate variables. Chapter 7 Convolutional Neural Network and Computer Vision7.1 Convolutional Neural Network Basic IdeasI. Local ConnectionsFor traditional BP neural networks, previous layer usually connects with next layer by global connections.Suppose that there is a former layer with $M$ nodes and a latter layer with $N$ nodes, the network will increase more $M\\times N$ weights. So it results in calculation cost and memory cost with the complexity of $O(M\\times N)=O(n^2)$.On the other hand, local connections mode only connects nearby nodes between layers, based on the knowledge that only combination of some local pixels presents some features together. If we limit the connections to $C$ nearby nodes in space, the connected weights will be reduced to $C\\times N$. In that mode, the complexity of calculation and memory will be also reduced to $O(C\\times N)=O(N)$. II. Parameters SharingIn image processing, we hold the bilief that the features of image have locality. Because local features don’t have similarity, if we use specific weights for different local features it will not get better performance. And consider different images has great difference in the structure, we share connected weights between nodes with different local features.According to this mode, we can further reduce the number of connected weights to $C$ with the complexity of $O(C)$. 7.2 Convolutional OperationDiscrete convolution operation satisfies above properties which are local connections and parameters sharing.Define continuous convolution operation $f*g$: $$(f*g)(t)=\\int_{-\\infty}^{\\infty}f(\\tau)g(t-\\tau)d\\tau$$ And one-dimensional discrete convolution operation: $$(f*g)(x)=\\sum\\limits_if(i)g(x-i)$$ Multiple Convolutional KernelsCNN usually uses multiple convolutional kernels to extract features for better performance. Multiple channels convolution Boundary FillingIn order to avoid convolutional feature collapse, we usually fill the bounds of the input tensor to make the center of the convolution kernel can start scanning from the boundary. In this way, the size of the input tensor and output tensor of the convolution operation does not change. 7.3 Classic Network StructureI. VGG Network【Features】 Replace big size kernels with multiple size of $3\\times 3$ kernels Same receptive field of the convolutional kernels but deeper network structure Reduce model’s parameters using size of $3\\times 3$ kernels II. InceptionNetIII. ResNet","link":"/blog/Chapter5-8/"},{"title":"Chapter 1 to 4","text":"Chapter 1—Introduction of Deep LearningBackgroundIn 1981, neurobiologist David Hubel discovered the mechanism of information processing in the visual system, demonstrating that the visual cortex of the brain is hierarchical. His contribution is mainly two, one is that he believes that recognizing visual functions, one is abstraction, and the other is iteration.Abstraction is the abstraction of very concrete, figurative elements, that is, primitive light pixels and other information, to form meaningful concepts. These meaningful concepts will iterate upwards and become more abstract concepts that people can perceive.Thus, for computers, it needs to simulate the process of abstraction and recursive iteration. Modern Deep LearningConvolutional neural networks(CNN) simulate this process with convolutional layers that are usually stacked.The lower convolutional layers can extract local features of the images, such as corners, edges, lines, and so on. The higher convolutional layers are able to learn more complex features from the lower convolutional layers to achieve classification and recognition of the images. Reinforcement LearningReinforcement Learning mainly includes 4 elements: agent, state, action, reward. Features There are no supervisors, only a feedback signal.Feedback is delayed and not generated immediately.Reinforcement learning is sequential learning, and time has an important meaning in reinforcement learning.The behavior of the agent will affect all future decisions. Chapter 2—Deep Learning FrameworkCaffeCaffe(Convolutional Architecture for Fast Feature Embedding): mainly used in video and image processing.The official website of Caffe is http://caffe.berkeleyvision.org/ TensorFlowTensorFlow is an open-source database that uses data flow graphs for numerical computation. Nodes represent mathematical operations in the graph, and the lines in the graph represent multidimensional arrays of data, that is, tensors, that are related to each other between nodes.Computation Graph in TensorFlow: The leaf node or start node is always a tensor. Tensors cannot appear as non-leaf nodes. Computational graphs always express complex operations in a hierarchical order. PytorchThe biggest advantage of PyTorch is that the neural network built is dynamic, while TensorFlow and Caffe are both static neural network structures.The design of PyTorch follows the three levels of abstraction from low to high, $tensor\\rightarrow variable(autograd)\\rightarrow nn.Module$, representing high-dimensional arrays, automatic derivation, and neural networks. Chapter 3—Machine Learning BasicsBasic Concepts Loss Function: $L(y, \\hat{y})$ is a measure of model error. Training Error: average error on the training set. Generalization Error: average error on the test set. If we unilaterally pursue the minimization of training errors, it will lead to an increase in the complexity of model parameters, resulting in overfitting of the model.To prevent overfitting: Validate set tuning parameters The selection of parameters (i.e. parameter tuning) must be carried out on a dataset independent of the training and testing sets, and such a dataset used for model tuning is called a development or validation set. Loss function for regularization Regularization is added to the optimization objective to punish the complexity of redundancy. $$\\mathop{min}\\limits_{h}L(\\boldsymbol{y},\\boldsymbol{\\hat{y}};\\boldsymbol{\\theta})+\\lambda\\cdot J(\\boldsymbol{\\theta})$$ Supervised LearningSupervised learning is mainly applicable to two main types of problems: regression and classification. Classification Model evaluation metrics: Balanced problems: $Accuracy = \\frac{k}{D}$ Non-balanced problems: $F-Metric$ Non-balanced problemsDefine the class that is a minority of the sample as a positive class and the class that is a majority of the sample as a negative class 【Predictions】 Predict a positive sample as a positive class(true positive, TP) Predict a negtive sample as a positive class(false positive, FP) Predict a positive sample as a negtive class(flase negtive, FN) Predict a negtive sample as a negtive class(true negtive, TN) $$Define\\ \\ recall:\\R = \\frac{|TP|}{|TP|+|FN|}$$ The recall rate measures the rate of correct detection by the model among all positive samples, so it also becomes the recall rate. $$Define\\ \\ precision:\\ P = \\frac{|TP|}{|TP|+|FP|}$$ The accuracy rate measures the percentage of all samples predicted by the model to be positive, and is therefore also called the accuracy rate.F-Metric reconciles the average between recall and precision. $$F_{\\alpha}=\\frac{(1+\\alpha^2)RP}{R+\\alpha^2P}$$ $$if\\ \\alpha=1\\Longrightarrow F_1=\\frac{2RP}{R+P}$$ Chapter 4—Pytorch Deep LearningPytorch Tensor Features Tensor can use GPU for calculation In the calculation, it can be automatically added to the calculation diagram as a node, and it can be automatically differentiated. Tensor ObjectBasic Operations12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788&gt;&gt;&gt; import torch&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; print('torch.Tensor default format:{}'.format(torch.Tensor(1).dtype))torch.Tensor default format:torch.float32 # single point float&gt;&gt;&gt; print('torch.tensor default format:{}'.format(torch.tensor(1).dtype))torch.tensor default format:torch.int64 # 64 bits integer# Create tensor by list&gt;&gt;&gt; a = torch.tensor([[1,2,3],[3,4,5]],dtype=torch.float64)&gt;&gt;&gt; print(a)tensor([[1., 2., 3.], [3., 4., 5.]], dtype=torch.float64)# Create tensor by ndarray&gt;&gt;&gt; b = torch.tensor(np.array([[1,2,3],[3,4,5]]),dtype=torch.uint8)&gt;&gt;&gt; print(b)tensor([[1, 2, 3], [3, 4, 5]], dtype=torch.uint8) # Set tensor device&gt;&gt;&gt; cuda0 = torch.device('cuda:0')&gt;&gt;&gt; c = torch.ones((2,2),device=cuda0)&gt;&gt;&gt; print(c) tensor([[1., 1.], [1., 1.]], device='cuda:0')# copy tensor to CPU&gt;&gt;&gt; c = c.to('cpu', torch.double)&gt;&gt;&gt; print(c.device)cpu# tensors multiply&gt;&gt;&gt; a = torch.tensor([[1,2],[3,4]])&gt;&gt;&gt; b = torch.tensor([[1,2],[3,4]]) &gt;&gt;&gt; c = a*b # multiply by corresponding element&gt;&gt;&gt; print(c)tensor([[ 1, 4], [ 9, 16]])&gt;&gt;&gt; c = torch.mm(a, b) # multiply by metrixs&gt;&gt;&gt; print(c)tensor([[ 7, 10], [15, 22]]) # Special functions&gt;&gt;&gt; a = torch.tensor([[1,2],[3,4]])# Discard too small or too large elements in matrix&gt;&gt;&gt; torch.clamp(a,min=2,max=3)tensor([[2, 2], [3, 3]])# Rounds to the nearest integer&gt;&gt;&gt; a=torch.tensor([[-1.2, -1.5, 0.3], [-0.8, 7.4, 1.2]])&gt;&gt;&gt; torch.round(a)tensor([[-1., -2., 0.], [-1., 7., 1.]])# Hyperbolic tangent function, mapping function values to (0,1)&gt;&gt;&gt; torch.tanh(a)tensor([[-0.8337, -0.9051, 0.2913], [-0.6640, 1.0000, 0.8337]]) # Create tensors&gt;&gt;&gt; print(torch.arange(5))tensor([0, 1, 2, 3, 4])&gt;&gt;&gt; print(torch.arange(1,10,2)) # the third parameter is foot lengthtensor([1, 3, 5, 7, 9])&gt;&gt;&gt; print(torch.linspace(0, 5, 10)) # the third parameter is elements numbertensor([0.0000, 0.5556, 1.1111, 1.6667, 2.2222, 2.7778, 3.3333, 3.8889, 4.4444, 5.0000])&gt;&gt;&gt; print(torch.ones(3,3))tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]])&gt;&gt;&gt; print(torch.zeros((3,3), dtype=torch.uint8))tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=torch.uint8)# Random numbers# uniform distribution of [0,1]&gt;&gt;&gt; torch.rand(3,3)tensor([[0.7228, 0.4286, 0.5304], [0.4195, 0.0597, 0.5446], [0.0225, 0.4951, 0.5704]])# elements that are sampled satisfy normal distribution&gt;&gt;&gt; torch.randn(3,3)tensor([[-0.5142, 0.6429, -1.8848], [-0.7895, -0.5115, 0.0638], [ 1.7370, -2.2135, 0.1514]])# uniform distribution of {a,...,b} and not including b&gt;&gt;&gt; torch.randint(0,9,(3,3))tensor([[4, 6, 6], [3, 7, 6], [2, 7, 4]]) Indexes and Slices1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&gt;&gt;&gt; a = torch.arange(9).view(3,3) &gt;&gt;&gt; a[2,2] # basic indextensor(8)&gt;&gt;&gt; a[1:, :-1] # slicetensor([[3, 4], [6, 7]])&gt;&gt;&gt; a[::2] # slice with foot lengthtensor([[0, 1, 2], [6, 7, 8]])&gt;&gt;&gt; rows=[0,1]&gt;&gt;&gt; cols=[2,2]&gt;&gt;&gt; a[rows,cols] # integer indextensor([2, 5])&gt;&gt;&gt; index=a&gt;4 # bool index&gt;&gt;&gt; print(index)tensor([[False, False, False], [False, False, True], [ True, True, True]])&gt;&gt;&gt; print(a[index])tensor([5, 6, 7, 8])# torch.nonzero return index matrix that corresponding value is not zero&gt;&gt;&gt; a = torch.randint(0,2,(3,3))&gt;&gt;&gt; print(a)tensor([[1, 0, 1], [1, 1, 0], [0, 1, 0]])&gt;&gt;&gt; index=torch.nonzero(a)&gt;&gt;&gt; print(index)tensor([[0, 0], [0, 2], [1, 0], [1, 1], [2, 1]])&gt;&gt;&gt; [[i,j,a[i, j]] for i, j in index][[tensor(0), tensor(0), tensor(1)], [tensor(0), tensor(2), tensor(1)], [tensor(1), tensor(0), tensor(1)], [tensor(1), tensor(1), tensor(1)], [tensor(2), tensor(1), tensor(1)]]# torch.where(condition, x, y) if condition is true then return corresponding element in x else return the corresponding element in y. x.shape()=y.shape()&gt;&gt;&gt; x = torch.randn(3,2)&gt;&gt;&gt; y = torch.ones(3,2)&gt;&gt;&gt; print(x)tensor([[-1.2620, -1.9865], [ 1.1022, 0.8017], [-0.5744, -0.4387]])&gt;&gt;&gt; print(torch.where(x&gt;0,x,y))tensor([[1.0000, 1.0000], [1.1022, 0.8017], [1.0000, 1.0000]]) Tensor Transformation, Splicing and Splitting12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394&gt;&gt;&gt; a = torch.rand(1,2,3,4,5)&gt;&gt;&gt; print(&quot;the number of elements:&quot;, a.nelement()) the number of elements: 120&gt;&gt;&gt; print(&quot;the number of axises:&quot;, a.ndimension())the number of dimensions: 5 &gt;&gt;&gt; print(&quot;matrix dimension:&quot;, a.size(), a.shape) matrix dimension: torch.Size([1, 2, 3, 4, 5]) torch.Size([1, 2, 3, 4, 5]) # Tensor Reshape# For Tensor.view, physical storage for tensors must be continuous&gt;&gt;&gt; a = torch.rand(1,2,3,4,5)&gt;&gt;&gt; b = a.view(2*3,4*5)&gt;&gt;&gt; print(b.shape)torch.Size([6, 20])&gt;&gt;&gt; c = a.reshape(-1) # if set -1 in some dimension, it will be calculated automatically&gt;&gt;&gt; print(c.shape)torch.Size([120])&gt;&gt;&gt; d = a.reshape(2*3, -1)&gt;&gt;&gt; print(d.shape)torch.Size([6, 20])# discard some axises whose dimension is 1&gt;&gt;&gt; b = torch.squeeze(a)&gt;&gt;&gt; print(b.shape)torch.Size([2, 3, 4, 5])# add 1 dimension at axis 0&gt;&gt;&gt; torch.unsqueeze(b,0).shapetorch.Size([1, 2, 3, 4, 5])# Tensor Transpose# 2 dimensions matrix&gt;&gt;&gt; &gt;&gt;&gt; b = torch.tensor([2,3])&gt;&gt;&gt; btensor([2, 3])&gt;&gt;&gt; print(torch.t(b))tensor([2, 3])&gt;&gt;&gt; b = torch.tensor([[2,3]]) # transpose the outer matrix&gt;&gt;&gt; print(torch.t(b))tensor([[2], [3]])&gt;&gt;&gt; print(torch.t(b[0]))tensor([2, 3])&gt;&gt;&gt; print(torch.transpose(b, 1, 0))tensor([[2], [3]])# high latitude matrix&gt;&gt;&gt; a = torch.rand((1,224,224,3))&gt;&gt;&gt; print(a.shape) torch.Size([1, 224, 224, 3])&gt;&gt;&gt; b = a.permute(0,2,3,1)&gt;&gt;&gt; print(b.shape)torch.Size([1, 224, 3, 224])# Tensor splicing# torch.cat will splice matrices on the existed axis&gt;&gt;&gt; a = torch.randn(2,3)&gt;&gt;&gt; b = torch.randn(3,3)&gt;&gt;&gt; c = torch.cat((a, b)) # input (a, b) must be tuple because there are other attributes&gt;&gt;&gt; d = torch.cat((b, b, b), dim=1)&gt;&gt;&gt; print(c.shape)torch.Size([5, 3])&gt;&gt;&gt; print(d.shape) torch.Size([3, 9])# torch.stack will splice metrices on a new axis&gt;&gt;&gt; c = torch.stack((b,b), dim=1) &gt;&gt;&gt; d = torch.stack((b,b), dim=0) &gt;&gt;&gt; print(c.shape)torch.Size([3, 2, 3])&gt;&gt;&gt; print(d.shape)torch.Size([2, 3, 3])# Tensor Splitting# torch.split input every matrix's size to be split&gt;&gt;&gt; a = torch.randn(10, 3)&gt;&gt;&gt; &gt;&gt;&gt; for x in torch.split(a, [1,2,3,4], dim=0):... print(x.shape) ... torch.Size([1, 3])torch.Size([2, 3])torch.Size([3, 3])torch.Size([4, 3])&gt;&gt;&gt; for x in torch.split(a, 4, dim=0): # also can be an integer ... print(x.shape)... torch.Size([4, 3])torch.Size([4, 3])torch.Size([2, 3])# torch.chunk input matrix total number to be split&gt;&gt;&gt; for x in torch.chunk(a, 4, dim=0): ... print(x.shape)... torch.Size([3, 3])torch.Size([3, 3])torch.Size([3, 3])torch.Size([1, 3]) Tensor Reduction12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&gt;&gt;&gt; a = torch.tensor([1,2],[3,4])&gt;&gt;&gt; print(&quot;global maximum value:&quot;, torch.max(a)) global maximum value: tensor(4)&gt;&gt;&gt; print(&quot;colomn maximum value:&quot;, torch.max(a, dim=0))colomn maximum value: torch.return_types.max(values=tensor([3, 4]),indices=tensor([1, 1]))&gt;&gt;&gt; print(&quot;colomn accumulation value:&quot;, torch.cumsum(a, dim=0))colomn accumulation value: tensor([[1, 2], [4, 6]])&gt;&gt;&gt; print(&quot;row multiplication value:&quot;, torch.cumprod(a, dim=1))row multiplication value: tensor([[ 1, 2], [ 3, 12]])&gt;&gt;&gt; a = torch.Tensor([[1,2], [3,4]]) # data type to floating point&gt;&gt;&gt; a.mean(), a.median(), a.std()(tensor(2.5000), tensor(2.), tensor(1.2910))# calculate with the axis&gt;&gt;&gt; a.mean(dim=0), a.median(dim=0), a.std(dim=0)(tensor([2., 3.]), torch.return_types.median( values=tensor([1., 2.]),indices=tensor([0, 0])), tensor([1.4142, 1.4142]))&gt;&gt;&gt; a = torch.randint(0,3,(3,3))&gt;&gt;&gt; print(a)tensor([[2, 0, 1], [1, 2, 2], [1, 0, 1]])# torchl.unique function find unique elements in the matrix&gt;&gt;&gt; print(torch.unique(a))tensor([0, 1, 2])# Pytorch Tensor Automatic Differentiation&gt;&gt;&gt; x = torch.arange(9).view(3, -1)&gt;&gt;&gt; x.requires_gradFalse&gt;&gt;&gt; x = torch.rand(3, 3, requires_grad=True)&gt;&gt;&gt; print(x)tensor([[0.9214, 0.6373, 0.4736], [0.4541, 0.8828, 0.4526], [0.2727, 0.8647, 0.0488]], requires_grad=True)&gt;&gt;&gt; w = torch.ones(3,3,requires_grad=True)&gt;&gt;&gt; print(w)tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], requires_grad=True)&gt;&gt;&gt; y = torch.mm(w,x)&gt;&gt;&gt; print(y)tensor([[1.6481, 2.3848, 0.9750], [1.6481, 2.3848, 0.9750], [1.6481, 2.3848, 0.9750]], grad_fn=&lt;MmBackward0&gt;)# cancle tensor's automatic differentiation&gt;&gt;&gt; detached_y = y.detach()&gt;&gt;&gt; print(detached_y)tensor([[1.6481, 2.3848, 0.9750], [1.6481, 2.3848, 0.9750], [1.6481, 2.3848, 0.9750]])&gt;&gt;&gt; yy = torch.mean(y)&gt;&gt;&gt; yy.backward() # grad backward&gt;&gt;&gt; print(y.grad)None&gt;&gt;&gt; print(w.grad)tensor([[0.2258, 0.1988, 0.1318], [0.2258, 0.1988, 0.1318], [0.2258, 0.1988, 0.1318]])&gt;&gt;&gt; print(x.grad) tensor([[0.3333, 0.3333, 0.3333], [0.3333, 0.3333, 0.3333], [0.3333, 0.3333, 0.3333]])# with torch.no_grad() includes code snippets that do not compute differentitation&gt;&gt;&gt; y =torch.sum(torch.mm(w,x)) &gt;&gt;&gt; print(y.requires_grad)True&gt;&gt;&gt; with torch.no_grad():... y = torch.sum(torch.mm(w,x))... print(y.requires_grad)... False","link":"/blog/Chapter1-4/"},{"title":"Common Screen Commands","text":"IntroductionIn project development, when executing programs on the Linux terminal, if the terminal is closed, the program execution will also terminate. This poses significant inconvenience for long-running programs. Screen facilitates the management of multiple command-line workflows without concern for their interference. Programs are automatically backgrounded and continue execution until completion. Start a New screen Session1234# Start a new screen session named &quot;my_session&quot;screen -S my_session# Automatically name the new screen sessionscreen View Existing screen Sessions12# List all screen sessionsscreen -ls Attach to an Existing screen Session12# Attach to the screen session named &quot;my_session&quot;screen -r my_session Detach from an Existing screen Session12345678# Way 1# Detach from the screen session named &quot;my_session&quot;screen -d my_session# Way 2# Enter the following keys in turn and the program# will also continue to execute in the backgroundCtrl+aa Delete an Existing screen Session12# Delete the screen session named &quot;my_session&quot;screen -X -S my_session quit","link":"/blog/CommonScreenCommands/"},{"title":"EBlog Configuration on Icarus Theme","text":"Blog Configuration On Github1. Setup initialize Hexo project in the target &lt;folder&gt; 12$ hexo init &lt;folder&gt;$ cd &lt;folder&gt; 2. Install Icarus Theme To install Icarus as a node package via NPM, run the following command from the root of your Hexo site: 1npm install -S hexo-theme-icarus hexo-renderer-inferno Use the hexo command to change the theme to Icarus: 1hexo config theme icarus 3. Theme Configuration Icarus’ default theme configuration file is _config.icarus.yml. 3.1 Overall style configurationVersionThis version of the theme configuration file is not advised to change by yourself, determining whether to upgrade the theme configuration. 1version: 5.1.0 Theme Variantdefault and cyberpunk determine the skin of Icarus theme. default has always been suggested if you want to make a acadmic style blog. 1variant: default LogoThe logo of your site will display on the navigation bar and the footer. The value of the logo can either be the path or URL to your logo image: 1logo: https://ms-blogimage.oss-cn-chengdu.aliyuncs.com/picture/img/EBlog202306222351609.png FaviconSet a icon for your blog website in the form of URL or path to the website’s icon. 12head: favicon: https://ms-blogimage.oss-cn-chengdu.aliyuncs.com/picture/img/EBlog202306222351609.png Navigation BarThe navbar section defines the menu items and links in the navigation bar. You may put any menu item in the navigation bar by adding : to the menu setting. To put links on the right side of the navigation bar, add : to the links setting. 12345678910111213navbar: # Navigation menu items menu: Zhihao Li's Blog: / Archives: /archives Categories: /categories Tags: /tags About: /about # Links to be shown on the right of the navigation bar links: GitHub: icon: fab fa-github url: https://github.com/LZHMS/LZHMS.github.io FooterThe footer section defines the links on the right side of the page footer. The link format is exactly the same as links in the navbar section. 1234567891011121314footer: # Copyright text copyright: © 2023 Zhihao Li # Links to be shown on the right of the footer section links: Creative Commons: icon: fab fa-creative-commons url: https://creativecommons.org/ Attribution 4.0 International: icon: fab fa-creative-commons-by url: https://creativecommons.org/licenses/by/4.0/ GitHub: icon: fab fa-github url: https://github.com/LZHMS/LZHMS.github.io 3.2 Article configurtionCode Highlight You can choose a theme from all themes listed under highlight.js/src/styles to customize the code blocks. Copy the file name (without the .css extension) to the theme setting. To hide the “copy” button of every code block, set clipboard to false. If you wish to fold or unfold all code blocks, set the fold setting to folded or unfolded. 12345678highlight: # Code highlight themes # https://github.com/highlightjs/highlight.js/tree/master/src/styles theme: atom-one-light # Show copy code button clipboard: true # Default folding status of the code blocks. Can be &quot;&quot;, &quot;folded&quot;, &quot;unfolded&quot; fold: unfolded Read TimeYou can show a word counter and the estimated reading time of your article above the article title by setting readtime to true in the article section. 12article: readtime: true Update Timeset update_time to true in the article section of your theme configuration file to show every article updated time. 12article: update_time: true Article LicensingYou can show a section at the end of your posts/pages describing the licensing of your work. Both text and icons are accepted as license links. 123456789101112article: # Article licensing block licenses: Creative Commons: icon: fab fa-creative-commons url: https://creativecommons.org/ Attribution: icon: fab fa-creative-commons-by url: https://creativecommons.org/licenses/by/4.0/ Noncommercial: icon: fab fa-creative-commons-nc url: https://creativecommons.org/licenses/by-nc/4.0/ SidebarTo make a sidebar fixed when you scroll the page, set the sticky setting of that sidebar to true in the sidebar section. 123456789sidebar: # Left sidebar configurations left: # Whether the sidebar sticks to the top when page scrolls sticky: false # Right sidebar configurations right: # Whether the sidebar sticks to the top when page scrolls sticky: true 3.3 Widgets ConfigurationProfile Widget Set multiple author_title and display by rows and set font-family in hexo-theme-icarus\\layout\\widget\\profile.jsx 123{author ? &lt;p class=&quot;title is-size-4 is-block&quot; style=&quot;line-height: 'inherit'; font-family: Times New Roman&quot;&gt;{author}&lt;/p&gt; : null}{authorTitle ? &lt;p style=&quot;white-space: pre-line; font-style: italic; font-family: Times New Roman; margin-bottom: 0.50rem; font-size: 1.0em&quot;&gt;{authorTitle}&lt;/p&gt; : null}{location ? &lt;p class=&quot;is-size-5 is-flex justify-content-center&quot; style=&quot;font-family: Times New Roman&quot;&gt; 1234# Author titleauthor_title: | Computer Science Machine Learning Set social_links using Font Awesome Icons 12345678910111213141516social_links: Github: icon: fab fa-github url: https://github.com/LZHMS Facebook: icon: fab fa-facebook url: https://www.facebook.com/profile.php?id=100094074308733 Twitter: icon: fab fa-twitter url: https://twitter.com/ZhihaoLi1376106 Email: icon: fa-solid fa-envelope url: mailto:LZH1314521ligao@163.com QQ: icon: fab fa-qq url: https://w.4rxb.com/s/yq3hxp 4. Configure Home Page on the SiteIn the normal case, the default home page includes some abstacts of blogs. However, in some case we want to to display our information or introduction about the website. That’s the time when we need to individually configure the home page. Create a index.md anticle under the source directoryThis is an anticle used for our individual content to be display on the home page. Modify index_generator in the file of _config.ymlWe need to modify the index_generator:path to an invalid value for example default-index in order to shield the default home page. Add the home page to the websiteIn the configuration file of theme, we can add an item of home under menu item. And then set the home value like / || fa fa-home if we need an icon for the home page. 5. Open the GalleryIf we want to display multiple pictures in the gallery, we can use the following code to open the gallery. 12&lt;div class=&quot;justified-gallery&quot;&gt;&lt;/div&gt;","link":"/blog/EBlogConfiguration/"},{"title":"A Graph Plot Tool that Integrates Data Processing And Visualization","text":"Project Envrionments123456789# confiure the project environmentimport pandas as pdimport numpy as npfrom sklearn.preprocessing import Normalizerimport matplotlib.pyplot as pltimport seaborn as snsfrom matplotlib.font_manager import FontPropertiesimport warningswarnings.filterwarnings('ignore') Overview Structure123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347class GraphPlot: # Initialize the configuration of GraphPlot Tool def __init__(self, fname1=&quot;times.ttf&quot;, fname2=&quot;ARLRDBD.TTF&quot;): # Set up Seaborn style sns.set(style=&quot;darkgrid&quot;) plt.rcParams['axes.unicode_minus'] = False # Fonts style self.Efont_prop1 = FontProperties(fname=&quot;C:/Windows/Fonts/&quot; + fname1) self.Efont_prop2 = FontProperties(fname=&quot;C:/Windows/Fonts/&quot; + fname2) # Set colors self.colors_name = ['yellow', 'blue', 'green', 'magenta', 'red', 'cyan', 'purple', 'orange', 'gray', 'pink'] self.colors_rgb = ['#f94144', '#f3722c', '#f8961e', '#f9844a', '#f9c74f', '#90be6d', '#43aa8b', '#4d908e', '#577590', '#277da1'] self.figsize = (10, 6) self.dpi = 400 self.data = None def LoadData(self, data_path, sheet_name=0, sep=',', header=0, index_col=None): &quot;&quot;&quot;LoadData Function Input: data_path: the path of data file sheet_name: the table of Excel file sep: the separate sign of csv file header: the row of column names index_col: the index column of data file Output: self.data: the data read from given data file Function: From given source to reach the needed data. &quot;&quot;&quot; if data_path.split('.')[-1] in ('xlsx', 'xls'): self.data = pd.read_excel(data_path, sheet_name=sheet_name, header=header, index_col=index_col) elif data_path.split('.')[-1] == 'csv': self.data = pd.read_csv(data_path, sep=sep, header=header, index_col=index_col) else: raise Exception('Unknown data file type!') self.PrintData() def DataProcessing(self, date_name=None, fill_name=None, fill_method=&quot;Nearest&quot;, standard_name=None, standard_method=&quot;Zscore&quot;): &quot;&quot;&quot;DataProcessing Function Input: date_name: the column need to be converted to date style fill_name: the columns need to fill the missing values fill_method: the methods used to fill the missing values, alternatives like &quot;Nearest&quot;(default), &quot;Linear&quot;, &quot;Polynomial&quot;, &quot;Spline&quot;, &quot;Mean&quot;, &quot;Ffill&quot;, &quot;Bfill&quot;, other specific value. standard_name: the columns need to standardized standard_method: the methods used to standardize the columns, alternatives like &quot;Zscore&quot;(default), &quot;Minmax&quot;. Output: self.data: the data has been transformed Function: Process the data got from dat file by filling the missing values and standardize some columns. &quot;&quot;&quot; if date_name is not None: # Assuming the date_name column has be converted to the format like &quot;2024-1-31&quot; self.data[date_name] = pd.to_datetime(self.data[date_name]) if fill_name is not None: if fill_method == &quot;Nearest&quot;: self.data[fill_name] = self.data[fill_name].interpolate(method=&quot;nearest&quot;) elif fill_method == &quot;Linear&quot;: self.data[fill_name] = self.data[fill_name].interpolate(method=&quot;linear&quot;) elif fill_method == &quot;Polynomial&quot;: self.data[fill_name] = self.data[fill_name].interpolate(method=&quot;polynomial&quot;, order=2) elif fill_method == &quot;Spline&quot;: self.data[fill_name] = self.data[fill_name].interpolate(method=&quot;spline&quot;, order=2) elif fill_method == &quot;Mean&quot;: self.data[fill_name] = self.data[fill_name].fillna(self.data[fill_name].mean()) elif fill_method == &quot;Ffill&quot;: self.data[fill_name] = self.data[fill_name].fillna(method=&quot;ffill&quot;) elif fill_method == &quot;Bfill&quot;: self.data[fill_name] = self.data[fill_name].fillna(method=&quot;bfill&quot;) else: self.data[fill_name] = self.data[fill_name].fillna(int(fill_method)) if standard_name is not None: stdata = self.data[standard_name] if standard_method == &quot;Zscore&quot;: self.data[standard_name] = (stdata - stdata.mean()) / stdata.std() elif standard_method == &quot;Minmax&quot;: self.data[standard_name] = (stdata - stdata.min()) / (stdata.max() - stdata.min()) self.PrintData() def PrintData(self): &quot;&quot;&quot;PrintData Function Input: None Output: None Function: Output the length of data, the first 5 elements and last 5 elements of data. &quot;&quot;&quot; print('-'*30 + '\\n' + 'Data Information:\\n' + '-'*30) print(&quot;Data Length: %d&quot; % (len(self.data))) print(&quot;Data Head:&quot;) print(self.data.head()) print(&quot;Data Tail:&quot;) print(self.data.tail()) print('-'*30) def LinePlot(self, column_X, column_Y, fontsize=8, color=7, font_prop=None, title=None): &quot;&quot;&quot;LinePlot Function Input: column_X: the column data as the x axis column_Y: the column data as the y axis fontsize: the main size of the font in the figure color: the color of plotted line font_prop: the main property of fonts title: the title of the figure Output: None Function: Plot a normal line figure and save the image as the format of SVG. &quot;&quot;&quot; font_prop = self.Efont_prop2 if font_prop is None else font_prop plt.figure(figsize=self.figsize, dpi=self.dpi) plt.plot(column_X, column_Y, color=self.colors_name[color]) plt.yticks(fontproperties=font_prop, fontsize=fontsize) plt.xticks(fontproperties=font_prop, fontsize=fontsize) plt.title(title, fontproperties=font_prop, fontsize=fontsize + 4) plt.savefig(&quot;images/LinePlot.svg&quot;, format=&quot;svg&quot;) def BiLinesPlot(self, column_X, column_Y1, column_Y2, fontsize=8, colors=[7, 6], labels=['Y1', 'Y2'], font_prop=None, title=None): &quot;&quot;&quot;BiLinesPlot Function Input: column_X: the column data as the x axis column_Y1: the column data as the y1 axis column_Y2: the column data as the y2 axis fontsize: the main size of the font in the figure colors: the colors list of the two data series labels: the labels list of the two data series font_prop: the main property of fonts title: the title of the figure Output: None Function: Plot a two lines figure and save the image as the format of SVG. &quot;&quot;&quot; font_prop = self.Efont_prop2 if font_prop is None else font_prop plt.figure(figsize=self.figsize, dpi=self.dpi) plt.plot(column_X, column_Y1, color=self.colors_rgb[colors[0]], label=labels[0]) plt.plot(column_X, column_Y2, color=self.colors_rgb[colors[1]], label=labels[1]) plt.yticks(fontproperties=font_prop, fontsize=fontsize) plt.xticks(fontproperties=font_prop, fontsize=fontsize) plt.legend(prop=font_prop) plt.title(title, fontproperties=font_prop, fontsize=fontsize + 4) plt.savefig(&quot;images/BiLinesPlot.svg&quot;, format=&quot;svg&quot;) def MultiLinesPlot(self, column_X, columns_Y, fontsize=8, labels=None, font_prop=None, title=None): &quot;&quot;&quot;MultiLinesPlot Function Input: column_X: the column data as the x axis columns_Y: the columns data as the y axis must has the same length fontsize: the main size of the font in the figure labels: the labels list of the data series font_prop: the main property of fonts title: the title of the figure Output: None Function: Plot a multiple lines figure and save the image as the format of SVG. &quot;&quot;&quot; font_prop = self.Efont_prop2 if font_prop is None else font_prop plt.figure(figsize=self.figsize, dpi=self.dpi) labels = ['Y' + str(i+1) for i in range(columns_Y.shape[1])] if labels is None else labels for i in range(columns_Y.shape[1]): plt.plot(column_X, columns_Y[:, i], color=self.colors_rgb[i], label=labels[i]) plt.yticks(fontproperties=font_prop, fontsize=fontsize) plt.xticks(fontproperties=font_prop, fontsize=fontsize) plt.legend(prop=font_prop) plt.title(title, fontproperties=font_prop, fontsize=fontsize + 4) plt.savefig(&quot;images/MultiLinesPlot.svg&quot;, format=&quot;svg&quot;) def LineShadowPlot(self, column_X, column_Y, fontsize=8, color=7, alpha=0.3, font_prop=None, title=None): &quot;&quot;&quot;LineShadowPlot Function Input: column_X: the column data as the x axis column_Y: the column data as the y axis fontsize: the main size of the font in the figure color: the color of plotted line alpha: the parameter controls the transparency of the filled region font_prop: the main property of fonts title: the title of the figure Output: None Function: Plot a line figure with filled region and save the image as the format of SVG. &quot;&quot;&quot; font_prop = self.Efont_prop2 if font_prop is None else font_prop plt.figure(figsize=self.figsize, dpi=self.dpi) plt.plot(column_X, column_Y, color=self.colors_name[color]) plt.fill_between(column_X, column_Y, color=self.colors_name[color], alpha=alpha) plt.yticks(fontproperties=font_prop, fontsize=fontsize) plt.xticks(fontproperties=font_prop, fontsize=fontsize) plt.title(title, fontproperties=font_prop, fontsize=fontsize + 4) plt.tight_layout() plt.savefig(&quot;images/LineShadowPlot.svg&quot;, format=&quot;svg&quot;) def MultiLinesShadowPlot(self, column_X, columns_Y, fontsize=8, labels=None, alpha=0.3, font_prop=None, title=None): &quot;&quot;&quot;MultiLinesShadowPlot Function Input: column_X: the column data as the x axis columns_Y: the columns data as the y axis must has the same length fontsize: the main size of the font in the figure labels: the labels list of the data series alpha: the parameter controls the transparency of the filled region font_prop: the main property of fonts title: the title of the figure Output: None Function: Plot a multiple lines figure with filled region and save the image as the format of SVG. &quot;&quot;&quot; font_prop = self.Efont_prop2 if font_prop is None else font_prop plt.figure(figsize=self.figsize, dpi=self.dpi) labels = ['Y' + str(i+1) for i in range(columns_Y.shape[1])] if labels is None else labels for i in range(columns_Y.shape[1]): plt.plot(column_X, columns_Y[:, i], color=self.colors_rgb[i], label=labels[i]) plt.fill_between(column_X, columns_Y[:, i], color=self.colors_rgb[i], alpha=alpha) plt.yticks(fontproperties=font_prop, fontsize=fontsize) plt.xticks(fontproperties=font_prop, fontsize=fontsize) plt.legend(prop=font_prop) plt.title(title, fontproperties=font_prop, fontsize=fontsize + 4) plt.tight_layout() plt.savefig(&quot;images/MultiLinesShadowPlot.svg&quot;, format=&quot;svg&quot;) def NormPlot(self, column, bins=30, fontsize=8, colors=[7, 4], labels=['Data Distribution', 'Normal Distribution'], font_prop=None, title=None): &quot;&quot;&quot;NormPlot Function Input: column: the column data to visualize the distribution bins: the number of bars in hist fontsize: the main size of the font in the figure font_prop: the main property of fonts colors: the first element set the color of hist graph, and the second element set the color of curve graph labels: the label of hist graph and the label of curve graph title: the title of the figure Output: None Function: Plot a data distribution figure with the normal distribution curve and save the image as the format of SVG. &quot;&quot;&quot; font_prop = self.Efont_prop2 if font_prop is None else font_prop # Produce Norm Distribution Data mean, std = np.mean(column), np.std(column) x = np.linspace(mean - 3 * std, mean + 3 * std, 100) y = (1 / (np.sqrt(2 * np.pi) * std)) * np.exp(-0.5 * ((x - mean) / std) ** 2) # Plot Data Distribution plt.figure(figsize=self.figsize, dpi=self.dpi) plt.hist(column, bins=bins, density=True, alpha=0.7, color=self.colors_rgb[colors[0]], label=labels[0]) plt.plot(x, y, color=self.colors_name[colors[1]], label=labels[1]) plt.yticks(fontproperties=font_prop, fontsize=fontsize) plt.xticks(fontproperties=font_prop, fontsize=fontsize) plt.legend(prop=font_prop) plt.title(title, fontproperties=font_prop, fontsize=fontsize + 4) plt.savefig(&quot;images/NormPlot.svg&quot;, format=&quot;svg&quot;) def ACFPlot(self, column, lags, y_bound=[-1, 1], fontsize=8, font_prop=None, title=None): &quot;&quot;&quot;ACFPlot Function Input: column: the column data to visualize the distribution lags: the number of lags y_bound: the lower bound and upper bound of the y axis fontsize: the main size of fonts font_prop: the main property of fonts title: the title of the figure Output: None Function: Plot a ACF figure of the data and save the image as the format of SVG. &quot;&quot;&quot; from statsmodels.graphics.tsaplots import plot_acf font_prop = self.Efont_prop2 if font_prop is None else font_prop _, ax = plt.subplots(facecolor='white', figsize=self.figsize, dpi=self.dpi) plot_acf(column, lags=lags, ax=ax) ax.set_ylim(y_bound) plt.yticks(fontproperties=font_prop, fontsize=fontsize) plt.xticks(fontproperties=font_prop, fontsize=fontsize) plt.title(title, fontproperties=font_prop, fontsize=fontsize + 4) plt.savefig(&quot;images/ACFPlot.svg&quot;, format=&quot;svg&quot;) def HeatMapPlot(self, columns, fontsize=8, font_prop=None, title=None): &quot;&quot;&quot;HeatMapPlot Function Input: columns: the columns data need to calculate the correlation matrix fontsize: the main size of fonts font_prop: the main property of fonts title: the title of the figure Output: None Function: Plot a heat map figure of the data and save the image as the format of SVG. &quot;&quot;&quot; correlation_matrix = columns.corr() font_prop = self.Efont_prop2 if font_prop is None else font_prop font_format = {'fontsize': 10, 'fontweight': 'bold', 'color': 'black'} plt.figure(figsize=self.figsize, dpi=self.dpi) sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=&quot;.2f&quot;, annot_kws=font_format, cbar=False) plt.yticks(fontproperties=font_prop, fontsize=fontsize) plt.xticks(fontproperties=font_prop, fontsize=fontsize) plt.title(title, fontproperties=font_prop, fontsize=fontsize + 4) plt.tight_layout() plt.savefig(&quot;images/HeatMapPlot.svg&quot;, format=&quot;svg&quot;) def ScatterMatrixPlot(self, columns, bins=40, colors=[(0.3, 0.3, 0.3), (0.5, 0.5, 0.5)], alphas=[0.7, 0.7], fontsize=8, font_prop=None, title=None): &quot;&quot;&quot;ScatterMatrixPlot Function Input: columns: the columns data need to calculate the correlation matrix bins: the number bars of the hist graph colors: the edge color of the hist and the point color of the scatter graph alphas: the transparency of the edge in hist and point in scatter graph fontsize: the main size of fonts font_prop: the main property of fonts title: the title of the figure Output: None Function: Plot a scatter matrix figure of the data and save the image as the format of SVG. &quot;&quot;&quot; font_prop = self.Efont_prop2 if font_prop is None else font_prop variables = [columns.iloc[:, i] for i in range(columns.shape[1])] # Create a scatter matrix plot _, axes = plt.subplots(nrows=columns.shape[1], ncols=columns.shape[1], figsize=self.figsize, dpi=self.dpi) # Plot scatter plots for each pair of variables for i in range(columns.shape[1]): for j in range(columns.shape[1]): if i == j: # If on the diagonal, create a histogram axes[i, j].hist(variables[i], bins=bins, color=self.colors_rgb[i], edgecolor=colors[0], alpha=alphas[0]) else: # Otherwise, create a scatter plot axes[i, j].scatter(variables[j], variables[i], alpha=alphas[1], s=4, c=colors[1]) plt.yticks(fontproperties=font_prop, fontsize=fontsize) plt.xticks(fontproperties=font_prop, fontsize=fontsize) plt.title(title, fontproperties=font_prop, fontsize=fontsize + 4) plt.tight_layout() plt.savefig(&quot;images/ScatterMatrixPlot.svg&quot;, format=&quot;svg&quot;) Contributors Zhihao Li","link":"/blog/GraphPlotTool/"},{"title":"Solution to OpenSSL Connection Problems With Github","text":"Problems Uploading Files with GitSometimes we can use git tool to successfully upload projects to Github, but in other time especially after a period of configuration, we often meet the following error:OpenSSL SSL_read: Connection was reset, error 10054So this post is written to demonstrate this bug and give some solutions. What’s the OpenSSLFirstly, in the course of Computer Network, we had learned the HTTPS(Hypertext Transfer Protocol Secure) which works as authentication between client and server for data transmission. And the service of HTTPS works on SSL/TLS protocol.SSL/TLS is a protocol for establishing a secure connection between a client and a server. It is used for authentication, encryption, and integrity check. Therefore, OpenSSL is a software library for implementing the Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols. The Relationship Between OpenSSL and IP AddressMany web servers and services use OpenSSL to secure communication over IP addresses. When you access a website via its IP address (e.g., an HTTPS website), OpenSSL may be used to encrypt and secure the data exchange.SSL/TLS certificates, managed and validated by OpenSSL, are tied to specific domain names and IP addresses. When a client connects to a server using an IP address, the server’s SSL/TLS certificate must match that IP address. What’s Wrong with the GithubAt the same time, Github uses OpenSSL to secure communication over IP addresses. But what the most import thing is that GitHub, like many other online services, may periodically change its IP addresses for various reasons, including security, network optimization, and load balancing.In this case, if you use the previous IP address to access GitHub, you may encounter the above error. How to Solve the ProblemI. Check your Ip address of Github domain name in hosts file 123140.82.112.4 github.com199.232.69.194 github.global.ssl.fastly.net140.82.114.9 codeload.Github.com We can utilize this website https://www.ipaddress.com/ip-lookup to check whether the IP address corresponds with its domain name. II. Refresh DNS using PowerShell tools on your computer 1ipconfig /flushdns III. Deactivate SSL authenticationIf the above steps don’ work, we have to deactivate SSL authentication which is not advised sometimes. 1git config --global http.sslVerify &quot;false&quot;","link":"/blog/OpenSSLConnectionWithGithub/"},{"title":"Genetic Algorithm","text":"算法简介遗传算法（Genetic Algorithm，GA）是一种基于自然选择和遗传操作的随机全局搜索优化算法。它通过模拟自然选择和遗传中发生的复制、交叉(crossover)和变异(mutation)等现象，从任一初始种群（父代）开始，通过随机选择、交叉和变异操作，产生更具有生存优势的子代，使群体不断向搜索空间最优的方向进化，最后收敛到一群最适应环境的个体，从而求得问题的最佳解。 达尔文进化论保留了种群的个体性状，而遗传算法则保留了针对给定问题的候选解集合(即individuals)。这些候选解经过迭代评估 (evaluate)，生成子代解。更优的解有更大的机会被选择，并将其特征传递给下一代候选解集合。 相关概念染色体(Chromosome)/位串(Bit String)个体的表示形式, 对应于遗传学中的染色体. 一条染色体表示为一个二进制串，其中每个位代表一个基因，表征染色体上是否存在该基因。 基因(Gene)基因是染色体中的元素，用于表示个体的特征，用 0 和 1 表示其是否存在于一条染色体上。 特征值(Eigenvalue)在用串表示整数时，基因的特征值与二进制数的权一致，即二进制串的位权一致。 基因型(Genotype) 进化理论：通过基因型表征繁殖和突变，基因型是组成染色体的一组基因的集合。 遗传算法：每个个体都由代表基因集合的染色体构成, 对应于位串(个体均为单染色体型) 表现型(Phenotype)生物体的基因型在特定环境下的表现特征, 对应于GA中的位串解码后的参数. 适应度(Fitness)各个个体对环境的适应程度叫做适应度(fitness)。在算法的每次迭代中，会使用适应度函数/目标函数对个体进行度量评估，得到其适应度值。 种群 (Population)遗传算法保持大量的个体 (individuals) —— 针对当前问题的候选解集合。由于每个个体都由染色体表示，因此这些种族的个体 (individuals) 可以看作是染色体集合： 遗传算子(Genetic Operators) 选择(Selection): 选择操作从种群中概率选择适应度值最高的个体作为父代，生成子代 交叉(Crossover): 交叉操作将父代的两条染色体进行交叉，生成子代随机地将选择的双亲样本的部分染色体互换(交叉)，以生成后代的两个新染色体，也称为基因重组 变异(Mutation): 变异操作将父代的一条染色体进行变异，生成子代。突变操作的目的是定期随机更新种群，将新模式引入染色体以便探索求解空间的未知区域，避免陷入局部最优。 算法原理染色体编码 编码：原问题的解到基因型的映射，即将问题的可行解从其解空间转换到遗传算法的搜索空间 二进制数编码方案：染色体上的基因序列是由二进制表示的若参数 $U\\in [U_1, U_2]$, 表示为长度 $k$ 的位串, 产生 $2^k$ 个不同基因型$000\\cdots 000=0\\rightarrow U_1$ $000\\cdots 001=1\\rightarrow U_1+\\delta$ $000\\cdots 010=2\\rightarrow U_1+2\\delta$ $\\vdots$ $111\\cdots 111=2^k-1\\rightarrow U_2$ 其中，$\\delta = \\frac{U_2-U_1}{2^k-1}$ 解码：将染色体上的基因序列转换为原问题的可行解$$U = U_1 + (\\sum^k_{i=1}b_i\\cdot2^{i-1})\\cdot\\frac{U_2-U_1}{2^k-1}$$其中，$(\\sum^k_{i=1}b_i\\cdot2^{i-1})$表示将基因位串按权展开，进一步将其转换为原可行解 初始种群在遗传算法中, 需要随机初始化一个待进化种群 $P_0$, 并配置参数: 最大进化代数$T$，群体大小 $M$, 交叉概率 $P_c$, 变异概率 $P_m$. 适应度尺度变换在算法迭代过程中，利用适应度函数计算出每个个体的适应度值，但是由于其相对于原问题的目标函数可能存在群体间适应度相当而造成的竞争减弱，导致种群收敛于局部最优解。 因此，需要对适应度值进行尺度变换，以增强种群间的竞争能力，常用的经典方法有: 线性尺度变换、乘幂尺度变换以及指数尺度变换 线性尺度变换$$F’ = aF+b$$其中，$a$为缩放系数，$b$为平移系数，$F$为变换前适应度值，$F’$为变换后适应度值。 乘幂尺度变换$$F’ = F^k$$其中, $k$ 为幂次，$F$为变换前适应度值，$F’$为变换后适应度值。 指数尺度变换$$F’ = e^{-\\beta F}$$其中，$\\beta$ 的大小决定了适应度尺度变换的强弱. 选择操作选择操作从旧群体中以一定概率选择优良个体组成新的种群，以繁殖得到下一代个体。个体被选中的概率跟适应度值有关，个体适应度值越高，被选中的概率越大。$$P_i = \\frac{F_i}{\\sum^M_{j=1}F_j}$$ 交叉操作交叉操作是指从种群中随机选择两个个体，依概率对两个染色体进行交换组合，把父串基因序列遗传给子串，从而产生新的个体。 单点交叉算子: 该算子在配对的染色体中随机的选择一个交叉位置，然后在该交叉位置对配对的染色体进行基因位变换 变异操作为了防止遗传算法在优化过程中陷入局部最优解，在搜索过程中需要对个体进行变异，以探索新的解空间。 单点变异算子：对基因序列中某一个位进行变异，随机变异为进制中其他一位 终止条件 算法已迭代到最大代数，主要用于限制运行时间和计算资源 种群个体没有明显的改进，当代种群最佳适应度值与父代种群最佳适应度值相比，其差异小于某个阈值，则算法可以停止 应用实践Configure Running Environment12# prepare corresponding environmentimport numpy as np Parameters for Genetic Algorithm Setting the crossover probability to 1 ensures adequate evolution of the population In general, variation is less likely to occur, so a variation rate of 0.005 is set 12345678910# Set parameters for Genetic AlgorithmGENE_SIZE = 48 # Gene lengthPOP_SIZE = 200 # Population sizeCROSSOVER_RATE = 1 # Crossover rateMUTATION_RATE = 0.005 # Mutation rateN_GENERATIONS = 50 # Maximum generations# Set parameters for optimization problemsX_BOUND = [-3, 3]Y_BOUND = [-3, 3] Objective Function$$F(x, y) = 3\\times (1-x)^2\\times e^{-[x^2+(y+1)^2]}-10\\times (\\frac{x}{5}-x^3-y^5)\\times e^{-x^2-y^2}-\\frac{1}{3^{e^{-(x+1)^2-y^2}}}$$ 123# define object functiondef F(x, y): return 3*(1-x)**2*np.exp(-(x**2)-(y+1)**2)- 10*(x/5 - x**3 - y**5)*np.exp(-x**2-y**2)- 1/3**np.exp(-(x+1)**2 - y**2) Coding Strategy Parameters Demonstrate pop represents the population matrix: A row represents a binary code represents DNA The number of rows of the matrix is the number of populations Odd columns represent X Even columns represent Y 123456789def TranslateDNA(pop): x_pop = pop[:,1::2] y_pop = pop[:,::2] Gene_Size = GENE_SIZE / 2 # pop:(POP_SIZE, GENE_SIZE)*(GENE_SIZE,1) --&gt; (POP_SIZE,1) x = x_pop.dot(2**np.arange(Gene_Size)[::-1])/float(2**Gene_Size-1)*(X_BOUND[1]-X_BOUND[0])+X_BOUND[0] y = y_pop.dot(2**np.arange(Gene_Size)[::-1])/float(2**Gene_Size-1)*(Y_BOUND[1]-Y_BOUND[0])+Y_BOUND[0] return x, y Crossover and Mutation1234567891011121314151617181920212223242526272829def Mutation(child, Mutation_Rate=0.003): # Mutation with Mutation_Rate probability if np.random.rand() &lt; Mutation_Rate: # Randomly generate an mutation location mutate_point = np.random.randint(0, GENE_SIZE) # Inverts the binary bit of the mutation point child[mutate_point] = child[mutate_point] ^ 1def CrossoverMutation(pop, Crossover_Rate=0.8): new_pop = [] # Traverse each individual in the population, taking that individual as the father for father in pop: child = father # The child first gets all the genes of the father # Crossover occurs with a certain probability when producing offspring if np.random.rand() &lt; Crossover_Rate: # Another individual is selected in the population and that individual is taken as the mother mother = pop[np.random.randint(POP_SIZE)] # Randomly generate an intersection cross_points = np.random.randint(low=0, high=GENE_SIZE) # The child gets the mother's genes located behind the intersection child[cross_points:] = mother[cross_points:] # Each child has a certain chance of mutating Mutation(child, MUTATION_RATE) new_pop.append(child) return new_pop Calculate Fitness of New Population Subtracting the minimum fitness is to prevent negative fitness In this way, wa can gurantee the range of fitness is [0, np.max(pred) - np.min(pred)] Add a small number to prevent the appearance of fitness to 0 1234def GetFitness(pop): x, y = TranslateDNA(pop) pred = F(x, y) # Calculate objective value return (pred - np.min(pred)) + 1e-3 Select New Population12345# nature selection according to pop's fitnessdef SelectPop(pop, fitness): idx = np.random.choice(np.arange(POP_SIZE), size=POP_SIZE, replace=True, p=(fitness) / (fitness.sum()) ) return pop[idx] Print Results12345678def Print_Info(pop): fitness = GetFitness(pop) max_fitness_index = np.argmax(fitness) print(&quot;Max_Fitness:&quot;, fitness[max_fitness_index]) x, y = TranslateDNA(pop) print(&quot;Optimal gene:&quot;, pop[max_fitness_index]) print(&quot;(x, y):&quot;, (x[max_fitness_index], y[max_fitness_index])) print(&quot;Optimal value:&quot;, F(x[max_fitness_index], y[max_fitness_index])) Run GA12345678910111213def GeneticAlgorithm(): # initial population pop = np.random.randint(2, size=(POP_SIZE, GENE_SIZE)) for _ in range(N_GENERATIONS): pop = np.array(CrossoverMutation(pop, CROSSOVER_RATE)) fitness = GetFitness(pop) pop = SelectPop(pop, fitness) Print_Info(pop)if __name__ == '__main__': GeneticAlgorithm() Running Results12345Max_Fitness: 0.08738358838357263Optimal gene: [1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0](x, y): (-0.028632940568503074, 1.499641925075169)Optimal value: 7.052501227315164 References 遗传算法(Genetic Algorithm, GA)详解与实现","link":"/blog/GeneticAlgorithm/"},{"title":"A Template of Literature Survey For Reading Papers","text":"IntroductionRecently, I’m reading some papers and I had searched for many blogs about paper reading. To my disappointment, most of them are not organized and the format is not consistent. So I decided to write a template of literature survey for reading papers.Of course, you’re very welcome to make some modifications to this template. Template With English VersionA Literature Survey About Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in ClutterI.Summary Overview… II.Research Interests… III.Problems Solved… IV.Previous Research… V.Author’s Innovation… VI.Author’s Contribution… VII.Algorithm FlowRecent Research… Existing Problems… Author’s Processing… Problem Formulation… Constructed Model… VIII.Remaining Problems… IX.Summary And ViewsSummary… Personal Views… Template With Chinese Version关于 Volumetric Grasping Network: Real-time 6 DOF Grasp Detection in Clutter 的文献阅读报告1.摘要概述… 2.研究的领域… 3.解决的问题… 4.之前的研究… 5.作者的创新… 6.作者的贡献… 7.算法的流程7.1 最近的研究… 7.2 存在的问题… 7.3 作者的处理… 7.4 问题的表述… 7.5 构建的模型… 8.遗留的问题… 9.总结与思考9.1 个人总结… 9.2 个人观点…","link":"/blog/PaperReadingTemplate/"},{"title":"Research Guidelines on Deep Reinforcement Learning","text":"I. IntroductionIt is a project that leads me to start academic research. What is the most important there is giving me many guidelines about how to find my own ideas. II. Guidelines About This ProgramStart My Research Project in GroupsFor beginners, there are some very useful suggestions about how to start a research project. Guideline The best method to do research in some aspect would be to aim for a super recent topic. Research Directions:We can just combine two things for example: Large Language Model and Deep Reinforcement Learning. Deep Reinforcement Learning $\\rightarrow$ Graph Presentation Learning Deep Reinforcement Learning $\\rightarrow$ Large Language Model Large Language Model $+$ Graph Presentation Learning Methods For Beginners Start with a paper with code; You can change the code and get something new and can also reproduce the results; You can do comparison; A paper may have something interesting that you can put or replace in another paper; Strategy: go quickly and rank importance according to novelty and code.After selecting the paper, next thing is to run the code and check if you can reproduce the results. Once you are at that point and then it is easy to make changes and discuss ideas.After all, if we want to do a very very good research, we need to put forward a simple cool idea on a paper already with code. Cooperation in GroupWe can go in parallel adding comments and ideas in an overleaf and work on a code. Time Allocation Arrange enough time for your goals with different improvement.","link":"/blog/ResearchGuideline/"},{"title":"A Template of Daily Log For Recording Your Project","text":"Daily Log 1 Author: Zhihao Li Date: November 2, 2023 Project: Robotics Stage I: Prepare the environmentRecords And Backups Configure Model Virtual Environment 1234pip install barbarpip install imgaug# based on CPU devicepip3 install torch torchvision torchaudio Problems And SolutionsProblemFailed to build mpi4py ERROR: Could not build wheels for mpi4py, which is required to install pyproject.toml-based projects Solutionconda install mpi4pyWhen I used pip to install the mpi4py, I encounted the above error. But I changed to conda and it workd very well. ProblemOSError: /lib64/libc.so.6: version GLIBC_2.18' not found when trying to import open3d Solutionpip3 install open3d Learning And Summary Learning Data augmentationData augmentation is a common data preprocessing technique used to expand limited datasets and improve the generalization ability of machine learning models. Data augmentation involves applying a series of transformations to the original data, generating new data samples, thereby increasing the dataset’s size, diversity, and complexity.Data augmentation techniques can be applied to various machine learning tasks such as image classification, object detection, natural language processing, and more. Common data augmentation techniques include: Random cropping: Randomly cropping sub-images of different sizes from the original image. Random flipping: Randomly flipping the image horizontally or vertically. Random rotation: Randomly rotating the image by a certain angle. Random scaling: Randomly changing the size of the image. Noise injection: Adding random noise to the image. Color jittering: Randomly adjusting attributes such as brightness, contrast, and saturation. Summary Build Project Structure 1234567891011121314151617181920212223# Tiling.pyData{ Slides{ ACC_Cases{ Slide_1, Slide_2, ... }, ACC Cases Tiles Pass{}, ACC Cases Tiles Fail{}, ACC_Norm_Cases{ Slide_1, Slide_2, ... }, ACC Norm Cases Tiles Pass{}, ACC Norm Cases Tiles Fail{}, save_path{ blured.json } }}&quot;&quot;&quot;Note: Cases file stores all slides; Cases Tiles Pass file stores normal tiles of all sildes, Cases Tiles Fail file stores noise tiles of all sildes.&quot;&quot;&quot;","link":"/blog/ProjectLogTemplate/"},{"title":"Connect the Linux Server By Private Key in WinSCP Software","text":"Introduction of WinSCPWinSCP (Windows Secure Copy) is a free and open-source graphical SFTP (SSH File Transfer Protocol), SCP (Secure Copy Protocol), FTP (File Transfer Protocol), and WebDAV (Web Distributed Authoring and Versioning) client for the Windows operating system. It allows users to securely transfer files between a local computer and a remote computer. User Account GenerateSometimes, the linux server only uses public key to generate user account instead of IP address and password so we need to generate the private key first. Generate the Private KeyOpen the git terminal and run the following command: 1ssh-keygen -t rsa After running, we can get the id_rsa file in the .ssh directory and the id_rsa.pub file in the same directory. Add the Public Key to the ServerOnce got the public key, we can just use it to create a new user account on the server. The following command is used to add the public key to the server. 1ssh-copy-id -i ~/.ssh/id_rsa.pub zhli@cwfang.tpddns.cn WinSCP Configuration Firstly, we need to open the WinSCP and click the ‘Tools’ button to Run PuTTYgen: Then, click Load button to find previously generated private key file(id_rsa): There when we choose the key file, it’s necessary to switch target file type to All Files(*.*) for matching. After that, just wait for processing and finally we can Save private key in the format of .ppk: .ppk format is a need for WinSCP to obtain the local computer’s authorization to the linux server. Config the private key in WinSCP globallyIn Advanced module we can find the Authentication interface used for SSH connection. In this module, Private key fileis used to select the private key file ending with .ppk format. The generated .ppk file is just our private key file and it is used to match the public key file in the linux server. In this way, we can just use WinSCP to upload the file to the linux server. WinSCP ConnectionFinishing above steps, it’s just time to use WinSCP to connect to the linux server. In WinSCP, we can find the Session interface to set up the connection. In Session interface, we can set up the connection information, including the server domain(Host name), username, password, port.","link":"/blog/SSHPrivateKey/"},{"title":"The Basic Principles and Some Further Discussion of GANs","text":"Maximum Likelihood Estimation Given a data distribution $P_{data}(x)$ We have a distribution $P_G(x;\\theta)$ parameterized by $\\theta$ E.g. $P_G(x:\\theta)$ is a Gaussian Mixture Model, $\\theta$ are means and variances of the Gaussians We want to find $\\theta$ such that $P_G(x;\\theta)$ close to $P_{data}(x)$ Sample ${x^1, x^2,…,x^m}$ from $P_{data}(x)$ We can compute $P_G(x^i;\\theta)$ Likelihood of generating the samples$$L = \\prod_{i=1}^m P_G(x^i;\\theta)\\tag{1}$$ So we can just solve the optimal $\\theta$:$$\\theta^* = arg \\max_{\\theta}\\prod_{i=1}^m P_G(x^i;\\theta)\\tag{2}$$$$= arg \\max_{\\theta} \\log\\prod_{i=1}^m P_G(x^i;\\theta)\\tag{3}$$$$=arg \\max_{\\theta}\\sum_{i=1}^m \\log P_G(x^i;\\theta)\\tag{4}$$$$\\approx \\max_{\\theta} E_{x\\sim P_{data}}[\\log P_G(x;\\theta)]\\tag{5}$$$$=arg \\max_{\\theta}\\int_x P_{data}(x)\\log P_G(x;\\theta)dx - \\int_x P_{data}(x)\\log P_{data}(x)dx\\tag{6}$$$$=arg \\min_{\\theta} KL(P_{data}(x)||P_G(x;\\theta))\\tag{7}$$ In the equation (2), $\\theta$ means the parameters of G(Generate) model and $x^i$ means the $i$-th sample from $P_{data}(x)$. So this equation expresses the probability of $P_G$ generating the samples from $P_{data}$. In the equation (3) and (4), we use the logarithm to make the equation easier to calculate and it doesn’t influence the optimal $\\theta$. Equation (5) is the approximate of summation results and it only has the difference with $\\frac{1}{m}$. Int te euqation (6), there is a extra term $\\int_x P_{data}(x)\\log P_{data}(x)dx$ which is the constant for $G$ network so it also doesn’t influence the optimal $\\theta$. Basic Idea of GANBut the Generator $G$ is hard to be learned by maximum likelihood. Min-max GANDefine a value function $V(G, D)$:$$G^* = arg \\min_G \\max_D V(G, D)\\tag{8}$$ Given G, the optimal $D^*$ maximizing:$$V = E_{x\\sim P_{data}}[\\log D(x)] + E_{x\\sim P_G(x)}[\\log (1-D(x))]\\tag{9}$$$$=\\int_xP_{data}(x)\\log D(x)dx + \\int_xP_G(x)\\log (1-D(x))dx\\tag{10}$$$$=\\int_x[P_{data}(x)\\log D(x) + P_G(x)\\log (1-D(x))]dx\\tag{11}$$ Given $D$, the optimal $G^*$ minimizing:$$P_{data}(x)\\log D(x) + P_G(x)\\log (1-D(x))\\tag{12}$$ Optimal $D^*$If we maximum each $P_{data}(x)\\log D(x) + P_G(x)\\log (1-D(x))$ for any input $x$, there will be the optimal $D$. Given $x$, the optimal $D^*$ maximizing:$$P_{data}(x)\\log D(x) + P_G(x)\\log (1-D(x))\\tag{13}$$There we can view $P_{data}(x)$ as a constant value of $a$, and $P_G(x)$ as a constant value of $b$.$D^*$ maximizing: $f(D) = a\\log D + b\\log (1-D)$. Solving it, $D^* = \\frac{1}{a+b}$, that is$$D^* = \\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}\\tag{14}$$ Conduct Result$$V(G, D^*) = E_{x\\sim P_{data}}[\\log \\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}] +E_{x\\sim P_G(x)}[\\log \\frac{P_{G}(x)}{P_{data}(x)+P_G(x)}]\\tag{15}$$$$=\\int_x P_{data}(x)\\log \\frac{P_{data}(x)}{P_{data}(x)+P_G(x)}dx + \\int_x P_G(x)\\log \\frac{P_{G}(x)}{P_{data}(x)+P_G(x)}dx\\tag{16}$$$$=\\int_x P_{data}(x)\\log \\frac{\\frac{P_{data}(x)}{2}}{\\frac{P_{data}(x)+P_G(x)}{2}}dx + \\int_x P_G(x)\\log \\frac{\\frac{P_{G}(x)}{2}}{\\frac{P_{data}(x)+P_G(x)}{2}}dx\\tag{17}$$$$=-2\\log2+\\int_x P_{data}(x)\\log \\frac{P_{data}(x)}{\\frac{P_{data}(x)+P_G(x)}{2}}dx + \\int_x P_G(x)\\log \\frac{P_{G}(x)}{\\frac{P_{data}(x)+P_G(x)}{2}}dx\\tag{18}$$$$=-2\\log2+KL(P_{data}(x)||\\frac{P_{data}(x)+P_G(x)}{2}) + KL(P_G(x)||\\frac{P_{data}(x)+P_G(x)}{2})\\tag{19}$$$$=-2\\log2+2JSD(P_{data}(x)||P_G(x))\\tag{20}$$ Optimal $G^*$$$G^* = arg \\min_G \\max_D V(G, D)$$When got the optimal $D^*$,$$\\max_D V(G, D) = -2\\log 2+2JSD(P_{data}(x)||P_G(x))\\tag{21}$$So to minimize above equation, we just get the optimal G:$$P_G(x) = P_{data}(x)\\tag{22}$$ Further DiscussionThe adversarial idea is very innovative which seems there two bodies, one for making decisions and the other for checking the quality, will enhance each other alternately. In some robust research domain, this idea will provide a new way to improve, like learning with noisy labels. Reference Generative Adversarial Network","link":"/knowledge/GANs/"},{"title":"Markov Decision Process Model Based on Value Iteration","text":"TheoriesMarkov Decision ProcessGenerally, we notes a MDP model as $(S, A, T_a, R_a, \\gamma)$. Its transition function is $T_a(s,s’)=\\Pr(s_{t+1}|s_t=s, a_t=a)$, reward function is $R_a(s,s’)$. And actions choosing satisfies a specific distribution.The cotinuous decisions are noted as trace $\\tau$, formally in formula: $\\tau=${$s_t, a_t, r_t, s_{t+1}, \\cdots, a_{t+n}, r_{t+n}, s_{t+n+1}$} And in many situations, we very care about the expected reward of a specific trace because that will support us to choose the optimal action currently. So we use the method like weighted time series to calculate cumulative reward: $$R(\\tau_t) = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots=r_t+\\sum_{i=1}^\\infty \\gamma^ir_{t+i}$$ After we got the return value of traces, we can just calculate the value of a state to form our policy. $$V^{\\pi}(s)=E_{\\tau\\sim p(\\tau_t)}[\\sum_{i=0}^\\infty \\gamma^ir_{t+i}|s_t=s]$$ However, although we can get the value function to form optimal policy, we cann’t still calculate the values of all states. So we need Bellmax Equation to solve the problem. Bellman Equation$$V^{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)[\\sum_{s’\\in S}T_a(s,s’)[R_a(s,s’)+\\gamma V^{\\pi}(s’)]$$ For a specific state $s$, when choosing some action, we will get a stochastic new state which satisfies some distribution. Bellman Equation tells us to calculate the expected average value of these possible new states’ return. And in detail, the return of each state have two parts: the immediate reward $R_a(s,s’)$ and the future reward $\\gamma V^{\\pi}(s’)$. That inspires us that we can calculate the value of states recursively. Value IterationValue Iteration is a method to calculate Bellman Equation by traversing the state and action space. Firstly, it stores a value table of all states. And in traversing process, it will calculate the value of each state and update the value table by choosing the action with the highest return. ExperimentsTaxi Environment of OpenAI Gym Taxi EnviromentThe Taxi example is an environment where taxis move up, down, left, and right, and pichup and dropoff passengers. There are four disignated locations in the Grid world indicated by R(ed), B(lue), G(reen), and Y(ellow). Taxi ActivitiesIn an episode, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger’s location, picks up the passenger, then drives to the passenger’s destination(another one of the four specified locations), and drops off the passenger. States and Actions Space $500=25\\times5\\times4$ discrete statesWith the grid size of $5 \\times 5$, there are $25$ taxi positions. For the passenger, there are $5$ possible locations(including the case when the passenger is in the taxi). For the destination, there are $4$ possible locations. $6$ discrete deterministic actionsFor the Taxi diver, $0$: Move south $1$: Move north $2$: Move east $3$: Move west $4$: Pick up passenger $5$: Drop off passenger Rewards $-1$ for each action $+20$ for delivering the passenger $-10$ for picking up and dropping off the passenger illegally The following pictures are taxi example demostration. The left shows taxi actions with a random policy and the right shows taxi actions with the optimal policy. ResultsNow we want to check how the discount factor influences the value function from the same start state. So we choosing the discount factor ranging from $0.0$ to $1.0$ with footstep of 0.05 to measure the average rewards and cumulative rewards on random group and optimal group. Discount Factor Random Cum_Reward Random_Aver_Reward Optimal Cum_Reward Optimal_Aver_Reward 0.00 -37 -3.70 -20 -2.00 0.05 -10 -1.00 -20 -1.00 0.10 -55 -5.50 10 0.91 0.15 -37 -3.70 11 1.10 0.20 -55 -5.50 -20 -1.00 0.25 -28 -2.80 15 2.50 0.30 -46 -4.60 11 1.10 0.35 -28 -2.80 5 0.31 0.40 -10 -1.00 7 0.50 0.45 -37 -3.70 7 0.50 0.50 -64 -6.40 7 0.50 0.55 -19 -1.90 13 1.60 0.60 -28 -2.80 9 0.75 0.65 -46 -4.60 10 0.91 0.70 -37 -3.70 9 0.75 0.75 -46 -4.60 6 0.40 0.80 -37 -3.70 4 0.24 0.85 -37 -3.70 7 0.50 0.90 -28 -2.80 7 0.50 0.95 -37 -3.70 5 0.31 1.00 -37 -3.70 11 1.10 ConclusionsFrom the following experimental results, we can conclude that the discount factor has a significant impact on the value function. The optimal group has a higher average and cumulative reward than the random group, and the discount factor has a lower bound $\\gamma=0.4$ to get optimal policy.In my opinion, the discount factor reflects the future reward’s influence on the current state. If it is set too small, that means the most reward comes from the immediate reward which is a greedy policy with the possibility of failure. On the other hand, if set too high, we also cann’t get the best action with the highest reward. So we’d better to set the discount factor to an appropriate value. Codes12345678910111213141516from argparse import ArgumentParserclass BaseOptions: def __init__(self): self.parser = ArgumentParser() self.parser.add_argument('--algorithm', type=str, default='ValueItration') self.parser.add_argument('--n_rounds', type=int, default=500, help='Number of rounds') self.parser.add_argument('--ub_gamma', type=float, default=1, help='upper bound of discount factor') self.parser.add_argument('--lb_gamma', type=float, default=0, help='lower bound of discount factor') self.parser.add_argument('--NA', type=int, default=6, help='Length of Actions Space') self.parser.add_argument('--NS', type=int, default=500, help='Length of States Space') self.parser.add_argument('--end_delta', type=float, default=0.00001, help='end delta') self.parser.add_argument('--print_interval', type=int, default=50, help='print interval') def parse(self): return self.parser.parse_args() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&quot;&quot;&quot;-------------------------------------------------------Project: Solving as MDP using Value Iteration AlgorithmAuthor: Zhihao LiDate: October 19, 2023Research Content: Deep Reinforcement Learning-------------------------------------------------------&quot;&quot;&quot;from options import BaseOptionsfrom value_iteration import ValueMDPimport gym # openAi gymimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsfrom matplotlib.font_manager import FontPropertiesimport warningswarnings.filterwarnings('ignore')# Set up Seaborn stylesns.set(style=&quot;darkgrid&quot;)Efont_prop = FontProperties(fname=&quot;C:\\Windows\\Fonts\\ARLRDBD.TTF&quot;)label_prop = FontProperties(family='serif', size=7, weight='normal')legend_font = FontProperties(family='serif', size=7, weight='normal')if __name__ == '__main__': opts = BaseOptions().parse() # set project's options # Set OpenAI Gym environment env = gym.make('Taxi-v3', render_mode=&quot;rgb_array&quot;) gamma_delta = 0.01 aver_rewards = np.zeros(len(np.arange(opts.lb_gamma, opts.ub_gamma + gamma_delta, gamma_delta))) random_aver_rewards = np.zeros(aver_rewards.shape) cum_rewards = np.zeros(aver_rewards.shape) random_cum_rewards = np.zeros(aver_rewards.shape) for t, gamma in enumerate(np.arange(opts.lb_gamma, opts.ub_gamma + gamma_delta, gamma_delta)): # Init env and value iteration process VIMDP = ValueMDP(env, opts, gamma) # Apply the random policy VIMDP.env.reset(seed=t+101) VIMDP.ApplyRandomPolicy(steps=10) # Value Iteration in MDP observation = VIMDP.env.reset(seed=t+101) VIMDP.IterateValueFunction() # Apply the optimal policy VIMDP.ApplyOptimalPolicy(observation[0], steps=20) # Save reward results aver_rewards[t] = VIMDP.aver_reward random_aver_rewards[t] = VIMDP.random_aver_reward cum_rewards[t] = VIMDP.cum_reward random_cum_rewards[t] = VIMDP.random_cum_reward print(&quot;discount factor: %f&quot; % gamma) print(&quot;Applying the random policy, accumulated reward: %.5f, average reward: %.5f&quot; % (random_cum_rewards[t], random_aver_rewards[t])) print(&quot;Applying the optimal policy, accumulated reward: %.5f, average reward: %.5f&quot; % (cum_rewards[t], aver_rewards[t])) # plot the rewards xdata = np.arange(opts.lb_gamma, opts.ub_gamma + gamma_delta, gamma_delta) plt.subplot(211) plt.plot(xdata, random_aver_rewards, 'b-', label='random policy') plt.plot(xdata, aver_rewards, 'g-', label='optimal policy') plt.ylabel('Average Rewards', fontproperties=Efont_prop, fontsize=9) plt.yticks(fontproperties=label_prop, fontsize=7) plt.xticks(fontproperties=label_prop, fontsize=7) plt.legend(loc='lower right', fontsize=7, prop=legend_font) plt.subplot(212) plt.plot(xdata, random_cum_rewards, 'b--', label='random policy') plt.plot(xdata, cum_rewards, 'g--', label='optimal policy') plt.xlabel('Discount Factor', fontproperties=Efont_prop, fontsize=9) plt.ylabel('Cumulative Rewards', fontproperties=Efont_prop, fontsize=9) plt.yticks(fontproperties=label_prop, fontsize=7) plt.xticks(fontproperties=label_prop, fontsize=7) plt.legend(loc='lower right', fontsize=7, prop=legend_font) plt.savefig(&quot;Rewards.png&quot;, dpi=400) env.close() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import numpy as np&quot;&quot;&quot;--------------------------------------------------------------------------------------This section is for Value Iteration Algorithm for Taxi Gym.Author: Zhihao LiDate: October 19, 2023Arguments: env: OpenAI env. env.P represents the transition probabilities of the environment. env.P[s][a] is a list of transition tuples (prob, next_state, reward, done). end_delta: Stop evaluation once value function change is less than end_delta for all states. discount_factor: Gamma discount factor.--------------------------------------------------------------------------------------&quot;&quot;&quot;class ValueMDP: def __init__(self, env, opts, gamma) -&gt; None: self.env = env # taxi gym environment self.gamma = gamma # discount_factor self.NA = opts.NA # Actions Space's Length self.NS = opts.NS # States Space's Length self.V = np.zeros(self.NS) # Value Function self.end_delta = opts.end_delta # Delta value for stopping iteration self.new_policy = np.zeros(self.NS) # the optimal policy self.cum_reward = 0 # apply new policy and get all rewards self.aver_reward = 0 self.random_cum_reward = 0 # rewards applying random actions self.random_aver_reward = 0 def SingleStepIteration(self, state): &quot;&quot;&quot; Function: calculate the state value for all actions in a given state and update the value function. Returns: The estimate of actions. &quot;&quot;&quot; action_V = np.zeros(self.NA) # Record the value of each action for action in range(self.NA): for prob, nextState, reward, is_final in self.env.P[state][action]: action_V[action] += prob * (reward + self.gamma * self.V[nextState] * (not is_final)) return action_V def IterateValueFunction(self): while True: delta = 0 # initialize the every round of delta for s in range(self.NS): newValue = np.max(self.SingleStepIteration(s)) delta = max(delta, np.abs(newValue - self.V[s])) self.V[s] = newValue # updates value function if delta &lt; self.end_delta: # the maximum delta of all states break # get optimal policy for s in range(self.NS): # for all states, create deterministic policy newAction = np.argmax(self.SingleStepIteration(s)) self.new_policy[s] = newAction def ApplyOptimalPolicy(self, observation, steps): for i in range(steps): action = self.new_policy[observation] observation, reward, is_final, truncated, info = self.env.step(np.int8(action)) self.cum_reward += reward # self.env.render() if is_final: break self.aver_reward = self.cum_reward / (i + 1) def ApplyRandomPolicy(self, steps): for i in range(steps): observation, reward, is_final, truncated, info = self.env.step(self.env.action_space.sample()) self.random_cum_reward += reward # self.env.render() if is_final: break self.random_aver_reward = self.random_cum_reward / (i+1) Contributors Zhihao Li References OpenAI Gym","link":"/projects/ValueIterationMDP/"},{"title":"My Knowledge about Paper Writing","text":"Seven Simple, Actionable Suggestions For Making Papers BetterDon’t Wait to WriteWriting papers model: Your Idea $\\rightarrow$ Do Research $\\rightarrow$ Write Paper[Not Recommended] Your Idea $\\rightarrow$ Write Paper $\\rightarrow$ Do Research [Recommended] Recommend Reasons: Forces us to be clear, focused Crystallites what we don’t understand Opens the way to dialogue with others: reality check, critique, and collaboration Identify Your Key IdeaUseful Re-usable Idea You want to infect the mind of your reader with your idea, like a virus. Papers are far more durable than programs(think Mozert) Do Not be Intimidated Fallacy: You need to have a fantastic idea before you can write a paper. Idea: Your paper should have just one “ping”: one clear, sharp idea. You may not know exactly what the ping is when you start writing; but you must know when you finish. If you have lots of ideas, write lots of papers. Make certain that the reader is in no doubt what the idea is.(Be 100% explicit) “The main idea of this paper is…” “In this section we present the main contributions of the paper.” Tell A StoryYour Narrative FlowImagine you are explaining ar a whiteboard: Here is a problem It’s an interesting problem It’s an unsolved problem Here is my idea My idea works(details, data) Here’s how my idea compares to other people’s approaches Structure(Conference Paper) Title(1000 readers) Abstract(4 sentences, 100 readers) Introduction(1 page, 100 readers) The problem(1 page, 10 readers) My idea(2 page, 10 readers) The details(5 pages, 3 readers) Related work(1-2 pages, 10 readers) Conclusions and further work(0.5 pages) Nail Your Contributions to the MastDescribe the Problem Use an example to introduce the problem State Your Contributions Write the list of contributions first: Bulleted list of contributions The list of contributions drives the entire paper: the paper substantiates the claims you have made. Readers thinks “gosh, if they can really deliver this, that’s be exciting; I’d better read on” Evidence Your introduction makes claims The body of the paper provides evidence to support each claim Check each claim in the introduction, identify the evidence, and forward-reference it from the claim “Evidence” can be: analysis and comparison, theorems, measurements, case studies Related Work: LaterFallacy: To make my work look good, I have to make other people’s work look bad.Giving credit to others does not diminish the credit you get from your paper: Warmly acknowledge people who have helped you Be generous to the competition. “In his inspiring paper [Foo98] Foogle shows… We develop his foundation in the following ways…” Acknowledge weaknesses in your approach Failing to give credit to others can kill your paper Put Your Readers FirstPresenting the Idea Explain it as if you were speaking to someone using a whiteboard Conveying the intuition is primary, not secondary Once your reader has the intuition, she can follow the details (but not vice versa) Even if she skips the details, she still takes away something valuable Conveying the IntuitionIntroduce the problem, and your idea, using examples and only then present the general case. Do not recapitulate your personal journey of discovery. This route may be soaked with your blood, but that is not interesting to the reader. Instead, choose the most direct route to the idea. Listen to Your ReadersGetting Help Get your paper read by as many friendly guinea pigs as possible Each reader can only read your paper for the first time once! So use them carefully! Explain carefully what you want (“I got lost here” is much more important than “Jarva is mis-spelt”.) Getting Expert Help A good plan: when you think you are done, send the draft to the competition saying “could you help me ensure that I describe your work fairly?”. Often they will respond with helpful critique (they are interested in the area) They are likely to be your referees anyway, so getting their comments or criticism up front is Jolly Good Listening to Your ReviewersTreat every review like gold dust Be (truly) grateful for criticism as well as praise. Read every criticism as a positive suggestion for something you could explain more clearly DO NOT respond “you stupid person, I meant X”. Fix the paper so that X is apparent even to the stupidest reader. Thank them warmly. They have given up their time for you. References How to write a great research paper Seven simple suggestions","link":"/blog/WritingSkills/"},{"title":"A Lightweight Designed Beamer Template of Weekly Survey","text":"IntroductionNowadays, I’m working on a weekly report for my research group. Finding a concise and academic slides template is a need for us to represent our finds and ideas. Based on the PKU_beamer_lightweight_designed, I adjust some details according to my preference and share the tutorial in this post. Adjusted DetailsThis a lightweight designed beamer template of weekly survey with the version of Xidian University, which is useful for reporting your research work academically and concisely. Theme ColorI use the official red color of Xidian University as the theme color. The hex code of the color is #B0252A. Main FontI prefer the fontstyle of Times New Roman so I need to additionally include fontspec package.12\\usepackage{fontspec}\\setsansfont{Times New Roman} Remove Specific Frame From the HealineIn beamer, the default frames will be counted in the headline which is sometimes not suitable. Such as the title page, overline page and so on are usually independent pages. To remove them from the headline, I use the following code. configuration123456789101112\\makeatletter\\let\\beamer@writeslidentry@miniframeson=\\beamer@writeslidentry%\\def\\beamer@writeslidentry@miniframesoff{% \\expandafter\\beamer@ifempty\\expandafter{\\beamer@framestartpage}{}% does not happen normally {%else % removed \\addtocontents commands \\clearpage\\beamer@notesactions% }}\\newcommand*{\\miniframeson}{\\let\\beamer@writeslidentry=\\beamer@writeslidentry@miniframeson}\\newcommand*{\\miniframesoff}{\\let\\beamer@writeslidentry=\\beamer@writeslidentry@miniframesoff}\\makeatother Before those frames, we just turn off the frames style but if we next to count frame we also need to open this by the command of \\miniframeson. And the section*{} ensures the frame removes the number in some section. usage1234567\\miniframesoff\\begin{frame} \\section*{} \\begin{center} {\\Huge \\textit{Thanks for you3fr listening!}} \\end{center}\\end{frame} XidianU.sty CodesXidianU.sty123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133\\mode&lt;presentation&gt;\\newif\\ifbeamer@secheader\\beamer@secheaderfalse\\ProcessOptionsBeamer\\useoutertheme[subsection=false]{smoothbars}\\makeatletter\\newcommand{\\frameofframes}{/}\\newcommand{\\setframeofframes}[1]{\\renewcommand{\\frameofframes}{#1}}\\setbeamertemplate{footline} {% \\begin{beamercolorbox}[colsep=1.5pt]{upper separation line foot} \\end{beamercolorbox} \\begin{beamercolorbox}[ht=2.5ex,dp=1ex,% leftskip=.3cm,rightskip=.3cm plus1fil]{author in head/foot}% {\\usebeamerfont{author in head/foot}\\insertshortauthor}% \\hfill% {\\usebeamerfont{title in head/foot}\\insertshorttitle}% \\hfill% {\\usebeamerfont{frame number}\\usebeamercolor[fg]{frame number}\\insertframenumber~\\frameofframes~\\inserttotalframenumber} \\end{beamercolorbox}% \\begin{beamercolorbox}[colsep=1.5pt]{lower separation line foot} \\end{beamercolorbox} }\\makeatother\\useinnertheme{circles}%\\useinnertheme{rectangles}%\\useoutertheme{default}%\\useinnertheme[shadow=true]{rounded}\\definecolor{xidian}{HTML}{B0252A}% \\xdefinecolor{xidian}{cmyk}{0,1,1,0.45}%{rgb}{0.543,0.0,0.0703} %{cmyk}{0,100,100,45}%{rgb}{0.5,0.0,0.0} %RGB#820010\\xdefinecolor{xidian_gold}{cmyk}{0,0.35,0.75,0.05}\\xdefinecolor{xidian_blue}{cmyk}{0.6,0.35,0.0,0.4}\\xdefinecolor{xidian_darkblue}{cmyk}{1.0,0.6,0.0,0.5}\\xdefinecolor{xidian_gray}{cmyk}{0.0,0.0,0.08,0.55}\\xdefinecolor{xidian_dirt}{cmyk}{0.0,0.2,0.35,0.3}\\xdefinecolor{xidian_orange}{cmyk}{0.0,0.7,1.0,0.0}\\xdefinecolor{xidian_green}{cmyk}{0.2,0.0,1.0,0.15}\\xdefinecolor{xidian_darkgreen}{cmyk}{0.6,0.5,1.0,0.45}\\xdefinecolor{pantone_gold}{RGB}{135,103,79}\\xdefinecolor{pantone_silver}{RGB}{138,141,143}\\xdefinecolor{WM_Gold}{cmyk}{0.09,0.29,0.66,0.24}\\setbeamercolor{footline}{bg=xidian}%\\setbeamercolor{frametitle}{bg=white!70!pantone_gold,fg=xidian}\\setbeamercolor{frametitle}{bg=white,fg=xidian}\\setbeamercolor{title}{bg=xidian}%\\setbeamerfont{frametitle}{size=\\large}\\setbeamerfont{frametitle}{series=\\bfseries,size=\\large}%,parent=structure}\\setbeamerfont{footline}{series=\\bfseries}\\setbeamertemplate{navigation symbols}{}\\setbeamertemplate{bibliography item}[text]\\setbeamertemplate{caption}[numbered]\\beamertemplateshadingbackground{white!5}{white}\\setbeamercolor{palette primary}{use=structure,fg=white,bg=structure.fg}\\setbeamercolor{palette secondary}{use=structure,fg=white,bg=structure.fg!95!black}%{use=structure,fg=white,bg=structure.fg!90!black}\\setbeamercolor{palette tertiary}{use=structure,fg=white,bg=structure.fg!90!black}\\setbeamercolor{palette quaternary}{fg=white,bg=structure.fg!85!black}%\\setbeamercolor*{sidebar}{use=structure,bg=structure.fg}\\setbeamercolor{titlelike}{parent=palette primary}%% try\\setbeamercolor{block title}{bg=xidian_blue,fg=white}\\setbeamercolor{block body}{bg=xidian_blue!10}\\BeforeBeginEnvironment{definition}{% \\setbeamercolor{block title}{bg=xidian_blue,fg=white} \\setbeamercolor{block body}{bg=xidian_blue!10}}\\AfterEndEnvironment{definition}{ \\setbeamercolor{block title}{bg=xidian_blue,fg=white} \\setbeamercolor{block body}{bg=xidian_blue!10}}\\BeforeBeginEnvironment{theorem}{% \\setbeamercolor{block title}{bg=xidian_orange,fg=white} \\setbeamercolor{block body}{bg=xidian_orange!10}}\\AfterEndEnvironment{theorem}{ \\setbeamercolor{block title}{bg=xidian_blue,fg=white} \\setbeamercolor{block body}{bg=xidian_blue!10}}\\BeforeBeginEnvironment{proposition}{% \\setbeamercolor{block title}{bg=xidian_orange,fg=white} \\setbeamercolor{block body}{bg=xidian_orange!10}}\\AfterEndEnvironment{proposition}{ \\setbeamercolor{block title}{bg=xidian_blue,fg=white} \\setbeamercolor{block body}{bg=xidian_blue!10}}\\setbeamercolor*{block title example}{use={normal text,example text},bg=white!70!pantone_gold,fg=xidian}\\setbeamercolor{fine separation line}{}\\setbeamercolor{item projected}{fg=white}\\setbeamercolor{palette sidebar primary}{use=normal text,fg=normal text.fg}\\setbeamercolor{palette sidebar quaternary}{use=structure,fg=structure.fg}\\setbeamercolor{palette sidebar secondary}{use=structure,fg=structure.fg}\\setbeamercolor{palette sidebar tertiary}{use=normal text,fg=normal text.fg}%\\setbeamercolor{palette sidebar quaternary}{fg=white}\\setbeamercolor{section in sidebar}{fg=brown}\\setbeamercolor{section in sidebar shaded}{fg=grey}\\setbeamercolor{separation line}{}\\setbeamercolor{sidebar}{bg=xidian}\\setbeamercolor{sidebar}{parent=palette primary}\\setbeamercolor{structure}{fg=xidian}\\setbeamercolor{subsection in sidebar}{fg=brown}\\setbeamercolor{subsection in sidebar shaded}{fg=grey}\\AtBeginSection[]{ \\begin{frame} \\tableofcontents[sectionstyle=show/shaded,subsectionstyle=hide,subsubsectionstyle=hide] \\end{frame}} \\setbeamercolor{postgreen}{fg=black,bg=example text.fg!75!black!10!bg}\\setbeamercolor{postred}{fg=black,bg=white!70!pantone_gold}\\setbeamercolor{postblue}{fg=black,bg=xidian_blue!10}%\\AtBeginSubsection[]{% \\begin{frame}% \\tableofcontents[sectionstyle=show/shaded,subsectionstyle=hide,subsubsectionstyle=hide]% \\end{frame}%}\\mode&lt;all&gt; main.texmain.tex123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190\\documentclass[10pt,hyperref={colorlinks,citecolor=blue,urlcolor=xidian_blue,linkcolor=}]{beamer}\\usepackage{XidianU}\\usepackage{fontspec}\\setsansfont{Times New Roman}\\usepackage{lipsum}%\\usepackage[scheme = plain]{ctex}\\usepackage{charter} % Nicer fonts% other packages\\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}\\usepackage{amssymb}\\usepackage{graphicx}\\usepackage{subcaption}\\usepackage{bm}\\usepackage{natbib}\\usepackage{wrapfig}\\usepackage{amsfonts} \\usepackage{ragged2e}\\usepackage{parskip}\\apptocmd{\\frame}{}{\\justifying}{} % Allow optional arguments after frame.\\newcommand{\\theHalgorithm}{\\arabic{algorithm}}\\theoremstyle{plain}\\newtheorem{axiom}{Axiom}\\newtheorem{claim}[axiom]{Claim}\\newtheorem{assumption}{Assumption}\\newtheorem{remark}{Remark}\\newtheorem{proposition}{Proposition}\\setbeamertemplate{theorems}[numbered]% change for your title page information\\author[Zhihao Li]{Zhihao Li}\\title{Research Survey 1}\\subtitle{Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?}\\institute{School of Computer Science and Technology\\\\Xidian University}\\date{February 1, 2024}% official colors match with the Xidian color\\def\\cmd#1{\\texttt{\\color{red}\\footnotesize $\\backslash$#1}}\\def\\env#1{\\texttt{\\color{blue}\\footnotesize #1}}\\definecolor{deepblue}{rgb}{0,0,0.5}\\definecolor{deepred}{rgb}{0.6,0,0}\\definecolor{deepgreen}{rgb}{0,0.5,0}\\definecolor{halfgray}{gray}{0.55}\\show\\hss\\makeatletter\\let\\beamer@writeslidentry@miniframeson=\\beamer@writeslidentry%\\def\\beamer@writeslidentry@miniframesoff{% \\expandafter\\beamer@ifempty\\expandafter{\\beamer@framestartpage}{}% does not happen normally {%else % removed \\addtocontents commands \\clearpage\\beamer@notesactions% }}\\newcommand*{\\miniframeson}{\\let\\beamer@writeslidentry=\\beamer@writeslidentry@miniframeson}\\newcommand*{\\miniframesoff}{\\let\\beamer@writeslidentry=\\beamer@writeslidentry@miniframesoff}\\makeatother\\begin{document}{\\begin{frame} \\titlepage \\begin{figure}[htpb] \\begin{center} \\includegraphics[width=0.2\\linewidth]{Figures/XDUlogo.jpg} \\end{center} \\end{figure}\\end{frame}}\\section{Summary}\\begin{frame}{Weekly Work}\\begin{enumerate} \\item Read the paper of \\textit{Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?}; \\item Learn about some concepts;\\end{enumerate}\\end{frame}\\begin{frame}{A prompt tuning process is highlyrobust to label noises.}\\begin{enumerate} \\item \\textbf{Interest}: Studying the key reasons contributing to the robustness of the prompt tuning.paradigm. \\item \\textbf{Findings}: \\begin{enumerate} \\item the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; \\item the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. \\end{enumerate}\\end{enumerate}\\end{frame}\\begin{frame}{Author's Contributions} \\begin{itemize} \\item We demonstrate that \\textbf{prompt tuning for pre-trained vision-language models (e.g., CLIP) is more robust to noisy labels} than traditional transfer learning approaches, such as model fine-tuning and linear probes. \\item We further demonstrate that \\textbf{prompt tuning robustness can be further enhanced through the use of a robust training objective.} \\item We conduct an extensive analysis on why prompt tuning is robust to noisy labels to \\textbf{discover which components contribute the most to its robustness.} \\item We \\textbf{propose a simple yet effective method for unsupervised prompt tuning}, showing that randomly selected noisy pseudo labels can be effectively used to enhance CLIP zero-shot performance. The proposed robust prompt tuning outperformed prior work on a variety of datasets, even though noisier pseudo-labels are used for self-training. \\end{itemize}\\end{frame}\\section{Motivations}\\begin{frame}{Mathematical Models} \\begin{itemize} \\item CLIP\\\\ In the case of image classification, a normalized image embedding $\\boldsymbol{f}^{\\:v}$ is obtained by passing an image through CLIP's visual encoder, and a set of normalized class embeddings $[\\boldsymbol{f_i^{\\:t}}]_{i=1}^K$ by feeding template prompts of the form &quot;A photo of a&quot; into CLIP's text encoder. \\begin{equation} Pr(y=i|\\boldsymbol{x})=\\frac{\\exp(sim(\\boldsymbol{f}^{\\:v},\\boldsymbol{f}^{\\:t}_i))/\\tau}{\\sum_{j=1}^K\\exp(sim(\\boldsymbol{f}^{\\:v},\\boldsymbol{f}^{\\:t}_j))/\\tau} \\end{equation} \\item Prompt Tuning\\\\ The name of a class c is first converted into a classname embedding $\\boldsymbol{w}\\in R^d$ and prepended with a sequence of $M$ learnable tokens $\\boldsymbol{p_m}\\in R^d$ shared across all classes. \\begin{equation} P_c=[\\boldsymbol{p_1}, \\boldsymbol{p_2}, \\cdots, \\boldsymbol{p_M}, \\boldsymbol{w_c}]\\rightarrow \\boldsymbol{f}^{\\:t}_c \\end{equation} CoOp optimizes the shared learnable tokens $\\boldsymbol{p_1}, \\boldsymbol{p_1}, \\cdots, \\boldsymbol{p_M}$ on a small labeled dataset $D = [(\\boldsymbol{x_i}, c_i)^N_{i=1}]$ to minimize the cross-entropy loss: \\begin{equation} L_{CE}=-E_{(\\boldsymbol{x},c)\\in D}[\\log Pr(y=c|\\boldsymbol{x})] \\end{equation} \\end{itemize}\\end{frame}\\begin{frame}{Mathematical Models} \\begin{itemize} \\item Robust Prompt Tuning\\\\ Further enhance this robustness by optimizing the learnable prompts using the generalized cross-entropy (GCE) loss: \\begin{equation} L_{GCE}=E_{(\\boldsymbol{x},c)\\in D}[\\frac{1-Pr(y=c|\\boldsymbol{x})^q}{q}] \\end{equation} \\item Author's Conclusion: $q = 0.7$ leads to overall good performance across several experimental settings. \\end{itemize}\\end{frame}\\section{Robustness Analysis}\\begin{frame}{Pre-trained CLIP Generates Effective Class Embeddings} \\vspace{-1em} \\begin{figure} \\includegraphics[width=0.9\\textwidth]{Figures/Survey1/models.png} \\label{fig: Models} \\end{figure}\\vspace{-0.7em} \\begin{itemize} \\item Classifier-R v.s. Classifier-C: CLIP class embeddings provide a strong initialization for few-shot learning.\\vspace{-0.5em} \\item TEnc-FT v.s. Classifier-C: The highly expressive CLIP text encoder can easily overfit to the noisy labels.\\vspace{-0.5em} \\item Prompt Tuning v.s. Classifiers: The text encoder is essential for providing a strong but informative regularization of the text embeddings to combat noisy inputs.\\vspace{-0.5em} \\item Prompt Tuning v.s. TEnc-FT: The text encoder should be fixed to prevent overfitting. \\end{itemize}\\end{frame}\\begin{frame}{Other Aspects of Robustness}\\begin{itemize} \\item \\textbf{Effectiveness of Prompt} \\item \\textbf{Prompt Tuning Suppresses Noisy Gradients} \\item \\textbf{Generalization Across Model Architectures} \\item \\textbf{Robustness to Correlated Label Noise}\\end{itemize}\\end{frame}\\section{Robust UPL}\\begin{frame}{Improve UPL in Unsupervised Prompt Tuning} \\vspace{-1em} \\begin{figure} \\includegraphics[width=\\textwidth]{Figures/Survey1/UPL.png} \\label{fig: UPL} \\end{figure}\\vspace{-0.8em}\\begin{itemize} \\item Baseline UPL\\begin{itemize} \\item Phase 1: Leverage pre-trained CLIP to generate pseudo labels for unlabeled images. \\item Phase 2: Select \\textbf{the $K$ most confident samples per class} to optimize the learnable tokens through the typical prompt-tuning optimization process (described in CoOp). \\end{itemize} \\item Robust UPL\\\\ Based on UPL, \\textbf{randomly sample $K$ training samples} and optimize the prompt with the \\textbf{robust GCE loss}.\\end{itemize}\\end{frame}\\section{Next Stage}\\begin{frame}{New Plans for Next Week}\\begin{enumerate} \\item Reproduce the most of results about this paper. \\item Survey other relavent methods in this domain.\\end{enumerate} \\end{frame}\\miniframesoff\\begin{frame} \\section*{} \\begin{center} {\\Huge \\textit{Thanks for you3fr listening!}} \\end{center}\\end{frame}\\end{document} Template Overview Contributors Zhihao Li References PKU_beamer_lightweight_designed","link":"/blog/WeeklyReport/"},{"title":"Knowledge Introduction","text":"科学研究定义 国家教育部对科学研究的定义：科学研究是指为了增进知识包括关于人类文化和社会的知识以及利用这些知识去发明新的技术而进行的系统的创造性工作; 美国资源委员会对科学研究的定义：科学研究工作是科学领域中的检索和应用，包括对已有知识的整理、统计以及对数据的搜集、编辑和分析研究工作; 《科研项目完全指南：从课题选择到报告撰写》：剑桥在线词典将“研究”定义为“对某一主题的详细探究，特别是为了发现（新）信息或达成（新）理解”; 研究包括三个步骤：提出问题；收集用以回答问题的数据；给出问题的答案。 IntroductionThese days I have been sentimental about the fact that I had learned so much knowledge but what’s the really helpful to my current learning or research is not so directly accessible. Until now, I have learned for at least fifteen years covering basic subjects from chinese, mathematics, english, physics, chemistry, biology, geography, history to politics and engineering subject of computer science. However, to be honest, I have forgotten a lot of them which is not so necessary or neccessay for my current learning. In other words, I cann’t rethink them immediately when I engage in the related work. In my views, the sense of immediate recall is very important for research innovation which allows us to know what kind of knowledge there exists and how to reform the existed knowledge in new environment. So I have decided to develop this habit of recording learned knowledge which has the potential in motivating my future learning or research. Knowledge ListMathematical Principles仿射变换(Affine Transformation)线性模型:$$price = w_{area}\\cdot area + w_{age} \\cdot age + b$$仿射变换的特点是通过加权和对特征进行线性变换，并通过偏置项进行平移。 非线性频率压缩在滤波器设计中将整个模拟频率轴压缩到 $\\pi/T$ 之间，使得 $H_a(s), s=j\\Omega$ 压缩为 $\\widehat{H_a}(s_1),s_1=j\\Omega_1$, 可以利用正切变换实现频率压缩模型：$$\\Omega = \\frac{2}{T}\\tan(\\frac{1}{2}\\Omega_1T)$$这个设计思想实质上利用了正切函数定义域有限、值域无限以及奇函数的性质；推而广之，这种设计可以实现特定的单值压缩方法，也可以实现值域的延展。 一些类似的函数特性，对数函数，指数函数分别适合于定义域、值域取值 $0 \\sim 1$ 之间的情况，但是对目标域都有所限制，因此这些函数往往没有正切函数具有优良的特性。 Research InnovationLearning With Noisy Labels Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels Innovation: DNN的相互指导学习机制，两个模型分别动态地选取一些干净样本相互提供给对方进行学习，目的是过滤不同类型的噪声； Learning Points: 直觉是同辈相互纠错的学习机制，而且训练中其样本选取的动态性值得一提； DivideMix: Learning with Noisy Labels as Semi-supervised Learning Innovation: 对噪声数据集进行划分，并同时学习两个模型进行相互指导、标签集成，以克服不同类型的噪声； Learning Points: Mutual Learning 和 交互学习一定程度上可以增强模型鲁棒性； Project HabitsPackage ManagementMiniconda Configuration1234567891011121314151617############ Conda Environment Installation ############# Fetch the miniconda scriptexport HOME=$PWDwget -q https://repo.anaconda.com/miniconda/Miniconda3-py37_4.12.0-Linux-x86_64.sh -O miniconda.shsh miniconda.sh -b -p $HOME/miniconda3rm miniconda.shexport PATH=$HOME/miniconda3/bin:$PATH# Initialize condasource $HOME/miniconda3/etc/profile.d/conda.shhash -rconda config --set always_yes yes --set changeps1 yes# Create new environmentconda create -n my_env python=3.8conda activate my_env","link":"/knowledge/Introduction/"},{"title":"Design of A Basic Computer Model With Stack Function","text":"IntroductionLast weekend I undertook a project to design a basic computer model from clock generator design to microinstruction encoding. And I preferably chose to design a basic model with common stack functions. My Report Contributors Zhihao Li","link":"/projects/ComputerModel/"},{"title":"2D Virtual Try-on Based on Deep Learning","text":"(function(){var player = new DPlayer({\"container\":document.getElementById(\"dplayer0\"),\"autoplay\":true,\"hotkey\":true,\"preload\":\"metadata\",\"video\":{\"url\":\"https://ms-blogimage.oss-cn-chengdu.aliyuncs.com/videos/2D%E8%99%9A%E6%8B%9F%E8%AF%95%E8%A1%A3.mp4\"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})() 项目介绍本项目主要面向第 $14$ 届全国服务外包创新创业比赛 $A16$ 赛道虚拟试衣赛题，采用 $2D$ 虚拟试衣技术依托于 $VITON$ 开源数据集训练 $DNN$ 网络并着重进行工程化落地应用；项目选用了前沿顶刊论文的 $PFAFN$ 模型，在此基础上对模型进行优化改进，实现了模型压缩和推理加速并使用 $OpenVINO$ 框架进行部署应用，出色地完成了赛题的要求。 项目开发环境 开发平台 版本 开发工具 版本 Pycharm 2022.3.2 Visual Studio Code 1.80.1 Visual Studio 17.5.5 开发环境 版本 开发环境 版本 neural-compressor 2.2.1 nncf 2.5.0 numpy 1.23.4 onnx 1.14.0 opencv-python 4.7.0.72 onnxruntime 1.15.1 openvino 2022.3.0 pandas 1.3.5 pytorch-fid 0.3.0 rembg 2.0.50 pytorch 2.0.0 torch-pruning 1.1.9 intel-openmp 2021.4.0 模型结构介绍本项目基于 $PFAFN$ 模型重新设计各个网络模块，具体结构如下图所示： 项目工程化落地为了满足赛题方的要求，本项目开展了工程化落地部分，主要分为两个部分，模型训练和模型剪枝量化。项目工程化部署总图如下所示： 项目详细技术文档 实验结果：通道剪枝 Clothe Warp Module Metrics GFLOPs Para(M) SIZE(MB) Total SIZE(MB) Compresion Ratio FID FID Loss Original Module 6.63 9.37 35.8 112.0 100.00% 8.906 0.00% Ratio=0.2 with FineTuning 5.23 7.28 27.6 88.69 79.19% 9.013 1.20% Ratio=0.3 with FineTuning 4.40 6.48 24.8 65.73 58.69% 9.113 2.32% Ratio=0.4 with FineTuning 3.79 5.61 20.4 40.97 36.58% 9.304 4.47% Ratio=0.5 with FineTuning 3.42 4.55 16.8 35.47 31.67% 9.977 12.03% Image Generation Module Metrics GFLOPs Para(M) SIZE(MB) Total SIZE(MB) Compresion Ratio FID FID Loss Original Module 21.93 43.90 167 167 100.00% 8.906 0.00% Ratio=0.2 with FineTuning 16.54 35.02 112.3 112.3 67.25% 9.212 3.44% Ratio=0.25 with FineTuning 15.45 31.93 94.39 94.39 56.52% 9.405 5.60% Ratio=0.3 with FineTuning 13.90 29.89 80.25 80.25 48.05% 9.679 8.68% Ratio=0.35 with FineTuning 12.78 27.31 73.49 73.49 44.01% 9.835 10.43% Ratio=0.4 with FineTuning 11.20 26.12 68.52 68.52 41.03% 10.527 18.20% 最优剪枝方案 Model Original Model Sparsity Pruned Model FID FPS CWM 112MB 40% 40.97MB 9.504 2.92 IGM 167MB 25% 94.39MB 9.504 2.92 实验结果：量化感知训练 Optimization CPU-FID GPU-FID Original Model Quantized Model Unquantized 9.504 9.483 135.36MB 135.36MB Quantize CWM 9.783 9.701 40.97MB 10.85MB Quantize IGM 10.382 10.249 94.39MB 24.10MB Quantize CWM &amp; IGM 11.503 11.379 135.36MB 34.95MB 实验结果：img2col 优化加速 Runtimes CorrTorch(s) Img2Col(s) FPS Acceleration Rate n=1000 147.8491 94.7902 10.81 1.5598 n=10000 1489.1325 927.4293 10.77 1.6057 Average Time 0.1488 0.029 10.79 1.6017 参考文献 Y. Ge, Y. Song, R. Zhang, C. Ge, W. Liu, and P. Luo, “Parser-Free Virtual Try-on via Distilling Appearance Flows,” arXiv preprint arXiv:2103.04559, 2021. Y. Cheng, D. Wang, P. Zhou and T. Zhang, “Model Compression and Acceleration for DeepNeural Networks: The Principles, Progress, and Challenges,” in IEEE Signal Processing Magazine,vol. 35, no. 1, pp. 126-136, Jan. 2018, doi: 10.1109/MSP.2017.2765695. PyTorch Quantization Aware Training","link":"/projects/VirtualTryon/"},{"title":"SVM-based Adaptive Markov Chain for Tennis Match Prediction: Updatable Probabilities and Momentum Analysis","text":"IntroductionThis is an excellent paper of COMAP’s 2024 MCM/ICM contest. SVM-based Adaptive Markov Chain for Tennis Match Prediction: Updatable Probabilities and Momentum AnalysisZhihao Li, Changrong You, Zhihan LiuCOMAP’s Mathematical Contest in Modeling (MCM) and Interdisciplinary Contest in Modeling (ICM), 2024[paper][code] Research Background My Paper Contributors Zhihao Li Changrong You Zhihan Liu","link":"/publications/PUMCModelingPaper/"},{"title":"Research on Replenishment and Pricing Strategies Based on Time Series Linkage Analysis","text":"IntroductionThis is an excellent paper of mathematical modeling research with the honour of National Second Prize (&lt;1.53%). Research on Replenishment and Pricing Strategies Based on Time Series Linkage AnalysisZhihao Li, Pai Lin, Kaida HuangChina Undergraduate Mathematical Contest in Modeling (CUMCM), 2023[paper][code] Research Background My Paper Contributors Zhihao Li Pai Lin Kaida Huang","link":"/publications/TSLAModelingPaper/"},{"title":"Annual Report on Fall Semester in 2023","text":"Todo List Decide the milestones in this term. Make a major pipline of this term accompanied with timeline. Plot a graph on pipline of milestones based on the subject of Data Visualization. Make a Summary for 2023 New Year’s Resolutions for 2024 Informal Points October 12, 2023: Changed the surname to Taran for approaching researchers. October 13, 2023: Announced a new signature after a very long time: So many things! Formal PointsImportant PlanningUntil October 12, 2023, Schoolwork Todo List: Experiments from 2 to 5 of Data Visualization. Finish reading the book of Deep Reinforcement Learning. Prepare the examination of CET 6 again. Do some practice of small or medium projects. Finish reading the book of Assembly Language for Intel-Based Computers. Finish recording learning notes about the courses of Mrocomputer Principle and Interface Technology, Embedded Systems(Aichitecture, Programming and Design), Digital Signal Processing and Database System.","link":"/life/AnnualReport/"},{"title":"关于生命与健康的一些思考","text":"致敬人民好总理今天早上刚起来，就被一则重大的消息所震惊——我们的好总理李克强不幸逝世；我真的不敢相信，深深为之震惊：“他还那么年轻，这些年还一直在全心全意服务国家和人民，怎么会呢”。可叹人生苦短、世事无常，但总觉得总理音容犹在、笑貌宛存，中国又失去了一位人民的好总理······&lt;img src=&quot;https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/EBlog/Life/image.6e24avsioqs0.webp&quot; width=&quot;80%&quot;/&gt; 李克强总理的一生，是革命的一生、奋斗的一生、光辉的一生，是全心全意为人民服务的一生，是献身于共产主义事业的一生。向总理致敬，愿您一路走好！ 关于国务院总理的记忆我小时候便读过一些清正廉洁的书籍，对于政府官员为政作风，工作态度都有着很深的感触。我尤为记得国务院总理一直以来都是为国为民，无私奉献最好的榜样。特别是周恩来总理，一直到现在都是我的人生榜样，当时看到一本书上写道，真正是“鞠躬尽瘁，死而后已”。等到我读初中，正好是李克强总理在为国为民奔波劳累最辛苦的时候，在那时在李克强总理的身上我看到了周总理的影子，同样是“鞠躬尽瘁”，始终将为人民服务放在首位。 关于生命意义的思考李克强总理，享年68岁，前段时间才刚退休正应有机会安享晚年，却遭遇此劫难。在我的认知里，40-60岁正当中流砥柱之际，60-70岁正是老骥伏枥，志在千里之时，总理一生为人民鞠躬尽瘁却没得机会安详晚年······这让我想起保尔曾说，“人最宝贵的是生命，但生命只有一次，人的一生应该这样度过：当回忆往事的时候，他不会因为虚度年华而悔恨，也不会因为碌碌无为而羞愧；在临死的时候，他能够说，我的生命和全部的精力，都全部献给了世界上最壮丽的事业——为人类的解放事业而斗争”。我们的总理虽然走了，但是他的精神却永远留在了我们的心中。 为祖国健康工作50年让我深深触动地还有对生命健康的追求，从中可以看到我国医学事业仍需要努力，如果医学技术能够再先进一些，是不是就有很大可能挽回我们的好总理？我高中时也想过从事医学，为健康中国贡献自己的力量，但是阴差阳错选择了计算机科学与技术，中国医学的路途就交给其他有志青年，各自艰苦奋斗，任重而道远。 人生着实短暂，作为刚满20岁的我，在这个时间节点上已经开始为祖国事业贡献自己的力量，我会谨记保尔的忠告，会传承总理的精神，珍惜生命，为祖国健康工作50年。","link":"/life/HealthAndLife/"},{"title":"Wisdom","text":"Chapter 1 Conduct Myself in Life Understand your situation whenever and wherever you are and have the awareness of danger. Classify your fellows into two catogories: profit-oriented and friendship-oriented. In the toughest stage of life, no one will accompany you to the end, only to go forward alone. Be a self-driven individual wherever and whenever. Life Planning Graph: Chapter 2 Personal Qualities Do first and then talk, don’t do it and don’t say it. Looking back on my growth journey, I’m so lucky to have made exponential progress almost every year and also determined to maintain it forever. Fulfill my duties and do my job as excellently as possible. Chapter 3 InspirationHuman Vision VS Computer Vision Human VisionWe humans look at one word at a time and scan sentences sequentially: I think that’s because a single word in sight triggers our memory, and then we understand it at a glance; We scan a sentence sequentially because of the limited field of view, and although we may have a memory of the sentence, it takes many glances. Therefore, according to our human perception, memory can speed up reading and speed up information processing. The phrase ten lines at a glance is because there is a memory of the corresponding words, and the scanning speed is fast, and the amount of information captured is small. Computer VisionCompared with human vision, how can computers also use memory functions to speed up information processing, that is, how computer vision processes information blocks and executes one information block at a time.","link":"/life/Wisdom/"},{"title":"Deep Into Deep Learning","text":"第一讲 线性回归线性模型$$price = w_{area}\\cdot area + w_{age} \\cdot age + b$$ 基本概念小批量随机梯度下降(Minibatch Stochastic Gradient Descent)为了加宽执行计算损失函数关于模型参数的导数，通常会在每次需要计算更新的时候随机抽取一小批样本。 超参数(Hyperparameter)可以调整但不在训练过程中更新的参数。 调参(Hyperparameter Tuning)选择超参数的过程 泛化(Generalization)寻找一组最佳超参数能够在未知的数据上实现较低的损失。 卷积神经网络超参数 填充 无填充时输入图像 $Size=(n_h, n_w)$，卷积核 $Size=(k_h, k_w)$ $$_h\\times n_w \\Longrightarrow (n_h-k_h+1)\\times(n_w-k_w+1)$$ 填充图像时假设填充 $p_h\\times p_w$ $$_h\\times n_w \\Longrightarrow (n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1)$$ 通常取 $p_h=k_h-1, p_w=k_w-1$, 此时输出 $n_h\\times n_w$ $k_h = 2k+1, k\\in Z$, 上下两侧填充 $p_h/2$ $k_h = 2k, k\\in Z$, 上侧填充 $\\lceil p_h/2\\rceil$, 在下侧填充 $\\lfloor p_h/2\\rfloor$ 步幅高度和宽度的步幅分别设定为 $s_h, s_w$, 则输出图像形状: $$lfloor (n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor$$ 如果 $p_h=k_h-1, p_w=k_w-1$, 则得到 $\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor$ 如果 $n_h, n_w$ 可以被 $s_h, s_w$ 整除， 则得到 $(n_h/s_h)\\times(n_w/s_w)$ 超参数说明 一般用填充使得输入和输出的尺寸一致 $p_h=k_h-1, p_w=k_w-1$ 步幅等于1最好，提取信息最充分，但是模型的计算复杂度高，需要设置较多的网络层，通常步幅取 2 使得尺寸减半，降低计算复杂度。一般将步幅为 2 的网络层均匀插入到网络模型中，控制整体的复杂度。 卷积核的边长一般取奇数，为了使得填充具有对称性，即 $p_h=k_h-1, p_w=k_w-1$，但对模型结果影响不大 卷积核比较小时，可以增加网络层数来保证足够的感受野 References 李沐, 动手学深度学习","link":"/readings/DeepIntoDeepLearning/"},{"title":"Introduction of Deep Reinforcement Learning","text":"Reading Notes about the book Deep Reinforcement Learning written by Aske PlaatRecently, I have been reading the book Deep Reinforcement Learning written by Aske Plaat. This book is a good introduction to the theory of Deep Reinforcement Learning. And it is very inspiring when I learn the theory of Deep Reinforcement Learning. Introduction of Deep Reinforcement LearningReinforcement LearningThe author defines reinforcement learning as a field in which an agent learns by interacting with an environment. So my understanding is that there is an target environment which provides feedback signals for the action the agent had made. It is a form of active learning and has a certain level of autonomy. Definition of Deep Reinforcement LearningAs the book defines it, “Deep reinforcement learning is the combination of deep learning and reinforcement learning.” Compared with deep learning which is about approximating functions, this learning method is about learning from feedback and will utimately find a solution by trial and error.About their application, there is a table as shown below: Dataset Feature Low-Dimensional States High-Dimensional States Static Dataset classic supervised learning deep supervised learning Agent/Environment Interaction tabular reinforcement learning deep reinforcement learning In my opinion, deep reinforcement learning combines the features between deep learning and reinforcement learning so it will be also used for large, complex, high-dimensional environments. What the most important part of reinforcement learning is all about learning from success as well as from mistakes. The author inspired me that we can apply the human learning process or mechanism to the Artificial Intelligence. Just from this point, success is so good for summarizing the skills and mistakes is also good for accumulating experience. Sequntial Decision ProblemsFor me, this topic is so fresh and interesting. “Many real world problems can be modeled as a sequence of decisions.”, as the author described. We can view many daily problems as a sequence of decisions. Just like an algorithm to some problems, it has specific steps. But the difference is that the steps in sequntial decision problems are dynamic and algorithm is determined. Sequential decision steps will also have dynamic choices cooresponding to its environment which is a mirror image of the real world. It has two types of applications: robotic problems and games. Robotic problemsPreviously, all actions that a robot should take can be pre-programmed step-by-step by a programmer in meticulous detail. But this mode will be a disadvantage when the environments are slightly challenging. To fix this problem, we need the robot to be able to respond more flexible to different conditions. In this way, an adaptive program is needed which is just about today’s main topic. Four Related FiledsReinfocement is a field of interdisciplinary integration involving psychology, mathematics, engineering and biology. PsychologyReinforcement is also known as leaning by conditioning. Just to be simple, it is the process of learning by trial and error. That is continuous practice makes learning become a memeory of the body. MathematicsDiscrete optimization and graph theory are the basics of formulization of reinforcement learning. And we all kown it is the Mathematical formalizations that develops efficient planning and optimization algorithms. Symbolic reasoning and continuous optimization also play an important role in reinforcement. EngineeringThe filed of reinforcement learning is better known as optimal control. That is, applying reinforcement learning to the engineering has beening a creative way for optimal problems. BiologyNature-inspired optimization algorithms have been developed in artificial intelligence. It is worth mentioning connectionist AI. Mathematical logic and engineering approach intelligenceIt is a top-down deductive process with intelligence following deductively from theory. Connnectionism approach intelligenceIt is a bottom-up fashion and will forming intelligence out of many low level interactions. That is, intelligence follows inductively from parctice. Reinforcement Learning Paradigm Basic components: Dataset: produced dynamically. Agent: doess the learning of the policy. Environment: provides feedback. Goal: maximize the long term accumulated expected reward. There is a table showing the differences between reinforcement learning and supervised learning. Concept Supervised Learning Reinforcement Learning Inputs$x$ Full dataset of states Partial(One state at a time) Labels$y$ Full(correct action) Partial(Numeric action reward) Reference Aske Plaat, \"Deep Reinforcement Learning,\" Springer Nature Singapore, 2022","link":"/readings/Introduction/"},{"title":"Literature Survey Slides of Paper Learning Dexterous In-Hand Manipulation","text":"IntroductionThese days I have been reading the paper Learning Dexterous In-Hand Manipulation. And I need to do a acadamic presentation about this paper. To my disappointment, there are no templates of PPTs suitable for concise and precise academic presentation. So utilizing the previous experience, I decide to write the tutorial slides about this paper. Slides Display Resources [PDF][PPT] References Learning Dexterous In-Hand Manipulation","link":"/readings/LiteratureSurveySlides/"},{"title":"Reinforcement Learning","text":"I. IntroductionInspiration of AlphaGo Story: Machine can beat human people physically and intelligently. A new era for reinforcement learning and artificial intelligence: The ultimate goal of reinforcement learning is to find the optimal policy. II. Basic Concepts State: The status of agent with respect to the environment. State Space: the set of all states. $$S={s_i}$$ Action: For each state, there are some actions: $a_1, a_2, \\cdots, a_n$. Action Space of A State: the set of all possible actions of a state. $$A(s_i)={a_i}$$ State Transition: when taking an action, the agent may move from one state to another. $$s_1\\overset{a_1}{\\longrightarrow} s_2$$ Forbidden Area: the forbidden area is accessible but with penalty or inaccessible. Tabular Representation of State Transition: using a table to describe the state transition.(Deterministic Situation) State Transition Probability: use probability to describe state transition. $p(s_2|s_1, a_2)=1$ $p(s_i|s_1, a_2)=0\\ \\ \\forall i\\neq 2$ Policy: tells the agent what actions to take at a state. Deterministic Policy $\\pi(a_1|s_1)=0$ $\\pi(a_2|s_1)=1$ $\\pi(a_3|s_1)=0$ $\\vdots$ $\\pi(a_{n-1}|s_1)=0$ $\\pi(a_n|s_1)=0$ Stochastic Policy$\\pi(a_1|s_1)=0$ $\\pi(a_2|s_1)=0.5$ $\\pi(a_3|s_1)=0.5$ $\\vdots$ $\\pi(a_{n-1}|s_1)=0$ $\\pi(a_n|s_1)=0$ Tabular Representation of A Policy Reward: a real number we get after taking an action.(Human-machine Interface) Tabular Representation of Reward Transition Stochastic Reward Transition: conditional probability $p(r=-1|s_1, a_1)=0.5$ $p(r\\neq-1|s_1, a_1)=0.5$ Trajectory: a state-action-reward chain $$s_1\\mathop{\\longrightarrow}\\limits_{r=0}^{a_2}s_2\\mathop{\\longrightarrow}\\limits_{r=0}^{a_3}s_3\\cdots s_{n-1}\\mathop{\\longrightarrow}\\limits_{r=1}^{a_{n-1}}s_n$$ Return of A Trajectory: the sum of all the rewards $Return=0+0+0+\\cdots+1$ Discounted Return: the sum of all the rewards multiplied by discount factor $\\gamma\\in[0, 1]$ discounted return = $r_1+\\gamma r_2+\\gamma^2 r_3+\\cdots+\\gamma^{n-1} r_n$ the sum becomes finite balance the far and near future rewards EpisodeWhen interacting with the environment following a policy, the agent may stopat some terminal states. The resulting trajectory is called an episode (or atrial). episodic tasks: tasks with episodes which has finite trajectories.continuing tasks: tasks without terminal states, meaning the interaction with the environment will never end. Convert episodic tasks to continuing tasks Treat the target state as a special absorbing state. Once the agent reaches an absorbing state, it will never leave. The consequent rewards $r = 0$. Treat the target state as a normal state with a policy. The agent can still leave the target state and gain $r = +1$ when entering the target state. References Mathematical Foundations of Reinforcement Learning","link":"/readings/ReinforcementLearning/"},{"title":"Literature Survey about Volumetric Grasping Network:Real-time 6 DOF Grasp Detection in Clutter","text":"摘要概述在通用机器人研究领域中，grasp detection 任务要求能够在一堆杂乱的物体中识别从未遇到过的物体以及处理物体之间的堆叠问题。作者针对此项任务提出了Volumetric Grasping Network(VGN)，以$3$D场景的Truncated Signed Distance Function(TSDF)表示作为输入，并能够对$3$D场景中的每一个体积元直接输出机器人抓取质量预测值，以及钩爪方向和张开宽度。这种方法能够在$10$ms内规划下一次抓取任务并且可以在现实世界杂乱物体中清除$92$%的物体。 研究的领域机器人控制领域需要解决灵活性问题，即在复杂环境中能够计算无限次抓取并且处理物体聚类、堆叠以及高维噪声的问题。 解决的问题在杂乱环境中规划未知物体的平行抓取问题，目标是寻找能够使得机器人成功抓取并从工作空间中移除所有物体的夹持器配置。 之前的研究最近的研究大都关注于以数据驱动的方法规划抓取任务，但是规划的能力通常在单张深度图中局限于自顶向下的抓取。这种方法将搜寻能力约束为垂直抓取，假定物体能够垂直放置并且机器人只从一个单一的方向抓取。最近比较好的一些工作能够在姿势估计上处理$6$个自由度的抓取，但是只能处理单个、孤立的物体，这使得这种方法在现实场景应用中需要额外的堆叠检测，甚至需要较高的推理时间。 作者的创新作者提出了一种新颖的方法——$6$自由度实时抓取的综合方法，算法的输入是一个采用TSDF算法重建的表面模型，其中每一个体积元都利用截断符号距离表示真实场景表面附近的区域。作者的主要创新点在于对于TSDF算法不仅是用于在$3$D场景重建中融合多个点云数据映射成连续的表面模型，还利用TSDF算法构建的规则的结构借助深度神经网络进行场景的特征学习。 如上图，作者构建了一个全卷积网络(FCN)将输入TSDF映射到一个带有解决方案的相同空间中，其中的每一个体积元都包含有预测的抓取质量、钩爪方向和张开宽度。融合的特征使得这种方法能够通过网络的一步前向传递来检测整个工作空间的抓取。物体堆叠的处理：作者假设包含整个场景的$3$D信息能够使得神经网络捕获钩爪与环境之间的冲突。New Idea: 代替以往的研究以采样和评估单个的抓取策略，作者针对整个离散的抓取策略空间中所有抓取位置进行了评估。 作者的贡献 使得$6$自由度的抓取合成方法能够实时处理 使用全部3D场景信息去直接学习无冲突的抓取策略 算法的流程最近的研究由于深度学习对于未知的物体具有非常优越的泛化能力以及能够在杂乱的场景中找到可行的抓取策略，因此机器人抓取研究中经常选择深度学习方法来检测机器人的抓取。 综合的抓取方法可以分为两大类，第一种是输出自顶向下的抓取策略(尤其是$3$或$4$自由度)，第二种是输出$6$自由度的抓取策略。自顶向下的抓取策略或者使用顶视图数据，或者是深度图数据，或者是两者的结合并且以图形结构的形式返回可行的抓取策略；$6$自由度的抓取网络以整个场景的点云或者占据网格形式表达的$3$D信息作为输入。假设：场景的点云信息能够识别空间物体的表面，假定由TSDF表示的额外的距离到表面信息能够提高整个抓取检测性能表现。因此作者认为对整个场景信息的充分利用能够使得系统考虑物体之间的物理交互作用，所以作者将整个TSDF输入给网络。通过这个策略使得最终处理场景重建和堆叠检测时不再需要进行分割处理。 存在的问题这些抓取方法基本都作为抓取质量的预测器，这要求需要一些初始化过程在抓取策略空间中进行采样评估。初始化过程是非常麻烦的，因为这需要在计算复杂度与采样覆盖范围达到最优的平衡。 作者的处理作者训练抓取网络在整个空间体积表示中对于每一个体积元直接输出一个抓取策略(抓取方向和张开宽度)以及相联系的抓取质量预测值。网络采用非局部极大值抑制方法直接输出的就是最佳的抓取策略，而不用在运行时对候选的抓取策略进行评估得到最佳抓取策略，也不需要在抓取策略空间中进行迭代寻找可行的抓取策略，这是作者建立的网络模型能够进行实时处理的很重要的原因。 问题的表述 Gripper表示 $$\\tilde{\\boldsymbol{t}} = \\frac{T_{RV}(t)}{\\boldsymbol{v}}, \\tilde{\\boldsymbol{r}}=T_{RV}(\\boldsymbol{r}), \\tilde{w}=\\frac{w}{v}$$ 其中，$\\tilde{\\boldsymbol{t}}$ 表示钩爪被定义的体积元的坐标，$\\tilde{w}$ 表示以体积元的大小为单元进行计量； 规划的目标解决该该问题的目标就是寻找一个映射关系 $$f: \\boldsymbol{V}\\rightarrow \\boldsymbol{Q}, \\boldsymbol{R}, \\boldsymbol{W}$$ 其中，$\\boldsymbol{Q}, \\boldsymbol{R}, \\boldsymbol{W}$ 包含在每一个体积元 $\\tilde{\\boldsymbol{t}}$ 上的抓取质量 $q$, 抓取方向 $\\tilde{\\boldsymbol{r}}$ 和 张开宽度 $\\tilde{w}$. 构建的模型作者构建了 Volumetric Grasping Network(VGN), 利用深度神经网络去估计深度映射关系 $f$。 网络结构VGN采用FCN结构，首先设置了一个感知模块，用于将输入空间体 $\\boldsymbol{V}$ 映射成特征图，第二部分由三个卷积层组成，交织2倍的双线性上采样，紧随三个独立的模块分别用于预测抓取质量，旋转以及张开宽度。 输出说明 抓取质量模块输出 $1\\times N^3$ 大小的体积单元，其中每一项表示在该体积单元上成功完成抓取任务的概率值。 旋转模块输出以四元组的形式表示抓取策略的方向，其中四元组选用欧拉角进行表示 张开宽度模块输出预测了每一个体积元上应当采取的张开宽度 综合训练作者训练整个网络采用端到端的方式进行训练，构建了以下损失函数： $$\\zeta(\\hat{g_i}, \\tilde{g_i}) = \\zeta_q(\\hat{q_i}, q_i)+q_i(\\zeta_r(\\hat{\\boldsymbol{r}_i}, \\tilde{\\boldsymbol{r}_i})+\\zeta_w(\\hat{w_i}, \\tilde{w_i}))$$ 其中，$q_i\\in${$0, 1$} 表示可行抓取策略的目标标签，而 $\\hat{q_i}$ 表示VGN网络的预测输出标签，$\\zeta_q$ 表示$q_i$ 和$\\hat{q_i}$ 之间的二进制交叉损失，$zeta_w$ 表示展开宽度的预测 $\\hat{w}$ 与目标 $w$ 的均方误差。对于四元组表示的抓取方向，在这里以点积的形式进行计算损失： $$\\zeta_{quat} = 1-|\\boldsymbol{\\hat{r}}\\cdot \\tilde{\\boldsymbol{r}}|$$ 由于夹持器的对称性，在旋转$180^\\circ$的配置实际上对应的是相同的抓取，但是这会导致不一致的损失信号，因为网络会因为回归到两种不同的三维旋转之一而受到惩罚，因此作者对损失函数进行了扩展： $$\\zeta_r(\\boldsymbol{\\hat{r}}, \\tilde{\\boldsymbol{r}})=\\min(\\zeta_{quat}(\\boldsymbol{\\hat{r}}, \\tilde{\\boldsymbol{r}}), \\zeta_{quat}(\\boldsymbol{\\hat{r}}, \\tilde{\\boldsymbol{r_{\\pi}}}))$$ 遗留的问题 机器人夹具形状差异的问题作者所提出的方法进一步推广应用到不同的夹具几何形状上需要通过进一步的实验来验证。 物理模拟实验局限性的问题在真实的机器人实验中，仅根据模拟数据进行训练存在一定的局限性，需要在物理模拟中引入对抗性测试进一步地去验证。 总结与思考Summary这篇论文的主要贡献在于针对通用机器人领域中抓取物体的研究提出了创新性的算法与网络模型，解决了抓取任务中物体堆叠造成的冲突问题以及因计算复杂而造成的不可实时处理的问题。作者最大的创新之处在于摆脱了以往研究中通过采样和评估来获得单次的抓取策略，提出对整个场景空间中的所有抓取位置进行评估，选取抓取质量最好的位置进行抓取。 Personal Views对体抓取网络(VGN)的一些看法，作者不仅利用TSDF结构表示场景信息，并且借助于深度神经网络设计了一个特征学习模块，从场景(TSDF)中学习空间中每个体积元的特征，并通过进一步预测抓取质量、抓取方向以及张开宽度。这三个网络模块都是以3D场景中的每个体积元中心进行抓取的，因此在实际场景中，可以利用深度传感器检测物体的空间分布，然后对于覆盖的所有体积元选择抓取质量最高的体积元来获取抓取策略。可能这就是作者提出的体抓取网络名字的由来。一些延伸想法，作者提出的VGN需要根据TSDF重建的3D场景信息计算所有的体积元，然后预测获取他们的抓取质量、抓取方向以及张开宽度；但是在场景范围比较大的时候，模型的计算量也将会是非常巨大的，并且抓取物体不仅与抓取方向和抓取宽度有关，还与在物体的某个方向施加的抓取力度有关，因此或许可以进一步在模型中考虑抓取力度对成功率的影响。","link":"/readings/VGNPaperReport/"},{"title":"A Literature Survey about Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels","text":"I.Summary OverviewBackground: A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises.Interest: Studying the key reasons contributing to the robustness of the prompt tuning paradigm. Findings: the fixed classname tokens provide a strong regularization to the optimization of the model, reducing gradients induced by the noisy samples; the powerful pre-trained image-text embedding that is learned from diverse and generic web data provides strong prior knowledge for image classification. II.Research InterestsThe author studies the key reasons contributing to the robustness of the prompt tuning paradigm. III.Problems SolvedIn author’s work, they demonstrate that prompt tuning is robust to noisy labels, and investigate the mechanisms that enable this robustness. IV.Previous ResearchWhile prompt tuning has proven effective when training on downstream tasks with accurately annotated datasets, their robustness to noisy labels has been neglected. V.Author’s InnovationThe author investigates the mechanisms that enable this robustness and proposes a simple yet effective method for unsupervised prompt tuning, showing that randomly selected noisy pseudo labels can be effectively used to enhance CLIP zero-shot performance. VI.Author’s Contribution We demonstrate that prompt tuning for pre-trained vision-language models (e.g., CLIP) is more robust to noisy labels than traditional transfer learning approaches, such as model fine-tuning and linear probes. We further demonstrate that prompt tuning robustness can be further enhanced through the use of a robust training objective. We conduct an extensive analysis on why prompt tuning is robust to noisy labels to discover which components contribute the most to its robustness. Motivated by this property, we propose a simple yet effective method for unsupervised prompt tuning, showing that randomly selected noisy pseudo labels can be effectively used to enhance CLIP zero-shot performance. The proposed robust prompt tuning outperformed prior work on a variety of datasets, even though noisier pseudo-labels are used for self-training. VII.Algorithm FlowRecent Research CLIP: CLIP applies prompt engineering to incorporate the category information in the text input such that its pre-trained model can adapt to various image classification tasks without further training. CoOp: CoOp introduces learnable prompts optimized on target datasets to address CLIP’s problem ProDA: ProDA tackles CoOp’s issue by utilizing diverse prompts to capture the distribution of varying visual representations. UPL: UPL proposes a framework to perform prompt tuning without labeled data. TPT: TPT achieves zero-shot transfer by dynamically adjusting prompts using only a single test sample. Potential of prompt tuning: Label noise-robust learning Label noise-robust learning robust losses that tolerate noisy labels loss correction approaches that estimate a transition matrix to correct the predictions meta-learning frameworks that learn to correct the label noise in training examples regularization techniques that are customized to lower the negative impact of noise Existing Problems CLIP: the design of a proper prompt is challenging and requires heuristics. CoOp: CoOp has also faced criticism for disregarding the diversity of visual representations. Author’s Processing Demonstrate that prompt tuning on CLIP naturally holds powerful noise robustness. Explore the key factors behind such robustness. Show its application on unsupervised prompt tuning. Constructed Model CLIPIn the case of image classification, a normalized image embedding $\\boldsymbol{f}^v$ is obtained by passing an image $\\boldsymbol{x}$ through CLIP’s visual encoder, and a set of normalized class embeddings $[\\boldsymbol{f}^t_i]^K_{i=1}$ by feeding template prompts of the form “A photo of a “ into CLIP’s text encoder.$$Pr(y=i|\\boldsymbol{x})=\\frac{\\exp(sim(\\boldsymbol{f}^v,\\boldsymbol{f}^t_i))/\\tau}{\\sum_{j=1}^K\\exp(sim(\\boldsymbol{f}^v,\\boldsymbol{f}^t_j))/\\tau}$$ Prompt TuningThe name of a class c is first converted into a classname embedding $\\boldsymbol{w}\\in R^d$ and prepended with a sequence of $M$ learnable tokens $\\boldsymbol{p_m}\\in R^d$ shared across all classes.$$P_c=[\\boldsymbol{p_1}, \\boldsymbol{p_2}, \\cdots, \\boldsymbol{p_M}, \\boldsymbol{w_c}]\\rightarrow \\boldsymbol{f}^t_c$$CoOp optimizes the shared learnable tokens $\\boldsymbol{p_1}, \\boldsymbol{p_1}, \\cdots, \\boldsymbol{p_M}$ on a small labeled dataset $D = [(\\boldsymbol{x_i}, c_i)^N_{i=1}]$ to minimize the cross-entropy loss$$L_{CE}=-E_{(\\boldsymbol{x},c)\\in D}[\\log Pr(y=c|\\boldsymbol{x})].$$ Robust Prompt TuningFurther enhance this robustness by optimizing the learnable prompts using the generalized cross-entropy (GCE) loss$$L_{GCE}=E_{(\\boldsymbol{x},c)\\in D}[\\frac{1-Pr(y=c|\\boldsymbol{x})^q}{q}].$$ Author’s Conclusion: $q = 0.7$ leads to overall good performance across several experimental settings. VIII.Robustness Analysis Pre-trained CLIP Generates Effective Class Embeddings Author’s Conclusions: Classifier-R v.s. Classifier-C: CLIP class embeddings provide a strong initialization for few-shot learning. TEnc-FT v.s. Classifier-C: The highly expressive CLIP text encoder can easily overfit to the noisy labels. Prompt Tuning v.s. Classifiers: The text encoder is essential for providing a strong but informative regularization of the text embeddings to combat noisy inputs. Prompt Tuning v.s. TEnc-FT: The text encoder should be fixed to prevent overfitting. Effectiveness of Prompt Author’s Conclusions: Full Prompt Tuning v.s. CLS Tuning: The class embeddings generated by CLIP pre-trained text encoder plays a critical role in noise robustness. Hypothesis: The classname token $\\boldsymbol{w_c}$ provides a strong regularization to the model, since it is leveraged by the text encoder to encode relationships between the different visual concepts. Prompt Tuning Suppresses Noisy Gradients Prompt tuning can suppress gradient updates from noisy samples, while aggregating gradients from clean samples. This property likely arises from the highly constrained prompt tuning optimization, which restricting the model to fit the noisy labels. Generalization Across Model Architectures Context length The optimal context length is dataset dependent. Image encoders ViT-B/32-PT outperforms RN50-PT under most settings. Moreover, both methods do not suffer from a large performance drop and maintain competitive accuracy at high noise rates. Robustness to Correlated Label Noise Confusion noise: Each mislabeled sample is labeled as the incorrect class that is most favored by zero-shot CLIP. Author’s Conclusions: Confusion noise presents a bigger challenge to transfer learning, leading to larger degradation of classification accuracy at high noise ratios compared to random noise. Prompt tuning still achieves the best overall performance, providing further evidence for its robustness even to more challenging types of noise. IX.Application to Unsupervised Prompt Tuning Baseline UPL Phase 1: Leverage pre-trained CLIP to generate pseudo labels for unlabeled images. Phase 2: Select the $K$ most confident samples per class to optimize the learnable tokens through the typical prompt-tuning optimization process (described in CoOp). Features: UPL improved transfer performance by ensembling multiple predictions generated by models with different learnable prompts. Robust UPL Overview: Based on UPL, randomly sample $K$ training samples and optimize the prompt with the robust GCE loss X.Summary And ViewsSummaryThis paper focus on prompt tuning to research and analyze the attribution of robustness to label noise that it has naturally. And the author also combines the findings with the UPL model and proposes a more robust UPL model in unsupervised prompt tuning. Personal ViewsFirstly I learned a lot from this paper which analysis the robust of prompt tuning to label noise. This research spirit and methodology is a great need in motivating me to work on the research of robustness. And what’s impresses me most is the robust UPL model that is the author’s innovation about the previous research. XI.Domain LearningRelated Terms Vision-language model text-image embedding and image-text embedding few-shot prompt tuning fixed classname tokens zero-shot learning downstream tasks: few-shot learning, continual learning, object segmentation model-informed structure traditional fine-tuning and linear probing paradigms generalized cross-entropy (GCE) VisionLanguage Pre-Trained Models (VL-PTMs) meta-learning References Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?","link":"/readings/PINLPaperReport/"},{"title":"Tabular Value-Based Reinforcement Learning","text":"Reading Notes about the book Deep Reinforcement Learning written by Aske PlaatRecently, I have been reading the book Deep Reinforcement Learning written by Aske Plaat. This book is a good introduction to the theory of Deep Reinforcement Learning. And it is very inspiring when I learn the theory of Deep Reinforcement Learning. Tabular Value-Based Reinforcement LearningTabular Value-Based AgentsReinforcement learning paradigm: an agent and an environment. The origin of this concept can be traced back to the process that human interacts with the objective world. I ploted a graphical depiction to demonstrate the relationship showed as follows. Agent and EnvironmentFormally, we can represent the relationship in the figure above. $S_t\\stackrel{a_t}{\\longrightarrow} S_{t+1}$ $S_{t+1}\\stackrel{r_{t+1}}{\\longrightarrow} a_{t+1}$ Features The environment gives us only a number as an indication of the quality of an action; We can generate as many action-reward pairs as we need, without a large hand-labeled dataset (action and reward, action and reward…) Markov Decision ProcessSequential decision problems can be modelled as Markov decision processes(MDPs).The Markov property the next state depends only on the current state and the actions available in it(no-memory property) Define a Markov decision process for reinforcement learning as a 5-tuple ($S, A,T_a,R_a,\\gamma$) $S$ is a finite set of legal states of the environment $A$ is a finite set of actions($A_s$ is the finite set of actions in state $s$) $T_a(s, s’)=Pr(s_{t+1}=s’|s_t=s, a_t=a)$ is the probability that action $a$ in state $s$ at time $t$ will transition to state $s’$ at time $t+1$ $R_a(s,s’)$ is the reward received after action $a$ transitions state $s$ to state $s’$ $\\gamma\\in[0, 1]$ is the discount factor representing the difference between future and present rewards. State $S$ State Presentation: the state $s$ contains the information to uniquely represent the configuration of the environment. Deterministic EnvironmentIn discrete deterministic environments the transition function defines a one-step transition. That is, each action deterministically leads to a single new state. Stochastic EnvironmentThe outcome of the action is unknown beforehand by the agent because of continuous state space. And that result depends on elements in the environment.So we can conclude that the stachastic environment determines the stochastic states. Action $A$ An action changes the state of the environment irreversibly. Actions that the agent performs are also known as its behavior, just as the human’s behavior. Discrete or Continuous Action Space Action Space is related to the specific application for example, the actions in board games are discrete, while the actions in robotics are continuous. Value-Based methods work well for discrete action spaces, and Policy-Based methods work well for both action spaces. Transsition $T_a$ Model-Free Reinforcement LearningOnly the environment has access to the transition function while the agent has not. In this pattern, the transition $T_a(s, s’)$ equals to the nature laws that is internal to the environment, which the agent does not know. Model-Based Reinforcement LearningThere the agent has its own transition function, an approximation of the environment’s transition function, which is learned from the environment feedback. In my opinion, that is just our policy experince which is summarized from the past feedbacks. The dynamics of the MDP are modelled by transition function $T_a(\\cdot)$ and reward function $R_a(\\cdot)$.In Reinforcement Learning, reward learning is learning by backpropagation. In the dicision tree, action selection moves down, reward learning flows up. To be detailed, the downward selection policy chooses which actions to explore, and the upward propagation of the error signal performs the learning of the policy. Reward $R_a$Rewards are associated with single states, indicating their quality. Value Function $V^\\pi(S)$Usually, we are most often interested in the quality of a full decision making sequence from root to leaves. So the expected cumulative discounted future reward of a state is called the value function. Discount Factor $\\gamma$In continuous and long running tasks it makes sense to discount rewards from far in the future in order to more strongly value current information at the present time. In my view, it’s just weights factor for every time point. Policy $\\pi$The policy $\\pi$ is a conditional probability distribution that for each possible state specifies the probability of each possible action. Formally, the function $\\pi$ is a mapping from the state space to a probability distribution over the action space: $$\\pi: S\\rightarrow p(A)$$ For a particular probability from this distribution we notes: $\\pi(a|s)$.A special case of a policy is a deterministic policy, denoted by $\\pi(s)$, and the mapping: $$\\pi: S\\rightarrow A$$","link":"/readings/TabularRL/"},{"title":"西电计科B测——计算机网络综合实验","text":"本博客旨在帮助西电的学弟学妹们顺利“水”过B测，主要对往届学长的西电B测-计算机专业-计算机网络综合设计实验一站式指南博客中出现的验收问题进行了整理总结(仅供参考，有一些问题蛮抽象的，比如ARP报文组成，基本不可能会问)，其中问到我们组的问题标了下划线。这次B测大家普遍感觉唐军老师验收不严，很好过，只要大家自己做一遍，再看看相关的原理，肯定不会挂。 我们组的网络拓扑图： 一、静态路由/动态路由及路由连接 你们使用的是什么路由设置：静态 静态和动态路由的优点，从规模和维护成本上考虑 规模： 静态路由： 适用于较小的网络规模，因为手动配置静态路由表可能会变得繁琐和容易出错，尤其是在大型网络中。 动态路由： 适用于大型网络，因为它能够自动适应网络拓扑和流量变化，提供更高的可伸缩性。 维护成本： 静态路由： 相对较低，因为配置简单，不需要动态更新路由表。但是，在网络拓扑或流量变化时，需要手动修改路由表，增加了维护成本。 动态路由： 初始配置可能较复杂，但是一旦配置完成，路由表会自动更新以适应网络变化，减少了手动维护的工作量。 动态路由有哪些，他们是域内还是域外的 域内路由协议： OSPF（Open Shortest Path First）： 基于链路状态的路由协议，用于在单个自治系统内部交换路由信息。OSPF使用Dijkstra算法计算最短路径。 IS-IS（Intermediate System to Intermediate System）： 也是一种链路状态路由协议，通常用于大型网络中，如ISP网络。 RIP（Routing Information Protocol）： 距离向量路由协议，用于小型网络中。RIP基于跳数来计算最佳路径。 域外路由协议： BGP（Border Gateway Protocol）： 用于在不同自治系统之间交换路由信息。BGP是一个路径矢量协议，根据AS路径选择最佳路由。 静态路由路由表是在哪些设备上配置的，是如何配置的（解释一下指令，还有下一跳的方式，下一跳的目标地址在哪） AR1, LSW2，AR2: 例如：ip route-static 192.168.3.0 24 192.168.5.195 其中：第一个地址是目标网络的网络地址和子网掩码；第二个地址是下一跳地址，就是数据包离开当前设备后，需要去的下一个设备的地址(在这里就是和路由器相连的接口地址) rip路由为什么只配置路由器两端网络号(1，2配2，3配)就行，那样网段1和3怎么通信？ ​ 在RIP中，当配置路由时只需配置相邻路由器之间直接连接的网络号，是因为RIP是一种距离向量路由协议，它只关心到达目的地网络的跳数（即路由的距离），而不考虑具体的网络拓扑。如果网络中存在多个路由器，每个路由器只需知道到达其它网络的下一跳路由器即可，而不需要知道到达其它网络的整个路径。 两个主机之间为什么能ping通，不同网段之间的主机怎么通信（比如说路由器两端的pc是通过直连路由实现的） ​ 当两个主机处于同一个网络（同一个网段）时，它们可以直接通过 ARP（地址解析协议）找到对方的 MAC 地址，然后直接进行通信，比如使用 ICMP 协议进行 ping 测试。 ​ 当两个主机处于不同的网络（不同的网段）时，它们不能直接进行通信，因为它们不在同一个广播域内，无法直接通过 ARP 找到对方的 MAC 地址。在这种情况下，需要通过路由器来实现不同网段主机之间的通信。 ​ 假设有两个不同网段的主机 A 和 B，它们想要进行通信。它们分别连接到同一台路由器的不同接口上。路由器在收到来自主机 A 的数据包时，会检查数据包的目的 IP 地址，并查找自己的路由表，找到到达目的地的最佳路径。然后，路由器将数据包转发到相应的接口，进而将数据包发送给主机 B。在主机 B 回复时，路由器会将数据包转发给主机 A。 这种情况下，主机 A 和主机 B 通过路由器进行通信，实现了不同网段主机之间的通信。 rip的工作原理 RIP（Routing Information Protocol）是一种基于距离向量的动态路由协议，用于在小型和中型网络中传播路由信息。以下是RIP的基本工作原理： 距离向量： RIP使用距离向量作为路由选择的依据。每个路由器维护一个距离向量表，其中包含到达每个目的网络的跳数（即路由的距离）和下一跳路由器的信息。 路由更新： 路由器周期性地向相邻路由器发送路由更新消息，包含自己所知道的所有路由信息。这些更新消息称为RIP响应。 路由选择： 当路由器收到来自相邻路由器的路由更新消息时，它会比较接收到的路由与自己的距离向量表中的路由，选择距离最短的路径作为最佳路径，并更新自己的距离向量表。 路由失效： 如果一个路由器在一定时间内没有收到来自相邻路由器的路由更新消息，它会认为该路由失效，并将该路由从距离向量表中移除。 路由循环避免： RIP使用了一些机制来避免路由环路的发生，如最大跳数限制和毒性逆转（将失效路由的距离设置为无穷大）。 总的来说，RIP通过周期性的路由更新和基于距离向量的路由选择，使得网络中的路由器能够了解到整个网络的拓扑结构，并选择最佳路径进行数据转发，从而实现了动态路由的功能。 二、DHCP dhcp在哪开启？AR1 dhcp分配地址在哪实现的？AR1 dhcp的过程？（就是你在路由器上输入指令的顺序用文字描述出来，先设置路由器端口ip，然后允许dhcp，创建线程池，配置线程池网段、网关……） dhcp的原理 DHCP（Dynamic Host Configuration Protocol）是一种用于自动分配IP地址和其他网络配置参数的网络协议。以下是DHCP的基本工作原理： 客户端发现： 当设备加入网络或重新连接到网络时，它会发送一个广播消息（称为DHCP Discover），请求DHCP服务器提供IP地址和其他配置信息。 服务器提供： DHCP服务器接收到客户端的DHCP Discover 消息后，会向其发送一个DHCP Offer 消息，其中包含可用的IP地址和其他配置信息，如子网掩码、网关、DNS服务器等。 客户端请求： 客户端收到DHCP Offer后，会选择其中一个提供的IP地址，并向DHCP服务器发送一个DHCP Request消息，确认使用该IP地址。 服务器确认： DHCP服务器收到客户端的DHCP Request消息后，会向客户端发送一个DHCP Acknowledgment消息，确认分配给客户端的IP地址和其他配置信息。 续约与释放： 客户端在使用IP地址的过程中，会定期向DHCP服务器发送DHCP Request消息进行地址续约，以保持IP地址的有效性。当客户端不再需要IP地址时，会发送一个DHCP Release消息释放该地址。 通过DHCP，网络管理员可以集中管理IP地址的分配和网络配置，同时客户端设备可以自动获取所需的网络配置信息，使网络配置更加灵活和便捷。 ip池是怎么分配IP地址的（通过dhcp服务端分配….） 在DHCP服务器上配置的IP地址池是一组可用于分配的IP地址范围。当DHCP服务器收到来自客户端的DHCP Discover消息时，它会从IP地址池中选择一个可用的IP地址分配给客户端。分配IP地址的过程如下： 选择可用IP地址： DHCP服务器会检查IP地址池中哪些地址尚未被分配或者已经分配但过期未续约。服务器会选择一个未被使用的IP地址作为分配目标。 分配IP地址： DHCP服务器将选定的IP地址包含在DHCP Offer消息中，并发送给客户端。 确认分配： 客户端收到DHCP Offer消息后，会选择接受其中的IP地址。客户端会向DHCP服务器发送DHCP Request消息，确认接受所提供的IP地址。 分配成功： DHCP服务器收到客户端的DHCP Request消息后，会向客户端发送DHCP Acknowledgment消息，确认IP地址的分配成功。客户端随后会配置其网络接口，开始使用该IP地址进行通信。 续约和释放： 在使用IP地址期间，客户端会定期向DHCP服务器发送DHCP Request消息进行地址续约。如果客户端不再需要IP地址，可以发送DHCP Release消息释放该地址，使其返回到IP地址池中可供其他客户端使用。 通过这种方式，DHCP服务器可以有效地管理和分配IP地址，使网络中的设备可以自动获取所需的网络配置，减少了手动配置IP地址的工作量。 如果主机D也要用dhcp如何设置 在AR2的端口g0/0/1上配置DHCP 三、限速 说一下限速的命令，其中一些单词有什么含义，最后那个500000是什么意思，什么单位 设置的平均速率cir，单位是kbps 四、ARP ARP（Address Resolution Protocol，地址解析协议）是用来将IP地址解析为MAC地址的协议，ARP表用于存储IP地址和MAC地址的映射关系。 为什么需要ARP：局域网中，当主机或其他三层网络设备发送数据到，仅仅知道对方的IP地址是不够的，因为IP报文必须封装成帧才能通过物理网络发送（主机的以太网网卡只能之别MAC地址），ARP可以确定目标IP地址对应的MAC地址，将IP数据包封装在以太网帧中，以便在局域网中传输。 ARP报文组成： 硬件类型（Hardware Type）： 指定网络硬件的类型，例如以太网。 ​ 协议类型（Protocol Type）： 指定网络层协议的类型，例如IPv4。 ​ 硬件地址长度（Hardware Address Length）： 指定硬件地址的长度，以字节为单位，例如以太网中为6字节。 ​ 协议地址长度（Protocol Address Length）： 指定协议地址的长度，以字节为单位，例如IPv4地址为4字节。 ​ 操作码（Opcode）： 指定ARP消息的类型，例如ARP请求或ARP响应。 ​ 发送方硬件地址（Sender Hardware Address）： 发送ARP请求的设备的MAC地址。 ​ 发送方协议地址（Sender Protocol Address）： 发送ARP请求的设备的IP地址。 ​ 目标硬件地址（Target Hardware Address）： 用于ARP响应中，指定目标设备的MAC地址。 ​ 目标协议地址（Target Protocol Address）： 用于ARP请求和响应中，指定目标设备的IP地址。 ARP泛洪 概念： 场景一：处理ARP报文和维护ARP表需要消耗系统资源，ARP表项的规模具有规格限制；攻击者通过伪造大量源IP地址变化的ARP报文，使得设备的ARP表资源被无效ARP条目耗尽，从而合法的ARP报文不能正常生成ARP表项，通信中断。 场景二：给设备发送大量目标IP地址不能解析的IP报文，设备会产生大量的ARP Miss消息，生成并广播大量的ARP请求报文试图对目标IP地址进行解析，从而造成CPU负荷过重。 应对方法： 配置ARP报文限速：设备限制接受处理的ARP报文数量来保护CPU资源 1234sysint 接口arp anti-attack rate-limit enable #开启ARP报文限速arp anti-attack rate-limit 80 1 #限制每秒最多处理的ARP报文数量为80 配置ARP Miss消息限速：限制ARP Miss消息处理的速率，从而控制ARP请求报文发送数量 12sysarp-miss anti-attack rate-limit enable #开启ARP Miss消息限速 ARP表项严格学习：只有本设备主动发送的ARP请求报文的应答报文才会触发ARP表项的学习更新 123sysint 接口arp learning strict force-enable ARP欺骗 概念：攻击者通过发送伪造的ARP报文，恶意修改设备或网络内其他用户主机的ARP表项，造成用户或网络的报文通信异常 攻击者伪造的报文只修改了mac，还是拥有正确的ip地址，为啥还是可以使通信异常：在局域网中通信是通过物理地址（MAC地址）进行传输的，所以攻击者只需伪造MAC地址即可干扰通信 应对方法： 配置ARP报文合法性检查：对收到的ARP报文，检查以太网数据帧首部的源MAC地址和ARP报文数据区中的源MAC地址是否一致，不一致就丢弃 12sysarp anti-attack packet-check sender-mac 配置ARP表项固化： 12sysarp anti-attack entry-check [ fixed-mac | fixed-all | send-ack ] enable 查看ARP安全配置情况 1display arp anti-attack configuration all +++ 第一次Ping测试出现‘Request Time Out’是正常的，因为第一次通信主机中的ARP高速缓存表不存在其他主机的MAC地址，需要通过ARP协议解析去获得，第二次Ping就会互相正常通信。 五、配置命令 LSW1 12345678910111213sys vlan 1 int g0/0/1 port link-type access # 端口的链路类型为 Access 模式，交换机会把接受到的数据帧直接发送到指定vlanport default vlan 1 # 设置为端口都发送到默认VLAN1中int g0/0/2port link-type accessport default vlan 1int g0/0/3port link-type accessport default vlan 1# 这样做可以使与交换机端口相连的设备同处于一个广播域中，相互通信quit AR1 1234567891011121314sys int g0/0/0 # 配置GE g0/0/0 端口ip address 192.168.1.195 24 # 24代表子网掩码位数，即24位网络地址，8位主机地址dhcp enable # 开启dhcp服务ip pool DHCPForPC # 创建了一个名为DHCPForPC的IP地址池，用于DHCP服务器分配给主机的ip地址network 192.168.1.0 mask 24 # 指定ip地址池的网络地址和子网掩码gateway-list 192.168.1.195 # 指定DHCP服务提供的默认网关地址，与之前配置的接口IP地址相同int g0/0/0 dhcp select global # 指定该接口使用全局 DHCP 配置int g0/0/1ip address 192.168.5.2 24 quitip route-static 192.168.5.0 24 192.168.5.195ip route-static 192.168.3.0 24 192.168.5.195 LSW2 1234567891011121314151617# 作为三层交换机使用，有路由转发功能，所以这里也需要配置路由表，对于左右两个网段来说这是一个ip地址唯一的路由器接口sysvlan 1int vlanif 1ip address 192.168.5.195 24 # 与vlan 1关联的虚拟接口ip地址，实现与该VLAN相关的路由和转发功能int g0/0/1 port link-type accessport default vlan 1int g0/0/2 port link-type accessport default vlan 1int g0/0/3 port link-type accessport default vlan 1quitip route-static 192.168.1.0 24 192.168.5.2ip route-static 192.168.3.0 24 192.168.5.3 AR2 123456789sysint g0/0/0ip address 192.168.5.3 24int g0/0/1ip address 192.168.3.195 24quitip route-static 192.168.1.0 24 192.168.5.195ip route-static 192.168.5.0 24 192.168.5.195 LSW3 123456789sysvlan 3int g0/0/1 port link-type accessport default vlan 3int g0/0/2port link-type accessport default vlan 3quit","link":"/collaboration/%E8%A5%BF%E7%94%B5B%E6%B5%8B%E8%AE%A1%E7%BD%91%E7%BB%BC%E5%90%88%E5%AE%9E%E9%AA%8C/"},{"title":"Common syntax for Typroa","text":"Markdown加粗:1**斜体内容** Ctrl+B 斜体1*斜体内容* Ctrl+I 删除线1~~删除线内容~~ 分割线1--- 代办12- [ ] List item-空格[空格]空格List item 引用1&gt; 高亮1==高亮文本== 标记1`标记文本` 数学公式单独成行公式：1$$+回车 行内公式1$公式编写$ 上下标分式1x^2+y_0+y_{n+1} $x^2+y_0+y_{n+1}$ 1\\frac{y+x}{2} $\\frac{y+x}{2}$ 开方1\\sqrt{2} \\sqrt[3]{2} $\\sqrt{2} \\sqrt[3]{2}$ 累加累乘123\\sum{a} \\sum_{n=1}^{100}{a_n} \\sum\\limits_{n=1}^{100}{a_n}# 类乘\\prod_{n=1}{99}{a_n} $\\sum{a} \\sum_{n=1}^{100}{a_n} \\sum\\limits_{n=1}^{100}{a_n}\\prod_{n=1}{99}{a_n}$ 积分1\\int_0^1f(x)dx $\\int_0^1f(x)dx$ 极限1\\lim_{n\\rightarrow\\infty}{n} $\\lim_{n\\rightarrow\\infty}{n}$ 希腊字母 三角函数 运算符 箭头 连等式123456789\\begin{align} 第一行左式 &amp;= 第一行右式\\\\ 第二行左式 &amp;= 第二行右式\\\\ ...\\end{align}\\begin{align} f(x) &amp;= x^2+2x+1\\\\&amp;=(x+1)^2\\end{align} $\\begin{align}f(x) &amp;= x^2+2x+1\\&amp;=(x+1)^2\\end{align}$ 矩阵123456\\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{bmatrix}\\\\\\begin{matrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{matrix}\\\\\\begin{pmatrix} 0 &amp; -i \\\\ i &amp; 0 \\end{pmatrix}\\\\\\begin{Bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{Bmatrix}\\\\\\begin{vmatrix} a &amp; b \\\\ c &amp; d \\end{vmatrix}\\\\\\begin{Vmatrix} i &amp; 0 \\\\ 0 &amp; -i \\end{Vmatrix} $ \\begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \\end{bmatrix}\\\\begin{matrix} 0 &amp; 1 \\ 1 &amp; 0 \\end{matrix}\\\\begin{pmatrix} 0 &amp; -i \\ i &amp; 0 \\end{pmatrix}\\\\begin{Bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \\end{Bmatrix}\\\\begin{vmatrix} a &amp; b \\ c &amp; d \\end{vmatrix}\\\\begin{Vmatrix} i &amp; 0 \\ 0 &amp; -i \\end{Vmatrix}$","link":"/blog/Typora%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%B3%95%E6%95%B4%E7%90%86/"},{"title":"SOC微体系结构设计","text":"8时钟分频电路 偶数分频器（占空比为50%） 方案一：当计数器计到N/2 - 1时，将输出电平进行一次翻转，同时给计数器一个输出复位信号 123456789101112131415161718-- 6分频为例architecture a of div issignal clk:std_logic:='0';signal count:std_logic_vector(2 downto 0):='000';begin process(clk_in) --原来的时钟频率 begin if(clk_in'enent and clk_in='1') then if count /=2 then --N=6，所以是2，这里是≠2 count=count+1; else clk&lt;= not clk count&lt;='000'; end if; end if; end process; clk_out&lt;=clk;end a; 方案二：当计数器输出为[0 , N/2-1]时，时钟输出为一个值，而为[N/2 , N-1]时，又输出为一个值，计数器为N-1的时候，复位计数器 123456789101112131415161718192021222324252627282930313233architecture b of div issignal count:std_logic_vector(2 downto 0):='000';begin process(clk_in) begin if(clk'event and clk_in='1')then if count&lt;5 then count=count+1; else count = '000'; end if; end if; end process; process(count) begin if(count &lt; 3) then clk_out&lt;='0'; else clk_out&lt;='1'; end if; end process;end b; -- 占空比为0.5的7分频 3/7port( clk1,clk2; cnt1,cnt2;);process(clk_in)begin if(rising_edge(clk1)) then if(cnt&lt;6) then cnt&lt;=cnt+1; else if(cnt&lt;3) then clk='1'; else clk='0'; 串行进位加法器 并行进位加法器 有限状态机 Moore状态机设计：输出信号只和当前状态有关，下一个状态和当前状态以及输入信号有关 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849library IEEEuse ieee.std_logic_1164.allentity more is port(clk,rst:in std_logic; state_input:in std_logic; state_output:out std_logic_vector(1 downto 0); )end more;architecture Behaviorial of more istype states is (st0,st1,st2,st3); -- 定义状态枚举类型signal state:states;begin -- 下一个状态的切换 process(clk,rst) begin if(rst='1')then state&lt;=st0; elsif(clk'event and clk='1')then case state is when st0=&gt; if state_input='0' then state&lt;=st0;else state&lt;=st1; end if; when st1=&gt; if state_input='0' then state&lt;=st1;else state&lt;=st2; end if; when st2=&gt; if state_input='0' then state&lt;=st2;else state&lt;=st3; end if; when st3=&gt; if state_input='0' then state&lt;=st3;else state&lt;=st0; end if; end case; end if; end process;-- 输出信号process(state) begin case state is when st0=&gt; state_output&lt;='00'; when st1=&gt; state_output&lt;='01'; when st2=&gt; state_output&lt;='10'; when st3=&gt; state_output&lt;='11'; end case; end process;end Behavioral; Mealy状态机：输出信号与当前状态和输入都相关 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152library IEEEuse ieee.std_logic_1164.allentity more is port(clk,rst:in std_logic; state_input:in std_logic; state_output:out std_logic_vector(1 downto 0); )end more;architecture Behaviorial of more istype states is(st0,st1,st2,st3); -- 定义状态枚举类型signal state:states;begin -- 下一个状态的切换 process(clk,rst) begin if(rst='1')then state&lt;=st0; elsif(clk'enent and clk='1')then case state is when st0=&gt; if state_input='0' then state&lt;=st0;else state&lt;=st1; end if; when st1=&gt; if state_input='0' then state&lt;=st1;else state&lt;=st2; end if; when st2=&gt; if state_input='0' then state&lt;=st2;else state&lt;=st3; end if; when st3=&gt; if state_input='0' then state&lt;=st3;else state&lt;=st0; end if; end case; end if; end process;-- 输出信号process(state) begin case state is when st0=&gt; if state_input='0' state_output&lt;='00';else state_output&lt;='01';end if; when st0=&gt; if state_input='0' state_output&lt;='00';else state_output&lt;='01';end if; when st1=&gt; if state_input='0' state_output&lt;='01';else state_output&lt;='10';end if; when st2=&gt; if state_input='0' state_output&lt;='10';else state_output&lt;='11';end if; when st3=&gt; if state_input='0' state_output&lt;='11';else state_output&lt;='00';end if; end case; end process;end Behavioral; 按键消抖电路 组合逻辑电路设计 编码器/译码器设计 3-8译码器 123456789101112131415161718192021222324252627282930313233343536373839404142library ieee;use ieee.std_logic_1164.all;entity Decoder3to8 is port ( input : in std_logic_vector(2 downto 0); output : out std_logic_vector(7 downto 0); enable: in std_logic );end entity Decoder3to8;architecture Behavioral of Decoder3to8 isbegin process (input, enable) begin if enable = '0' then case input is when &quot;000&quot; =&gt; output &lt;= &quot;00000001&quot;; when &quot;001&quot; =&gt; output &lt;= &quot;00000010&quot;; when &quot;010&quot; =&gt; output &lt;= &quot;00000100&quot;; when &quot;011&quot; =&gt; output &lt;= &quot;00001000&quot;; when &quot;100&quot; =&gt; output &lt;= &quot;00010000&quot;; when &quot;101&quot; =&gt; output &lt;= &quot;00100000&quot;; when &quot;110&quot; =&gt; output &lt;= &quot;01000000&quot;; when &quot;111&quot; =&gt; output &lt;= &quot;10000000&quot;; when others =&gt; output &lt;= &quot;00000000&quot;; end case; else output &lt;= &quot;00000000&quot;; end if; end process;end architecture Behavioral; 多路选择器设计(4选一数据选择器) 12345678910111213141516171819-- 端口定义PORT（A0：IN STD_LOGIC； A1： IN STD_LOGIC； Data：IN STD_LOGIC_VECTOR(3 DOWNTO 0); EN: IN STD_LOGIC； Y: OUT STD_LOGIC ）;-- 结构体部分： 用WHEN...ELSE语句Y &lt;= Data(0) WHEN A=“00” ELSEData(1) WHEN A=“01” ELSEData(2) WHEN A=“10” ELSEData(3) WHEN A=“11” ELSE‘0’；-- 或者也可以用CASE WHENCASE A IS WHEN “00” =&gt; Y &lt;= Data(0)； WHEN “01” =&gt; Y &lt;= Data(1)； WHEN “10” =&gt; Y &lt;= Data(2)； WHEN “11” =&gt; Y &lt;= Data(3)； WHEN OTHER Y &lt;= ‘0’； 数码转换电路设计 将四位二进制转十进制的BCD码，同时将BCD码再转换成七段显示器码 存储器 RAM(随机存取存储器) 12345678910111213141516171819202122232425262728-- SRAM实现-- 端口定义port(address: in std_logic_vector(3 downto 0); data: inout std_logic_vector(7 downto 0); cs,oe,we:in std_logic);Architecture behav of ram16*8 is subtype word is std_logic_vector(7 downto 0); type ram_array is array(0 to 15) of word; signal index: in integer range 0 to 15; signal sram_store:ram_array;begin index &lt;= CONV_INTEGER(address)process(address,cs,oe,we,data) begin if cs='0' then if we='1' then -- 写入数据 sram_store(index)&lt;=data; elsif oe = '1' then --读出数据 data&lt;=sram_store(index); else data&lt;='zzzzzzzz'; --设置总线为三态 end if; else data&lt;='zzzzzzzz'; end if;end process;end behav; ROM(只读存储器) 123456789101112131415161718192021222324252627282930313233343536373839-- 设计一个256*8bit的ROM 8位地址线，8位数据线，使能oe-- 文件类型引用use ieee.std_logic_textio.alluse std.textio.allentity MyRom is generic(wordlength:integer:=8; addlength:integer:=8)； port( addr:in std_logic_vector(addrlength-1 downto 0); oe:in std_logic; dout: out std_logic_vector(wordlength-1 downto 0) );end entity MyRom; architecture behavior of MyRom is type matrix is array(integer range&lt;&gt;) of std_logic_vector(wordlength-1 downto 0); signal rom:matrix(0 to 2**addlength-1); procedure load_rom(signal data_word:out matrix) is file romfile:text open read_mode is &quot;romfile.dat&quot;; variable lbuf:line; variable i:integer:=0;-- 循环变量 variable fdata:std_logic_vector(7 downto 0);begin -- 读数据直至文件末尾 while not endfile(romfile) loop readline(romfile,lbuf); -- 逐行读数据 read(lbuf,fdata); -- 将行数据转成8位向量，保存到变量fdata中 data_word(i)&lt;=fdata; i:=i+1; end loop;end procedure;begin load_rom(rom); dout&lt;=rom(conv_integer(addr)) when oe='0' else (others=&gt;'z');end behavior; FIFO先进先出：利用双端口RAM和读写地址产生模块 同步控制的FIFO读写时钟相同，异步控制的FIFO读写时钟不同； 与存储器的区别：没有外部读写地址线，数据地址由内部读写指针自动+-完成 判断FIFO空和满： 当wr_ptr=rd_ptr时，FIFO数据为空； 当wr_ptr - rd_ptr = M-1 或者 rd_ptr - wr_ptr = 1时，FIFO数据为满； 当wr_ptr &gt;= rd_ptr时，wr_ptr - rd_ptr为FIFO内部的数据个数； 当wr_ptr &lt;= rd_ptr时 ，M-(rd_ptr - wr_ptr) 为FIFO内数据个数； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576771. 双端口RAMentity dualram isgeneric( width: positive:=8; -- positive代表≥0的整数 depth: positive:=8);port(-- port a 只用来写 clka:in std_logic; wr:in std_logic; addra:in std_logic_vector(depth-1 downto 0); datain:in std_logic_vector(width-1 downto 0); -- port b只用来读 clkb:in std_logic; rd:in std_logic; addrb:in std_logic_vector(depth-1 downto 0); dataout:out std_logic_vector(width-1 downto 0));end entity dualram;architecture Behavioral of dualram istype ram is array(2**depth-1 downto 0) of std_logic_vector(width-1 downto 0);signal dualram:ram;begin process(clka) begin if clka'enent and clka='1' then if wr='0' then dualram(conv_integer(addra))&lt;=datain; end if; end if; end process; process(clkb) begin if clkb'enent and clkb='1' then if rd='0' then dataout&lt;=dualram(conv_integer(addra)); end if; end if; end process; 2. 写地址计数器if rst='0' then wr_pt_t&lt;=(others=&gt;'0');elsif clk'event and clk='1' then if wq='0' then wr_pt_t&lt;=wr_pt_t+1; end if;end if; wr_pt&lt;=wr_pt_r3. 读地址计数器if rst = '0' then rd_pt_t&lt;=(others=&gt;0);elsif clk'event and clk='1' then if rq='0' and empty='0' then -- FIFO不为空 rd_pt_t&lt;=rd_pt_t+1; end if;end if;rd_pt&lt;=rd_pt_t;4. 空满状态产生器if rst='0' then empty&lt;='1';elsif clk'enent and clk='1' then if wr_pt=rd_pt then empty&lt;='1'; else empty&lt;='0';----if rst='0' then full&lt;='0';elsif clk'event and clk='1' then if wr_pt&gt;rd_pt then if(rd_pt+depth)=wr_pt then full&lt;='1'; else full&lt;='0'; else if (wr_pt+1)=rd_pt then full&lt;='1'; else full&lt;='0'; 123456-- 存储单元数据结构整数数组：type memory is array(integer range&lt;&gt;) of integer;位矢量：subtype word is std_logic_vector(k-1 downto 0);type memory is array(0 to 2**w-1) of word; CPU设计 时钟，IR，RN，PC，SP，IO，ALU，微控制器 1.时钟节拍设计1234567891011121314151617181920212223242526272829303132333435363738394041entity clock is Port( clk,rst:in std_logic; clk1,nclk1:out std_logic; --clk clk2,nclk2:out std_logic; --clk二分频 w0,w1,w2,w3:out std_logic --节拍信号 );end clock;architecture Behavioral of clock isbeginprocess(clk)variable count_clk2:integer:=0;variable count_w:integer:=0;begin if(rst='0')then w0&lt;='0'; w1&lt;='0'; w2&lt;='0'; w3&lt;='0'; clk1&lt;='0'; nclk1&lt;='1'; clk2&lt;='0'; nclk2&lt;='1'; count_clk2:=0; count_w:=0; elsif(rst='1')then clk1&lt;=clk; nclk1&lt;=not clk; if(clk'event and clk='1')then if(count_clk2=0)then count_clk2:=1;clk2&lt;='1';nclk2&lt;='0'; elsif(count_clk2=1)then count_clk2:=0;clk2&lt;='0';nclk2&lt;='1'; end if; if(count_w&gt;=0 and count_w&lt;=3)then w0&lt;='1';else w0&lt;='0';end if; if(count_w&gt;=4 and count_w&lt;=7)then w1&lt;='1';else w1&lt;='0';end if; if(count_w&gt;=8 and count_w&lt;=11)then w2&lt;='1';else w2&lt;='0';end if; if(count_w&gt;=12 and count_w&lt;=15)then w3&lt;='1';else w3&lt;='0';end if; if(count_w&lt;15)then count_w:=count_w+1;else count_w:=0;end if; end if; end if;end process;end Behavioral; 2. PC程序计数器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647-- PC 功能分析1. 加1功能2. 更新地址功能3. PC数值送到数据总线entity myPC is port( clk_pc: in std_logic; --PC时钟信号 n_rst: in std_logic; --清零信号 n_ld: in std_logic; --新PC送入，装载新地址 m_pc: in std_logic; --PC值+1控制信号 nPCH, nPCL: in std_logic; --高低位，PC输出总线控制信号 PC: in std_logic_vector(11 downto 0); --PC指针 addr: out std_logic_vector(11 downto 0); --ROM读地址输出 data: inout std_logic_vector(7 downto 0) --PC数值输出到数据总线 );end myPC;architecture Behavioral of PC issignal myPC: std_logic_vector(11 downto 0):=&quot;000000000000&quot;;begin addr &lt;= myPC; process(n_rst, clk_pc, m_pc, n_ld) begin if n_rst='0' then myPC &lt;= &quot;000000000000&quot;; --清零 data &lt;= &quot;ZZZZZZZZ&quot;; --数据总线高阻态 elsif clk_pc'event and clk_pc='1'then if m_pc='1' then --PC值+1 myPC &lt;= myPC+1; elsif n_ld='0' then --送入新PC myPC&lt;=PC; end if; end if; end process; process(nPCH, nPCL) begin if nPCH='0' and nPCL='1' then data(3 downto 0) &lt;= myPC(11 downto 8); --高四位输入到数据总线 data(7 downto 4) &lt;= &quot;0000&quot;; elsif nPCL='0' and nPCH='1' then --低位低电平，高位高电平有效 data(7 downto 0) &lt;= myPC(7 downto 0); --低八位输入到数据总线 else data &lt;= &quot;ZZZZZZZZ&quot;; --数据总线高阻态 end if; end process;end Behavioral; 3. 程序存储器ROM12345678910111213141516171819202122232425262728293031323334353637383940use IEEE.STD_LOGIC_TEXTIO.ALL; -- 文件操作的库use STD.TEXTIO.ALL;entity MyRom is generic( wordlength:integer:=8; -- 位宽度 addrlength:integer:=8 -- 地址位 ); Port ( addr:in std_logic_vector(addrlength-1 downto 0); -- ROM地址信号 oe:in std_logic; -- ROM使能 dout:inout std_logic_vector(wordlength-1 downto 0) -- 数据总线 );end MyRom;architecture Behavioral of MyRom istype matrix is array(integer range&lt;&gt;) of std_logic_vector(7 downto 0);signal rom:matrix(2**addrlength-1 downto 0);procedure load_rom(signal data_word:out matrix) is --过程调用的参数 file romfile:text open read_mode is &quot;C:\\Users\\akyna\\Codes\\vivado\\rom\\romfile.dat&quot;; --file romfile:text; --file_open(romfile,file_in,&quot;romfile.txt&quot;,read_mode); variable lbuf:line; variable i:integer:=0; variable fdata:std_logic_vector(7 downto 0);begin while not endfile(romfile) loop readline(romfile,lbuf); read(lbuf,fdata); -- 将每一行数据存入fdata data_word(i)&lt;=fdata; i:=i+1; exit when i=256; end loop;end procedure;begin load_rom(rom); dout&lt;=rom(conv_integer(addr))when oe='0' else &quot;ZZZZZZZZ&quot;;end Behavioral; 4. 指令存储器IR设计 12345678910111213141516171819202122232425262728293031323334353637383940414243444546-- IR功能分析1. 传送指令编码到微控制器2. 生成PC新地址3. 生成RAM读写地址entity IR is port(clk_IR:in std_logic; -- IR时钟信号 nrst:in std_logic; -- 复位信号 LD_IR1,LD_IR2,LD_IR3: in std_logic; -- IR指令存储控制信号 nAren:in std_logic; -- IR中RAM地址控制信号 data:inout std_logic_vector(7downto 0)-- 数据总线 IR:out std_logic_vector(7 downto 2); -- IR指令编码 PC:out std_logic_vector(11 downto 0); -- PC新地址 AR:out std_logic_vector(6 downto 0); -- RAM读写地址 RS:out std_logic; -- 源寄存器 RD:out std_logic; -- 目的寄存器 );end IR;architecture behavior od IR is begin process(clk_IR,rst) begin if nrst='0' then IR&lt;=(others=&gt;'0'); PC&lt;=(others=&gt;'0'); AR&lt;=(others=&gt;'0'); RS&lt;='0'; RD&lt;='0'; elsif clk'event and clk='1' then if(LD_IR1='1') then IR&lt;=data(7 downto 2); RS&lt;=data(0); RD&lt;=data(1); end if; -- 生成PC地址 if(LD_IR2='1') then PC(11 downto 8)&lt;=data(3 downto 0); end if; if(LD_IR3='1') then PC(7 downto 0)&lt;=data(7 downto 0); end if; if(LD_IR3='1' and nAren='0') then AR&lt;=data(6 downto 0); end if; end if; end process;end behavoir; 5. 寄存器RN设计 123456789101112131415161718192021222324252627-- RN功能分析1.数据锁存2.数据读写entity RN isPort ( clk_RN: in std_logic; -- RN时钟信号 nreset: in std_logic; -- 复位信号 Ri_EN: in std_logic; -- RN寄存器使能 RDRi: in std_logic; -- RN读信号 WRRi: in std_logic; -- RN写信号 RS: in std_logic; -- 源寄存器 RD: in std_logic; -- 目的寄存器 data : inout std_logic_vector(7 downto 0) -- 数据总线 );architecture behavioral of RN istype Rdata is array(7 downto 0) of std_logic_vector(7 downto 0);signal RDD:Rdata;begin process(clk_RN) begin if(rising_edge(clk_RN) and RN_EN='0') then if RDRi='1' then data&lt;=RDD(conv_integer(RS)); elsif WRRi='1' then RDD(conv_integer(RD))&lt;=data; end if; end if; end process;end Behavioral; 6. 数据存储器RAM设计 1234567891011121314151617181920212223242526272829303132333435-- RAM功能分析1. 数据存储功能2. 数据读写功能entity RAM_m isPort ( clk_RAM: in std_logic; n_reset: in std_logic; RAM_CS: in std_logic; nRAM_EN: in std_logic; -- RAM输出使能信号 wr_nRD: in std_logic; -- 读写控制，高电平写有效，低电平读有效 AR: in std_logic_vector(6 downto 0); -- RAM地址信号 data:inout std_logic_vector(7 downto 0); -- 数据总线 );end RAM_m;architecture Behavioral of RAM_m istype max is array(integer range&lt;&gt;) of std_logic_vector(7 downto 0); --定义数据类型signal tmp: max(0 to 2**7-1); --定义ram空间beginprocess(clk_RAM)begin if(clk_RAM'event and clk_RAM = '1')then if(n_reset = '0')then data &lt;= &quot;00000000&quot;; else -- 读数据 if(RAM_CS = '1'and wr_nRD = '0' and nRAM_EN = '0')then data &lt;= tmp(conv_integer(AR)); -- 写数据 elsif(RAM_CS = '1' and wr_nRD = '1')then tmp(conv_integer(AR)) &lt;= data; end if; end if; end if;end process;end Behavioral; 7. 堆栈指针SP设计 1234-- SP功能分析1. 数据存储功能2. 加1功能：出栈3. 减1功能：压栈 8. IO端口设计","link":"/collaboration/soc%E5%A4%8D%E4%B9%A0/"}],"tags":[{"name":"Computer Network","slug":"Computer-Network","link":"/tags/Computer-Network/"},{"name":"Collaboration Project","slug":"Collaboration-Project","link":"/tags/Collaboration-Project/"},{"name":"Embedded System","slug":"Embedded-System","link":"/tags/Embedded-System/"},{"name":"Eletromagnetic Physics","slug":"Eletromagnetic-Physics","link":"/tags/Eletromagnetic-Physics/"},{"name":"Microcomputer","slug":"Microcomputer","link":"/tags/Microcomputer/"},{"name":"Project Tools","slug":"Project-Tools","link":"/tags/Project-Tools/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Computer Vision","slug":"Computer-Vision","link":"/tags/Computer-Vision/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Data Visualization","slug":"Data-Visualization","link":"/tags/Data-Visualization/"},{"name":"OpenSSL","slug":"OpenSSL","link":"/tags/OpenSSL/"},{"name":"Mathematical Modeling","slug":"Mathematical-Modeling","link":"/tags/Mathematical-Modeling/"},{"name":"Project Habits","slug":"Project-Habits","link":"/tags/Project-Habits/"},{"name":"Deep Reinforcement Learning","slug":"Deep-Reinforcement-Learning","link":"/tags/Deep-Reinforcement-Learning/"},{"name":"Professional Knowledge","slug":"Professional-Knowledge","link":"/tags/Professional-Knowledge/"},{"name":"Research Habits","slug":"Research-Habits","link":"/tags/Research-Habits/"},{"name":"Life Knowledge","slug":"Life-Knowledge","link":"/tags/Life-Knowledge/"},{"name":"Computer Principle","slug":"Computer-Principle","link":"/tags/Computer-Principle/"},{"name":"College Life","slug":"College-Life","link":"/tags/College-Life/"},{"name":"Health","slug":"Health","link":"/tags/Health/"},{"name":"Life Wisdom","slug":"Life-Wisdom","link":"/tags/Life-Wisdom/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Literature Survey","slug":"Literature-Survey","link":"/tags/Literature-Survey/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","link":"/tags/Reinforcement-Learning/"},{"name":"Eletromagnetic B测","slug":"Eletromagnetic-B测","link":"/tags/Eletromagnetic-B%E6%B5%8B/"}],"categories":[{"name":"collaboration","slug":"collaboration","link":"/collaboration/"},{"name":"blog","slug":"blog","link":"/blog/"},{"name":"knowledge","slug":"knowledge","link":"/knowledge/"},{"name":"projects","slug":"projects","link":"/projects/"},{"name":"publications","slug":"publications","link":"/publications/"},{"name":"life","slug":"life","link":"/life/"},{"name":"readings","slug":"readings","link":"/readings/"}],"pages":[{"title":"","text":"MY CV About MeHi, I’m Jiawei Hu. This is a brief introduction about me.I am a junior student at Xidian University, and my favorite research direction is robotics and artificial intelligence. As a new blogger, I am very excited to share with you the learning experience and technical points in the major, and discuss the tricky problems in the technical field together. Development PlanningI am a person who has many specific plans about the development of the future. There are two basic principles that I formed in my growth. The one is that I very hate wasting time and the other is that I very advocate gathering my distracted time and energy. Therefore, I develop this module to show my personal plans on my future just as my personal characteristics to be known for you. Development Goal: Become an advanced researcher in some field. First Stage: Determine the research field to be work on in the future (Spend at least a month, completed) Second Stage: Prepare myself with enough knowledge to explore about this research field (Spend at least one or two years, currently working) Third Stage: Explore as much as possible to find intereting things to promote the scientific research (Spend as many as possible years, to be completed) Competitions August, 2023: National First Prize of China College Student Service Outsourcing Innovation and Entrepreneurship Competition(Top 0.3%) November, 2023: National Second Prize of China Undergraduate Mathematical Contest in Modeling (CUMCM)(Top &lt;1.53%) June, 2023: Top prize of Shaanxi Province Higher Mathematics Competition for College Students January, 2023: Provincial First Prize in National Mathematics Competition for University Students Honors 2022: Second class scholarship of Xidian University 2023: National inspirational Scholarship 2023: Huawei “Zhi Neng Ji Zuo” scholarship Awards Technology Stack Programming Languages: Python, C++, Matlab, C (Arrange In Order) Mastered Libraries: Pandas, Numpy, Sklearn, Matplotlib, Pytorch Frame Grasped Tools: Git, Github, MySQL, Linux Commands","link":"/index.html"}]}