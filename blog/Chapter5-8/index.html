<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Chapter 5 to 8 - Jiawei Hu - Jiawei Hu&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Jiawei Hu - Jiawei Hu&#039;s Blog"><meta name="msapplication-TileImage" content="/img/logo.png"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jiawei Hu - Jiawei Hu&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="This is an article about Deep Learning Practice from Chapter 5 to 8."><meta property="og:type" content="blog"><meta property="og:title" content="Chapter 5 to 8"><meta property="og:url" content="https://jiaweihu-xdu.github.io/blog/Chapter5-8/"><meta property="og:site_name" content="Jiawei Hu - Jiawei Hu&#039;s Blog"><meta property="og:description" content="This is an article about Deep Learning Practice from Chapter 5 to 8."><meta property="og:locale" content="en_US"><meta property="og:image" content="https://jiaweihu-xdu.github.io/img/og_image.png"><meta property="article:published_time" content="2023-09-04T13:09:32.000Z"><meta property="article:modified_time" content="2023-12-15T06:52:27.788Z"><meta property="article:author" content="Jiawei Hu"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Machine Learning"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://jiaweihu-xdu.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://jiaweihu-xdu.github.io/blog/Chapter5-8/"},"headline":"Chapter 5 to 8","image":["https://jiaweihu-xdu.github.io/img/og_image.png"],"datePublished":"2023-09-04T13:09:32.000Z","dateModified":"2023-12-15T06:52:27.788Z","author":{"@type":"Person","name":"Jiawei Hu"},"publisher":{"@type":"Organization","name":"Jiawei Hu - Jiawei Hu's Blog","logo":{"@type":"ImageObject","url":"https://jiaweihu-xdu.github.io/img/logo.png"}},"description":"This is an article about Deep Learning Practice from Chapter 5 to 8."}</script><link rel="canonical" href="https://jiaweihu-xdu.github.io/blog/Chapter5-8/"><link rel="icon" href="/img/logo.png"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/6.0.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body class="is-3-column"><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Jiawei Hu - Jiawei Hu&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">JIAWEI HU&#039;S LOG BOOK</a><a class="navbar-item" href="/blog">BLOG</a><a class="navbar-item" href="/knowledge">KNOWLEDGE</a><a class="navbar-item" href="/publications">PUBLICATIONS</a><a class="navbar-item" href="/projects">PROJECTS</a><a class="navbar-item" href="/life">LIFE</a><a class="navbar-item" href="/readings">READINGS</a><a class="navbar-item" href="/collaboration">COLLABORATION</a><a class="navbar-item" href="/archives">ARCHIVES</a><a class="navbar-item" href="/categories">CATEGORIES</a><a class="navbar-item" href="/tags">TAGS</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/jiaweiHu-XDU/jiaweiHu-XDU.github.io"><i class="fab fa-github"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Gitee" href="https://gitee.com/hujiawe_i"><i class="fa-brands fa-gofore"></i></a><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">Chapter 5 to 8</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2023-09-04T13:09:32.000Z" title="2023-09-04T13:09:32.000Z">2023-09-04</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="2023-12-15T06:52:27.788Z" title="2023-12-15T06:52:27.788Z">2023-12-15</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/blog/">blog</a></span><span class="level-item"><i class="far fa-clock"></i> 21 minutes read (About 3218 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><h3 id="Chapter-5-Logistic-Regression"><a href="#Chapter-5-Logistic-Regression" class="headerlink" title="Chapter 5 Logistic Regression"></a>Chapter 5 Logistic Regression</h3><p>Formal definitions: for some data samples ${&lt;\boldsymbol{x}^{(n)}, y^{(n)}&gt;}_{n=1}^N$, we can get a predictive model by statistical analysis. The model we have trained will output a preferable prediction value:</p>
<p>$$<br>y=f(\boldsymbol{x})<br>$$</p>
<p>for a given test data $\boldsymbol{x}={x_1, x_2, \cdots, x_n}$.</p>
<h4 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h4><p>Define: $f(\cdot)$ is regression model in linear combinations</p>
<p>$$<br>y = f(\boldsymbol{x})=\boldsymbol{w}_T\boldsymbol{x}+b\<br>=w_1x_1+\cdots+w_nx_n+b<br>$$</p>
<p>Optimization goal：</p>
<p>$$<br>\min\limits_{w,\ b} \ |f(\boldsymbol{x}^{(n)})-y^{(n)}|<br>$$</p>
<p>Loss Function( Mean squared error)：</p>
<p>$$<br>E = \sum\limits_n[y^{(n)}-(\boldsymbol{w}^T\boldsymbol{x}^{(n)}+b)]^2<br>$$</p>
<p>Assumption of the selection error function: The data sample points satisfy the normal distribution with the predicted values of the selected linear regression model as the mean, which is converted to a parametric estimation problem.<br>Define the conditional probability, which represents the probability value of the model prediction output as $y$ given the independent variables:</p>
<p>$$<br>p(y|\boldsymbol{x})\thicksim N(\boldsymbol{w}^T\boldsymbol{x}+b, \sigma^2)<br>$$</p>
<p>Derive the likelihood function of the conditional probability:</p>
<p>$$<br>p(y|\boldsymbol{x}) :\ \ \boldsymbol{L}(\boldsymbol{w}, b)=\prod\limits_n\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{1}{2\sigma^2}(y^{(n)}-\boldsymbol{w}^T\boldsymbol{x}^{(n)}-b)^2)<br>$$</p>
<p>$$<br>\Rightarrow \ln\boldsymbol{L}(\boldsymbol{w}, b)=n\ln(\frac{1}{\sqrt{2\pi}\sigma})+\ln(\prod\limits_n\exp(-\frac{1}{2\sigma^2}(y^{(n)}-\boldsymbol{w}^T\boldsymbol{x}^{(n)}-b)^2)<br>$$</p>
<p>$$<br>=n\ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{1}{2\sigma^2}\sum\limits_n(y^{(n)}-\boldsymbol{w}^T\boldsymbol{x}^{(n)}-b)^2<br>$$</p>
<p>$$<br>\frac{\partial{\ln\boldsymbol{L}(\boldsymbol{w}, b)}}{\partial{\boldsymbol{w}}}=0, \ \ \frac{\partial{\ln\boldsymbol{L}(\boldsymbol{w}, b)}}{\partial{b}}=0<br>$$</p>
<p>$$<br>\Rightarrow \boldsymbol{w}, b=\underset{\boldsymbol{w},\ b}{\arg \max}\ \ \boldsymbol{L}(\boldsymbol{w}, b)<br>$$</p>
<p>$$<br>=\underset{\boldsymbol{w},\ b}{\arg \min}\ \ \sum\limits_n(y^{(n)}-\boldsymbol{w}^T\boldsymbol{x}^{(n)}-b)^2<br>$$</p>
<p>If matrix operations are used, the error function is represented in vector form:</p>
<p>$$<br>\boldsymbol{Y}=\left[y^{(1)},y^{(2)},\cdots,y^{(n)}\right]’,\ \boldsymbol{w}=(w_1, w_2, \cdots, w_n)<br>$$</p>
<p>$$<br>\boldsymbol{X}=\left[\boldsymbol{x}_1, \boldsymbol{x}_2,\cdots, \boldsymbol{x}_m\right],\ \boldsymbol{x_i}=(x_i^{(1)}, x_i^{(2)}, \cdots, x_i^{(n)})’<br>$$</p>
<p>$$<br>\boldsymbol{b}=\left[b_1, b_2, \cdots, b_n\right]’, \ \ b_1=b_2=\cdots=b_n<br>$$</p>
<p>$$<br>\Longrightarrow \boldsymbol{E}=(\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{w}^T-\boldsymbol{b})^T(\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{w}^T-\boldsymbol{b})<br>$$</p>
<p>if we combine data samples weights $w$ with parameters $b$, that is:</p>
<p>$$<br>\boldsymbol{w}=(w_1, w_2, \cdots, w_n, b)<br>$$</p>
<p>$$<br>\boldsymbol{X}=\left[\boldsymbol{x}_1, \boldsymbol{x}_2,\cdots, \boldsymbol{x}_m,\boldsymbol{1}\right],\ \boldsymbol{x_i}=(x_i^{(1)}, x_i^{(2)}, \cdots, x_i^{(n)})’,\ \boldsymbol{1} = [1,1,\cdots,1]’<br>$$</p>
<p>$$<br>\Longrightarrow \boldsymbol{E}=(\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{w}^T)^T(\boldsymbol{Y}-\boldsymbol{X}\boldsymbol{w}^T)<br>$$</p>
<p>$$<br>\Longrightarrow\frac{\partial{E}}{\partial{\boldsymbol{w}}}=2\boldsymbol{X}^T(\boldsymbol{X}\boldsymbol{w}^T-\boldsymbol{Y})=0<br>$$</p>
<p>$$<br>\Longrightarrow\boldsymbol{w}^T=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y}<br>$$</p>
<p>$$<br>\Longrightarrow y=\boldsymbol{x}\boldsymbol{w}^T=\boldsymbol{x}(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y}<br>$$</p>
<h4 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h4><p>If there exists a nonlinear relationship between random variables ${x_i}(i=1,2,\cdots,n)$ and $y$, a generalized linear regression model can be defined by a nonlinear transformation $g(\cdot)$:</p>
<p>$$<br>y\mathop{\longrightarrow}\limits^{g(\cdot)} g(y);\ \boldsymbol{x}\mathop{\longrightarrow}\limits^{f(\cdot)} g(y)<br>$$</p>
<p>$$<br>f(\boldsymbol{x})=g(y)\Rightarrow y=g^{-1}(f(\boldsymbol{x}))<br>$$</p>
<p>【Example of binary classification】<br>In binary classification tasks, the goal is to fit a separating hyperplane.</p>
<p>$$<br>y=g^{-1}(f(\boldsymbol{x}))=0,\ f(\boldsymbol{x})&lt;0\ \ |\ \<br>1,\ f(\boldsymbol{x})&gt;0.<br>$$</p>
<p>To ensure differentiability of $g(\cdot)$, a function $\sigma(\cdot)$ is chosen to approximate the discontinuous step function, such as the logistic function:</p>
<p>$$<br>y=\sigma(f(\boldsymbol{x}))=\frac{1}{1+\exp(-\boldsymbol{w}^T\boldsymbol{x}-b)}<br>$$</p>
<p>$$<br>p(y=1|\boldsymbol{x})=\sigma(f(\boldsymbol{x}))<br>$$</p>
<p>$$<br>p(y=0|\boldsymbol{x})=1-\sigma(f(\boldsymbol{x}))<br>$$</p>
<p>$$<br>g^{-1}(x)=\sigma(x)\Rightarrow \sigma(g(y))=y=\frac{1}{1+e^{-f(\boldsymbol{x})}}=\frac{1}{1+e^{-g(y)}}<br>$$</p>
<p>$$<br>\Longrightarrow g(y)=\log\frac{y}{1-y}=\log\frac{p(y=1|\boldsymbol{x})}{p(y=0|\boldsymbol{x})}=f(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+b<br>$$</p>
<p>Parameter estimation: Maximum likelihood estimation is used to estimate the conditional probability $p(y|\boldsymbol{x};\boldsymbol{w},b)$.<br>Likelihood Function：</p>
<p>$$<br>\boldsymbol{L}(\boldsymbol{w}, b)=\prod\limits_n[\sigma(f(\boldsymbol{x}^{(n)}))]^{y^{(n)}}[1-\sigma(f(\boldsymbol{x}^{(n)}))]^{1-y^{(n)}}<br>$$</p>
<p>$$<br>\Longrightarrow \ln\boldsymbol{L}(\boldsymbol{w}, b)=\ln(\prod\limits_n[\sigma(f(\boldsymbol{x}^{(n)}))]^{y^{(n)}}[1-\sigma(f(\boldsymbol{x}^{(n)}))]^{1-y^{(n)}})<br>$$</p>
<p>$$<br>=\sum\limits_n[y^{(n)}\log(\sigma(f(\boldsymbol{x}^{(n)})))+(1-y^{(n)})\log(1-\sigma(f(\boldsymbol{x}^{(n)})))]<br>$$</p>
<h4 id="Pytorch-Logistic-Regression"><a href="#Pytorch-Logistic-Regression" class="headerlink" title="Pytorch Logistic Regression"></a>Pytorch Logistic Regression</h4><p>This is a demo for Logistic Regression Learning. Firstly, we need to prepare corresponding environment by importing some libraries.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> MultivariateNormal</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>

<h5 id="【Prepare-Data】"><a href="#【Prepare-Data】" class="headerlink" title="【Prepare Data】"></a>【Prepare Data】</h5><p>We set Normal Distribution’s mean vector and covariance matrix.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">mu1 = torch.ones(<span class="number">2</span>) * -<span class="number">3</span>   <span class="comment"># two variables</span></span><br><span class="line">mu2 = torch.ones(<span class="number">2</span>) * <span class="number">3</span></span><br><span class="line">sigma1 = torch.eye(<span class="number">2</span>) * <span class="number">0.5</span></span><br><span class="line">sigma2 = torch.eye(<span class="number">2</span>) * <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set Normal Distributions Object</span></span><br><span class="line">m1 = MultivariateNormal(mu1, sigma1)</span><br><span class="line">m2 = MultivariateNormal(mu2, sigma2)</span><br><span class="line"><span class="comment"># sampling</span></span><br><span class="line">x1 = m1.sample((<span class="number">100</span>,))</span><br><span class="line">x2 = m2.sample((<span class="number">100</span>,))</span><br><span class="line"></span><br><span class="line"><span class="comment"># set label value</span></span><br><span class="line">y = torch.zeros((<span class="number">200</span>, <span class="number">1</span>))</span><br><span class="line">y[<span class="number">100</span>:] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># combine and scramble samples</span></span><br><span class="line">x = torch.cat([x1, x2], dim=<span class="number">0</span>)</span><br><span class="line">idx = np.random.permutation(<span class="built_in">len</span>(x))</span><br><span class="line">x = x[idx]</span><br><span class="line">y = y[idx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># visualization</span></span><br><span class="line">plt.scatter(x1.numpy()[:,<span class="number">0</span>], x1.numpy()[:,<span class="number">1</span>])</span><br><span class="line">plt.scatter(x2.numpy()[:,<span class="number">0</span>], x2.numpy()[:,<span class="number">1</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h4 id="【Prepare-Pytorch-Linear-Model】"><a href="#【Prepare-Pytorch-Linear-Model】" class="headerlink" title="【Prepare Pytorch Linear Model】"></a>【Prepare Pytorch Linear Model】</h4><p>In the class <code>torch.nn</code>, <code>Linear</code> object achieves $y=x\boldsymbol{x}^T+b$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set features number</span></span><br><span class="line">D_in, D_out = <span class="number">2</span>, <span class="number">1</span></span><br><span class="line">linear = nn.Linear(D_in, D_out, bias=<span class="literal">True</span>)</span><br><span class="line">output = linear(x)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.shape, linear.weight.shape, linear.bias.shape, output.shape)</span><br></pre></td></tr></table></figure>

<h5 id="【Pytorch-Activation-Function】"><a href="#【Pytorch-Activation-Function】" class="headerlink" title="【Pytorch Activation Function】"></a>【Pytorch Activation Function】</h5><p>Logistic Regression used for binary classification problem will ultilize <code>torch.nn.Sigmoid()</code> fanction to map the result which linear model have calculated to $0\thicksim 1$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sigmoid = nn.Sigmoid()</span><br><span class="line">scores = sigmoid(output)</span><br></pre></td></tr></table></figure>

<h5 id="【Loss-Function】"><a href="#【Loss-Function】" class="headerlink" title="【Loss Function】"></a>【Loss Function】</h5><p>Logistic Regression uses cross-entropy as its loss function. <code>Torch.nn</code> provides many standard loss function and we can directly use <code>torch.nn.BCELoss</code> to calculate binary cross-entropy loss.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.BCELoss()</span><br><span class="line"><span class="built_in">print</span>(loss(sigmoid(output), y))</span><br></pre></td></tr></table></figure>

<h5 id="【Reconstruct-our-model】"><a href="#【Reconstruct-our-model】" class="headerlink" title="【Reconstruct our model】"></a>【Reconstruct our model】</h5><p>In Pytorch, we can inherit <code>nn.Module</code> to build our own model, but what we need to notice is that <code>forward()</code> method must be overwritten by subclasses.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegression</span>(nn.Module):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, D_in</span>):</span><br><span class="line">		<span class="built_in">super</span>(LogisticRegression, self).__init__()</span><br><span class="line">		self.linear = nn.Linear(D_in, <span class="number">1</span>)</span><br><span class="line">		self.sigmoid = nn.Sigmoid()</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">format</span>(<span class="params">self, x</span>):</span><br><span class="line">		x = self.linear(x)</span><br><span class="line">		output = self.sigmoid(x)</span><br><span class="line">		<span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">lr_model = LogisticRegression(<span class="number">2</span>)</span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line">loss(lr_model(x), y)</span><br></pre></td></tr></table></figure>

<h5 id="【Optimization-Algorithm】"><a href="#【Optimization-Algorithm】" class="headerlink" title="【Optimization Algorithm】"></a>【Optimization Algorithm】</h5><p>Logistic regression typically optimizes the objective function using gradient descent. Pytorch’s <code>torch.optim</code> package implements most commonly used optimization algorithms.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(lr_model.parameters(), lr=<span class="number">0.03</span>)</span><br></pre></td></tr></table></figure>

<p>After constructing optimizer, we can train the model iteratively. There are two main steps here, one is calling the <code>backward()</code> method of the loss function to calculate the model gradient, the other is calling the <code>step()</code> method of the optimizer to update the model parameters. It should be noted that we need to call the <code>zero_grad()</code> method of the optimizer to clear out parameters’ gradient firstly.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">iters = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(lex(x)/batch_size)):</span><br><span class="line">		<span class="built_in">input</span> = x[i*batch_size:(i+<span class="number">1</span>)*batch_size]</span><br><span class="line">		target = y[i*batch_size:(i+<span class="number">1</span>)*batch_size]</span><br><span class="line">		optimizer.zero_grad()</span><br><span class="line">		output = lr_model(<span class="built_in">input</span>)</span><br><span class="line">		l = loss(output, target)</span><br><span class="line">		l.backward()</span><br><span class="line">		optimizer.step()</span><br></pre></td></tr></table></figure>

<h3 id="Chapter-6-Neural-Network-Basics"><a href="#Chapter-6-Neural-Network-Basics" class="headerlink" title="Chapter 6 Neural Network Basics"></a>Chapter 6 Neural Network Basics</h3><h4 id="I-Basic-characteristics-of-neural-networks"><a href="#I-Basic-characteristics-of-neural-networks" class="headerlink" title="I. Basic characteristics of neural networks"></a>I. Basic characteristics of neural networks</h4><ul>
<li>Connectionist model based on statistics</li>
<li>Have the basic unit for processing signals</li>
<li>Processing units are connected in parallel with each other</li>
<li>There are the connection weights between processing units</li>
</ul>
<h5 id="【Neuron】"><a href="#【Neuron】" class="headerlink" title="【Neuron】"></a>【Neuron】</h5><p>$$<br>out=f(w_1x_1+w_2x_2+\cdots+w_nx_n+b)<br>$$</p>
<p>A neuron receives a set of tensors as input: $\boldsymbol{x}={x_1,x_2,\cdots,x_n}^T$, connection weights $\boldsymbol{w}={w_1,w_2,\cdots,w_n}$, then performs a weighted summation：</p>
<p>$$<br>sum=\sum_iw_ix_i=\boldsymbol{w}\boldsymbol{x}<br>$$</p>
<p>Sometimes, the weighted summation of neurons is also accompanied by a constant term $b$ as a bias:</p>
<p>$$<br>sum=\boldsymbol{w}\boldsymbol{x}+b<br>$$</p>
<p>Activation Function $f(\cdot)$ is applied to the input weighted $sum$ to produce the output of the neuron; if the order of $sum$ is greater than 1, then $f(\cdot)$ is applied to each element of $sum$ .</p>
<h5 id="【Activation-Function】"><a href="#【Activation-Function】" class="headerlink" title="【Activation Function】"></a>【Activation Function】</h5><ul>
<li>$softmax()\ \ function$<br>It is suitable for multivariate classification problems, and the function is to normalize n scalars representing n classes respectively to obtain the probability distribution of these n classes.</li>
</ul>
<p>$$<br>softmax(x_i)=\frac{\exp(x_i)}{\sum_j\exp(x_j)}<br>$$</p>
<ul>
<li>$sigmoid()\ \ function$<br>It is also usually expressed as $logitic()\ \ function$, suitable for binary classification problems, and is a binary version of $softmax$.</li>
</ul>
<p>$$<br>\sigma(x) = \frac{1}{1+\exp(-x)}<br>$$</p>
<p>that is, for two classes: $x_1, x_2$, there is the equation:</p>
<p>$$<br>softmax(x_1)=\frac{\exp(x_1)}{\exp(x_1)+\exp(x_2)}<br>=\frac{1}{1+\exp(x_2-x_1)}<br>$$</p>
<p>$$<br>P(x_1=1)=sigmoid(x_1)=\frac{1}{1+\exp(-x_1)}(x_2=0)<br>$$</p>
<ul>
<li>$Tanh()\ \ function$<br>It is a variant of $logistic()\ \ function$.</li>
</ul>
<p>$$<br>tanh(x) = \frac{2\sigma(x)-1}{2\sigma^2(x)-2\sigma(x)+1}<br>$$</p>
<ul>
<li>$ReLU()\ \ function(Rectified\ \ linear\ \ unit)$<br>This function only has the half of the range which is active and it can effectively avoid the problem of vanishing gradient.</li>
</ul>
<p>$$<br>ReLU(x) = \max(0,x)<br>$$</p>
<h5 id="【Output-Layer】"><a href="#【Output-Layer】" class="headerlink" title="【Output Layer】"></a>【Output Layer】</h5><p>The output of the activation function is just the output of the neuron. A neuron can have multiple outputs $o_1, o_2,\cdots,o_m$, but corresponding to different activation functions $f_1,f_2,\cdots,f_m$.</p>
<h5 id="【Neural-Network】"><a href="#【Neural-Network】" class="headerlink" title="【Neural Network】"></a>【Neural Network】</h5><p>A neural network is a directed graph and a computational graph.</p>
<h4 id="II-Perceptron"><a href="#II-Perceptron" class="headerlink" title="II. Perceptron"></a>II. Perceptron</h4><h5 id="【Monolayer-Perceptron】"><a href="#【Monolayer-Perceptron】" class="headerlink" title="【Monolayer Perceptron】"></a>【Monolayer Perceptron】</h5><p>Consider there is a neuron, which has two inputs $x_1,x_2$ with weights $w_1,w_2$. And we use symbolic function as activation function:</p>
<p>$$<br>f(x) = sgn(x)=-1,\ x&lt;0\ |\ 1,\ x\geq 0<br>$$</p>
<p>In training process, weights will be updated according to the following method:</p>
<p>$$<br>w’\leftarrow + \alpha \cdot (y-o)\cdot x<br>$$</p>
<p>About the above parameters, $o$ means activation value, $y$ is expected objective value, $\alpha$ is learning rate.</p>
<h5 id="【Multi-Layer-Perceptron-MLP】"><a href="#【Multi-Layer-Perceptron-MLP】" class="headerlink" title="【Multi-Layer Perceptron, MLP】"></a>【Multi-Layer Perceptron, MLP】</h5><p>We can take an example to specifically understand how different the MLP is. Supposing that we choose XOR function as model’s activation function:</p>
<p>$$<br>f(x_1,x_2)=0,\ x_1=x_2\ \ |\ \ 1,\ x_1\neq x_2<br>$$</p>
<p>And in this case, as the equation described, we just need two inputs to get our activate value. But how can we achieve XOR operation by using some neurons? As mentioned above, Monolayer Perception can fit a hyperplane $y=ax_1+bx_2$. And this is suitable for linearly separable problems. That’s because our mapping relationship is linear. So if we use one more layer, there are maybe good changes in mapping relationship between inputs and outputs.<br>Now we set two hidden layer neurons $h_1, h_2$, and then we can get new equations:</p>
<p>$$<br>h_1=w_{11}x_1+w_{21}x_2,\ \ h_2=w_{12}x_1+w_{22}x_2<br>$$</p>
<p>$$<br>y=0,\ h_1=h_2\ \ |\ \ 1,\ h_1\neq h_2<br>$$</p>
<h4 id="III-BP-Neural-Network"><a href="#III-BP-Neural-Network" class="headerlink" title="III. BP Neural Network"></a>III. BP Neural Network</h4><p>Define: Back Propagation(BP) algorithm propagates the error backward from the output layer to the front layer, and uses the error of the latter layer to estimate the error of the previous layer.</p>
<h5 id="【Gradient-Descent】"><a href="#【Gradient-Descent】" class="headerlink" title="【Gradient Descent】"></a>【Gradient Descent】</h5><p>In order to ensure the error back propagation, we utilize gradient descent algorithm to search in the weight space int the direction of the fastest error decline.</p>
<p>$$<br>w\leftarrow w+\Delta w<br>$$</p>
<p>$$<br>\Delta w=-\alpha\triangledown Loss(w)=-\alpha\frac{\partial{Loss}}{\partial{w}}<br>$$</p>
<p>Commonly used loss functions</p>
<ul>
<li>Mean Squared Error, MSE</li>
</ul>
<p>$$<br>Loss(o, y)=\frac{1}{n}\sum\limits_{i=1}^n|o_i-y_i|^2<br>$$</p>
<ul>
<li>Cross Entropy, CE</li>
</ul>
<p>$$<br>Loss(x_i)=-\log(\frac{\exp(x_i)}{\sum_j\exp(x_j)})<br>$$</p>
<h5 id="【Back-Propagation】"><a href="#【Back-Propagation】" class="headerlink" title="【Back Propagation】"></a>【Back Propagation】</h5><p>The key to backpropagating the error is to use the chain rule for partial derivatives. Take the following neural network as example to demonstrate.</p>
<p>$$<br>o=f_3(w_6\cdot f_2(w_5\cdot f_1(w_1\cdot i_1+w_2\cdot i_2)+w_3\cdot i_3)+w_4\cdot i_4)<br>$$</p>
<p>In gradient descent, in order to calculate $\Delta w_k$, we need to utilize the chain rule to get $\frac{\partial{Loss}}{\partial{w_k}}$. For example, to calculate $\frac{\partial{Loss}}{\partial{w_1}}$:</p>
<p>$$<br>\frac{\partial{Loss}}{\partial{w_1}}=\frac{\partial{Loss}}{\partial{f_3}}\frac{\partial{f_3}}{\partial{f_2}}\frac{\partial{f_2}}{\partial{f_1}}\frac{\partial{f_1}}{\partial{w_1}}<br>$$</p>
<h5 id="【Dropout-Regularization】"><a href="#【Dropout-Regularization】" class="headerlink" title="【Dropout Regularization】"></a>【Dropout Regularization】</h5><p>As a kind of regularization method, dropout will reduce overfitting in neural network by avoiding features co-adaptations.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># set coding environments</span></span><br><span class="line"></span><br><span class="line">p, count, iters, shape = <span class="number">0.5</span>, <span class="number">0.</span>, <span class="number">50</span>, (<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">dropout = nn.Dropout(p=p)</span><br><span class="line">dropout.train()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">    activations = torch.rand(shape) + <span class="number">1e-5</span></span><br><span class="line">    output = dropout(activations)</span><br><span class="line">    count += torch.<span class="built_in">sum</span>(output == activations *( <span class="number">1</span>/(<span class="number">1</span>-p)))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;In training mode Dropout influenced &#123;&#125; neurons&quot;</span>.<span class="built_in">format</span>(<span class="number">1</span>-<span class="built_in">float</span>(count)/(activations.nelement() * iters)))</span><br><span class="line"></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line">dropout.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">    activations = torch.rand(shape) + <span class="number">1e-5</span></span><br><span class="line">    output = dropout(activations)</span><br><span class="line">    count += torch.<span class="built_in">sum</span>(output == activations)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;In evaling mode Dropout influenced &#123;&#125; neurons&quot;</span>.<span class="built_in">format</span>(<span class="number">1</span>-<span class="built_in">float</span>(count)/(activations.nelement() * iters)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>In training mode Dropout influenced <span class="number">0.4952</span> neurons</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>In evaling mode Dropout influenced <span class="number">0.0</span> neurons</span><br></pre></td></tr></table></figure>

<h5 id="【Batch-Normalization】"><a href="#【Batch-Normalization】" class="headerlink" title="【Batch Normalization】"></a>【Batch Normalization】</h5><ul>
<li>Internal Covariate Shift<br>When training neural network, we always need to normalize input data to speed up the training process. However, learning algorithms for example SGD will continuously change the parameters in network and thus the distribution of activations in hidden layer will also change.</li>
<li>Batch Normalization Functions<ul>
<li>Accelerate training</li>
<li>High learning rate ability</li>
<li>Regularization</li>
</ul>
</li>
<li>Batch Normalization Achievement<br>For a specific activation $x^{(k)}$, we can use it to demonstrate how Batch Normalization works.<br>Now define: current batch has $m$ activations that is $\beta$:</li>
</ul>
<p>$$<br>\beta = (x_1,x_2,\cdots,x_m)<br>$$</p>
<p>Firstly, we need to calculate mean and variance of $\beta$</p>
<p>$$<br>\mu_{\beta}=\frac{1}{m}\sum\limits_{i=1}^mx_i,\ \delta^2_{\beta} = \frac{1}{m}\sum\limits_{i=1}^m(x_i-\mu_{\beta})^2<br>$$</p>
<p>Secondly, it’s necessary to normalize $\beta$ using calculated mean $\mu_{\beta}$ and variance $\delta^2_{\beta}$:</p>
<p>$$<br>\widehat{x_i}=\frac{x_i-\mu_{\beta}}{\delta_{\beta}^2+\xi}\sim N(0, 1)<br>$$</p>
<p>And $\xi = 1\times10^{-5}$ is used to avoid zero division.</p>
<ul>
<li>Batch Normalization Affine Mapping<br>Some hidden layers need data that is not standardized distribution, so Batch Normalization provides affine mapping $y_i=\gamma \widehat{x}_i+\beta$ for standard variables $x_i$ to restore expression ability in neural network. And these parameters will be trained with network original weights.<br>In training process, calculates the mean and variance of the moving average:</li>
</ul>
<p>$$<br>running_{mean} = (1-momentum)\times running_{mean} + momentum\times \mu_{\beta}<br>$$</p>
<p>$$<br>running_{var} = (1-momentum)\times running_{var} + momentum\times \delta^2_{\beta}<br>$$</p>
<p>After model training, we can get trained two parameters $\beta$ and $\gamma$ and two variables <code>running_mean</code> and <code>running_var</code>. If we use this model to do inference, we need to do the following transformation:</p>
<p>$$<br>y = \frac{\gamma}{\sqrt{running_{var}}+\xi}\cdot x+(\beta-\frac{\gamma}{\sqrt{running_{var}}+\xi}\cdot running_{mean})<br>$$</p>
<ul>
<li>Batch Normalization Usage<br>In Pytorch, <code>torch.nn.BatchNorm1d</code> achieved Batch Normalization function, also used as a typical layer in neural network. It has two critical parameters: <code>num_features</code> determines the number of features and <code>affine</code> determines whether Batch Normalization uses affine mapping or not.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment"># prepare running environments</span></span><br><span class="line"></span><br><span class="line">m = nn.BatchNorm1d(num_features=<span class="number">5</span>, affine=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;BEFORE:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;running_mean:&quot;</span>, m.running_mean)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;running_var:&quot;</span>, m.running_var)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">    output = m(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;AFTER:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;running_mean:&quot;</span>, m.running_mean)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;running_var:&quot;</span>, m.running_var)</span><br><span class="line"></span><br><span class="line">m.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="built_in">input</span> = torch.randn(<span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">    output = m(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;EVAL:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;running_mean:&quot;</span>, m.running_mean)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;running_var:&quot;</span>, m.running_var)</span><br><span class="line"></span><br><span class="line">BEFORE:</span><br><span class="line">running_mean: tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line">running_var: tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">AFTER:</span><br><span class="line">running_mean: tensor([-<span class="number">0.0585</span>, -<span class="number">0.0522</span>, -<span class="number">0.0177</span>,  <span class="number">0.0318</span>,  <span class="number">0.0267</span>])</span><br><span class="line">running_var: tensor([<span class="number">1.0350</span>, <span class="number">1.0406</span>, <span class="number">0.9752</span>, <span class="number">1.0325</span>, <span class="number">0.9902</span>])</span><br><span class="line">EVAL:</span><br><span class="line">running_mean: tensor([-<span class="number">0.0585</span>, -<span class="number">0.0522</span>, -<span class="number">0.0177</span>,  <span class="number">0.0318</span>,  <span class="number">0.0267</span>])</span><br><span class="line">running_var: tensor([<span class="number">1.0350</span>, <span class="number">1.0406</span>, <span class="number">0.9752</span>, <span class="number">1.0325</span>, <span class="number">0.9902</span>])</span><br></pre></td></tr></table></figure>

<p>In Batch Normalization, two parameters of affine mapping $\gamma$ and $\beta$ are called weight and bias. When not using affine, these parameters will be set <code>None</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;no affine, gamma:&quot;</span>, m.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;no affine, beta:&quot;</span>, m.bias)</span><br><span class="line"></span><br><span class="line">m_affine = nn.BatchNorm1d(num_features=<span class="number">5</span>, affine=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;with affine, gamma:&quot;</span>, m_affine.weight, <span class="built_in">type</span>(m_affine.weight))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;with affine, beta:&quot;</span>, m_affine.bias, <span class="built_in">type</span>(m_affine.bias))</span><br><span class="line">BEFORE:</span><br><span class="line">running_mean: tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line">running_var: tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">AFTER:</span><br><span class="line">running_mean: tensor([-<span class="number">0.0585</span>, -<span class="number">0.0522</span>, -<span class="number">0.0177</span>,  <span class="number">0.0318</span>,  <span class="number">0.0267</span>])</span><br><span class="line">running_var: tensor([<span class="number">1.0350</span>, <span class="number">1.0406</span>, <span class="number">0.9752</span>, <span class="number">1.0325</span>, <span class="number">0.9902</span>])</span><br><span class="line">EVAL:</span><br><span class="line">running_mean: tensor([-<span class="number">0.0585</span>, -<span class="number">0.0522</span>, -<span class="number">0.0177</span>,  <span class="number">0.0318</span>,  <span class="number">0.0267</span>])</span><br><span class="line">running_var: tensor([<span class="number">1.0350</span>, <span class="number">1.0406</span>, <span class="number">0.9752</span>, <span class="number">1.0325</span>, <span class="number">0.9902</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>no affine, gamma: <span class="literal">None</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>no affine, beta: <span class="literal">None</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> affine, gamma: Parameter containing:</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>], requires_grad=<span class="literal">True</span>) &lt;<span class="keyword">class</span> <span class="string">&#x27;torch.nn.parameter.Parameter&#x27;</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> affine, beta: Parameter containing:</span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>], requires_grad=<span class="literal">True</span>) &lt;<span class="keyword">class</span> <span class="string">&#x27;torch.nn.parameter.Parameter&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>

<p>As the results described, the types of <code>m_affine.weight</code> and <code>m_affine.bias</code> are both <code>Parameter</code>. That means they will be particapated in model training process but the types of <code>running_mean</code> and <code>running_var</code> are both <code>Tensor</code> which is called <code>buffer</code> updated and saved as intermediate variables.</p>
<h3 id="Chapter-7-Convolutional-Neural-Network-and-Computer-Vision"><a href="#Chapter-7-Convolutional-Neural-Network-and-Computer-Vision" class="headerlink" title="Chapter 7 Convolutional Neural Network and Computer Vision"></a>Chapter 7 Convolutional Neural Network and Computer Vision</h3><h4 id="7-1-Convolutional-Neural-Network-Basic-Ideas"><a href="#7-1-Convolutional-Neural-Network-Basic-Ideas" class="headerlink" title="7.1 Convolutional Neural Network Basic Ideas"></a>7.1 Convolutional Neural Network Basic Ideas</h4><h5 id="I-Local-Connections"><a href="#I-Local-Connections" class="headerlink" title="I. Local Connections"></a>I. Local Connections</h5><p>For traditional BP neural networks, previous layer usually connects with next layer by global connections.<br>Suppose that there is a former layer with $M$ nodes and a latter layer with $N$ nodes, the network will increase more $M\times N$ weights. So it results in calculation cost and memory cost with the complexity of $O(M\times N)=O(n^2)$.<br>On the other hand, local connections mode only connects nearby nodes between layers, based on the knowledge that only combination of some local pixels presents some features together. If we limit the connections to $C$ nearby nodes in space, the connected weights will be reduced to $C\times N$. In that mode, the complexity of calculation and memory will be also reduced to $O(C\times N)=O(N)$.</p>
<h5 id="II-Parameters-Sharing"><a href="#II-Parameters-Sharing" class="headerlink" title="II. Parameters Sharing"></a>II. Parameters Sharing</h5><p>In image processing, we hold the bilief that the features of image have locality. Because local features don’t have similarity, if we use specific weights for different local features it will not get better performance. And consider different images has great difference in the structure, we share connected weights between nodes with different local features.<br>According to this mode, we can further reduce the number of connected weights to $C$ with the complexity of $O(C)$.</p>
<h4 id="7-2-Convolutional-Operation"><a href="#7-2-Convolutional-Operation" class="headerlink" title="7.2 Convolutional Operation"></a>7.2 Convolutional Operation</h4><p>Discrete convolution operation satisfies above properties which are local connections and  parameters sharing.<br>Define continuous convolution operation $f*g$:</p>
<p>$$<br>(f*g)(t)=\int_{-\infty}^{\infty}f(\tau)g(t-\tau)d\tau<br>$$</p>
<p>And one-dimensional discrete convolution operation:</p>
<p>$$<br>(f*g)(x)=\sum\limits_if(i)g(x-i)<br>$$</p>
<ul>
<li>Multiple Convolutional Kernels<br>CNN usually uses multiple convolutional kernels to extract features for better performance.</li>
<li>Multiple channels convolution</li>
<li>Boundary Filling<br>In order to avoid convolutional feature collapse, we usually fill the bounds of the input tensor to make the center of the convolution kernel can start scanning from the boundary. In this way, the size of the input tensor and output tensor of the convolution operation does not change.</li>
</ul>
<h4 id="7-3-Classic-Network-Structure"><a href="#7-3-Classic-Network-Structure" class="headerlink" title="7.3 Classic Network Structure"></a>7.3 Classic Network Structure</h4><h5 id="I-VGG-Network"><a href="#I-VGG-Network" class="headerlink" title="I. VGG Network"></a>I. VGG Network</h5><p>【Features】</p>
<ul>
<li>Replace big size kernels with multiple size of $3\times 3$ kernels</li>
<li>Same receptive field of the convolutional kernels but deeper network structure</li>
<li>Reduce model’s parameters using size of $3\times 3$ kernels</li>
</ul>
<h5 id="II-InceptionNet"><a href="#II-InceptionNet" class="headerlink" title="II. InceptionNet"></a>II. InceptionNet</h5><h5 id="III-ResNet"><a href="#III-ResNet" class="headerlink" title="III. ResNet"></a>III. ResNet</h5></div><div class="article-licensing box"><div class="licensing-title"><p>Chapter 5 to 8</p><p><a href="https://jiaweihu-xdu.github.io/blog/Chapter5-8/">https://jiaweihu-xdu.github.io/blog/Chapter5-8/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Jiawei Hu</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-09-04</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-12-15</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/Computer-Vision/">Computer Vision,</a><a class="link-muted" rel="tag" href="/tags/Machine-Learning/">Machine Learning </a></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=64ad5faad2ddeb0019614bc5&amp;product=inline-share-buttons&amp;source=platform" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/alipay.png" alt="Alipay"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/wechat.png" alt="Wechat"></span></a><a class="button donate" href="https://www.buymeacoffee.com/DustValor" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/collaboration/ComputerNetwork/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">计算机通信与网络</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/Chapter1-4/"><span class="level-item">Chapter 1 to 4</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://jiaweihu-xdu.github.io/blog/Chapter5-8/';
            this.page.identifier = 'blog/Chapter5-8/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'eblog-5' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-96x96 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpg" alt="Jiawei Hu"></figure><p class="title is-size-4 is-block" style="line-height: 'inherit'; font-family: Times New Roman">Jiawei Hu</p><p style="white-space: pre-line; font-style: italic; font-family: Times New Roman; margin-bottom: 0.50rem; font-size: 1.0em">Computer Science
Machine Learning
</p><p class="is-size-5 is-flex justify-content-center" style="font-family: Times New Roman"><i class="fas fa-map-marker-alt mr-1"></i><span>Xidian University, China</span></p></div></div></nav><nav class="level menu-list is-mobile" style="margin-bottom:1rem"><a class="level-item has-text-centered is-marginless" href="/archives"><div><p class="heading">Posts</p><div><p class="title">33</p></div></div></a><a class="level-item has-text-centered is-marginless" href="/categories"><div><p class="heading">Categories</p><div><p class="title">6</p></div></div></a><a class="level-item has-text-centered is-marginless" href="/tags"><div><p class="heading">Tags</p><div><p class="title">23</p></div></div></a></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/JiaweiHu-XDU" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/JiaweiHu-XDU"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Cnblog" href="https://www.cnblogs.com/MarkStiff/"><i class="fa-brands fa-blogger"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Notion" href="https://zhihaoli.notion.site/"><i class="fa-solid fa-desktop"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:jiaweihu_xdu@163.com"><i class="fa-solid fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="QQ" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1444980878&amp;website=www.oicqzone.com"><i class="fab fa-qq"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Chapter-5-Logistic-Regression"><span class="level-left"><span class="level-item">1</span><span class="level-item">Chapter 5 Logistic Regression</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Linear-Regression"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Linear Regression</span></span></a></li><li><a class="level is-mobile" href="#Logistic-Regression"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">Logistic Regression</span></span></a></li><li><a class="level is-mobile" href="#Pytorch-Logistic-Regression"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">Pytorch Logistic Regression</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#【Prepare-Data】"><span class="level-left"><span class="level-item">1.3.1</span><span class="level-item">【Prepare Data】</span></span></a></li></ul></li><li><a class="level is-mobile" href="#【Prepare-Pytorch-Linear-Model】"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">【Prepare Pytorch Linear Model】</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#【Pytorch-Activation-Function】"><span class="level-left"><span class="level-item">1.4.1</span><span class="level-item">【Pytorch Activation Function】</span></span></a></li><li><a class="level is-mobile" href="#【Loss-Function】"><span class="level-left"><span class="level-item">1.4.2</span><span class="level-item">【Loss Function】</span></span></a></li><li><a class="level is-mobile" href="#【Reconstruct-our-model】"><span class="level-left"><span class="level-item">1.4.3</span><span class="level-item">【Reconstruct our model】</span></span></a></li><li><a class="level is-mobile" href="#【Optimization-Algorithm】"><span class="level-left"><span class="level-item">1.4.4</span><span class="level-item">【Optimization Algorithm】</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Chapter-6-Neural-Network-Basics"><span class="level-left"><span class="level-item">2</span><span class="level-item">Chapter 6 Neural Network Basics</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#I-Basic-characteristics-of-neural-networks"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">I. Basic characteristics of neural networks</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#【Neuron】"><span class="level-left"><span class="level-item">2.1.1</span><span class="level-item">【Neuron】</span></span></a></li><li><a class="level is-mobile" href="#【Activation-Function】"><span class="level-left"><span class="level-item">2.1.2</span><span class="level-item">【Activation Function】</span></span></a></li><li><a class="level is-mobile" href="#【Output-Layer】"><span class="level-left"><span class="level-item">2.1.3</span><span class="level-item">【Output Layer】</span></span></a></li><li><a class="level is-mobile" href="#【Neural-Network】"><span class="level-left"><span class="level-item">2.1.4</span><span class="level-item">【Neural Network】</span></span></a></li></ul></li><li><a class="level is-mobile" href="#II-Perceptron"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">II. Perceptron</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#【Monolayer-Perceptron】"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">【Monolayer Perceptron】</span></span></a></li><li><a class="level is-mobile" href="#【Multi-Layer-Perceptron-MLP】"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">【Multi-Layer Perceptron, MLP】</span></span></a></li></ul></li><li><a class="level is-mobile" href="#III-BP-Neural-Network"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">III. BP Neural Network</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#【Gradient-Descent】"><span class="level-left"><span class="level-item">2.3.1</span><span class="level-item">【Gradient Descent】</span></span></a></li><li><a class="level is-mobile" href="#【Back-Propagation】"><span class="level-left"><span class="level-item">2.3.2</span><span class="level-item">【Back Propagation】</span></span></a></li><li><a class="level is-mobile" href="#【Dropout-Regularization】"><span class="level-left"><span class="level-item">2.3.3</span><span class="level-item">【Dropout Regularization】</span></span></a></li><li><a class="level is-mobile" href="#【Batch-Normalization】"><span class="level-left"><span class="level-item">2.3.4</span><span class="level-item">【Batch Normalization】</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#Chapter-7-Convolutional-Neural-Network-and-Computer-Vision"><span class="level-left"><span class="level-item">3</span><span class="level-item">Chapter 7 Convolutional Neural Network and Computer Vision</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#7-1-Convolutional-Neural-Network-Basic-Ideas"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">7.1 Convolutional Neural Network Basic Ideas</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#I-Local-Connections"><span class="level-left"><span class="level-item">3.1.1</span><span class="level-item">I. Local Connections</span></span></a></li><li><a class="level is-mobile" href="#II-Parameters-Sharing"><span class="level-left"><span class="level-item">3.1.2</span><span class="level-item">II. Parameters Sharing</span></span></a></li></ul></li><li><a class="level is-mobile" href="#7-2-Convolutional-Operation"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">7.2 Convolutional Operation</span></span></a></li><li><a class="level is-mobile" href="#7-3-Classic-Network-Structure"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">7.3 Classic Network Structure</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#I-VGG-Network"><span class="level-left"><span class="level-item">3.3.1</span><span class="level-item">I. VGG Network</span></span></a></li><li><a class="level is-mobile" href="#II-InceptionNet"><span class="level-left"><span class="level-item">3.3.2</span><span class="level-item">II. InceptionNet</span></span></a></li><li><a class="level is-mobile" href="#III-ResNet"><span class="level-left"><span class="level-item">3.3.3</span><span class="level-item">III. ResNet</span></span></a></li></ul></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/"><span class="level-start"><span class="level-item">blog</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/collaboration/"><span class="level-start"><span class="level-item">collaboration</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/knowledge/"><span class="level-start"><span class="level-item">knowledge</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/life/"><span class="level-start"><span class="level-item">life</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/projects/"><span class="level-start"><span class="level-item">projects</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/readings/"><span class="level-start"><span class="level-item">readings</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Collaboration-Project/"><span class="tag">Collaboration Project</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/College-Life/"><span class="tag">College Life</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Computer-Network/"><span class="tag">Computer Network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Computer-Principle/"><span class="tag">Computer Principle</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Computer-Vision/"><span class="tag">Computer Vision</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Visualization/"><span class="tag">Data Visualization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-Reinforcement-Learning/"><span class="tag">Deep Reinforcement Learning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Eletromagnetic-B%E6%B5%8B/"><span class="tag">Eletromagnetic B测</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Eletromagnetic-Physics/"><span class="tag">Eletromagnetic Physics</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embedded-System/"><span class="tag">Embedded System</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Health/"><span class="tag">Health</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Life-Knowledge/"><span class="tag">Life Knowledge</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Life-Wisdom/"><span class="tag">Life Wisdom</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Literature-Survey/"><span class="tag">Literature Survey</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mathematical-Modeling/"><span class="tag">Mathematical Modeling</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Microcomputer/"><span class="tag">Microcomputer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenSSL/"><span class="tag">OpenSSL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Professional-Knowledge/"><span class="tag">Professional Knowledge</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Project-Tools/"><span class="tag">Project Tools</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reinforcement-Learning/"><span class="tag">Reinforcement Learning</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Jiawei Hu - Jiawei Hu&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2023-2024 Jiawei Hu</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv"><span id="busuanzi_container_site_uv">Site UV: <span id="busuanzi_value_site_uv"></span></span>   <span id="busuanzi_container_site_pv">Site PV: <span id="busuanzi_value_site_pv"></span></span></span></p><p class="is-size-7"> </p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/jiaweiHu-XDU/jiaweiHu-XDU.github.io"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><div id="dark" onclick="switchDarkMode()"></div><script type="text/javascript" src="/js/universe.js"></script></body></html>