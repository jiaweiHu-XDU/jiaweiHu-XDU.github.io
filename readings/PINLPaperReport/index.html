<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>A Literature Survey about Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels - Jiawei Hu - Jiawei Hu&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Jiawei Hu - Jiawei Hu&#039;s Blog"><meta name="msapplication-TileImage" content="/img/logo.png"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jiawei Hu - Jiawei Hu&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="This is a literature survey about the paper of Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels."><meta property="og:type" content="blog"><meta property="og:title" content="A Literature Survey about Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels"><meta property="og:url" content="https://jiaweihu-xdu.github.io/readings/PINLPaperReport/"><meta property="og:site_name" content="Jiawei Hu - Jiawei Hu&#039;s Blog"><meta property="og:description" content="This is a literature survey about the paper of Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels."><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/EBlog/Learning/image.738drfuftks0.webp"><meta property="article:published_time" content="2024-01-28T08:53:27.000Z"><meta property="article:modified_time" content="2024-01-30T13:57:36.716Z"><meta property="article:author" content="Jiawei Hu"><meta property="article:tag" content="Literature Survey"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/EBlog/Learning/image.738drfuftks0.webp"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://jiaweihu-xdu.github.io/readings/PINLPaperReport/"},"headline":"A Literature Survey about Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels","image":["https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/EBlog/Learning/image.738drfuftks0.webp"],"datePublished":"2024-01-28T08:53:27.000Z","dateModified":"2024-01-30T13:57:36.716Z","author":{"@type":"Person","name":"Jiawei Hu"},"publisher":{"@type":"Organization","name":"Jiawei Hu - Jiawei Hu's Blog","logo":{"@type":"ImageObject","url":"https://jiaweihu-xdu.github.io/img/logo.png"}},"description":"This is a literature survey about the paper of Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels."}</script><link rel="canonical" href="https://jiaweihu-xdu.github.io/readings/PINLPaperReport/"><link rel="icon" href="/img/logo.png"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/6.0.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.lug.ustc.edu.cn/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js"></script><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body class="is-3-column"><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Jiawei Hu - Jiawei Hu&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">JIAWEI HU&#039;S LOG BOOK</a><a class="navbar-item" href="/blog">BLOG</a><a class="navbar-item" href="/knowledge">KNOWLEDGE</a><a class="navbar-item" href="/publications">PUBLICATIONS</a><a class="navbar-item" href="/projects">PROJECTS</a><a class="navbar-item" href="/life">LIFE</a><a class="navbar-item" href="/readings">READINGS</a><a class="navbar-item" href="/collaboration">COLLABORATION</a><a class="navbar-item" href="/archives">ARCHIVES</a><a class="navbar-item" href="/categories">CATEGORIES</a><a class="navbar-item" href="/tags">TAGS</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/jiaweiHu-XDU/jiaweiHu-XDU.github.io"><i class="fab fa-github"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Gitee" href="https://gitee.com/hujiawe_i"><i class="fa-brands fa-gofore"></i></a><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">A Literature Survey about Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2024-01-28T08:53:27.000Z" title="2024-01-28T08:53:27.000Z">2024-01-28</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="2024-01-30T13:57:36.716Z" title="2024-01-30T13:57:36.716Z">2024-01-30</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/readings/">readings</a></span><span class="level-item"><i class="far fa-clock"></i> 9 minutes read (About 1348 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><div class="content"><h3 id="I-Summary-Overview"><a href="#I-Summary-Overview" class="headerlink" title="I.Summary Overview"></a>I.Summary Overview</h3><p><strong>Background</strong>: A vision-language model can be adapted to a new classification task through few-shot prompt tuning. We find that such a prompt tuning process is highly robust to label noises.<br><strong>Interest</strong>: Studying the key reasons contributing to the robustness of the prompt tuning paradigm.</p>
<p><strong>Findings</strong>: </p>
<ol>
<li><em>the fixed classname tokens</em> <strong>provide a strong regularization</strong> to the optimization of the model, <strong>reducing gradients induced by the noisy samples</strong>;</li>
<li>the powerful <em>pre-trained image-text embedding</em> that is learned from diverse and generic web data <strong>provides strong prior knowledge for image classification</strong>.</li>
</ol>
<h3 id="II-Research-Interests"><a href="#II-Research-Interests" class="headerlink" title="II.Research Interests"></a>II.Research Interests</h3><p>The author studies the key reasons contributing to the robustness of the prompt tuning paradigm.</p>
<h3 id="III-Problems-Solved"><a href="#III-Problems-Solved" class="headerlink" title="III.Problems Solved"></a>III.Problems Solved</h3><p>In author’s work, they demonstrate that prompt tuning is robust to noisy labels, and investigate the mechanisms that enable this robustness.</p>
<h3 id="IV-Previous-Research"><a href="#IV-Previous-Research" class="headerlink" title="IV.Previous Research"></a>IV.Previous Research</h3><p>While prompt tuning has proven effective when training on downstream tasks with accurately annotated datasets, their robustness to noisy labels has been neglected.</p>
<h3 id="V-Author’s-Innovation"><a href="#V-Author’s-Innovation" class="headerlink" title="V.Author’s Innovation"></a>V.Author’s Innovation</h3><p>The author investigates the mechanisms that enable this robustness and proposes a simple yet effective method for unsupervised prompt tuning, showing that randomly selected noisy pseudo labels can be effectively used to enhance CLIP zero-shot performance.</p>
<h3 id="VI-Author’s-Contribution"><a href="#VI-Author’s-Contribution" class="headerlink" title="VI.Author’s Contribution"></a>VI.Author’s Contribution</h3><ul>
<li>We demonstrate that <strong>prompt tuning for pre-trained vision-language models (e.g., CLIP) is more robust to noisy labels</strong> than traditional transfer learning approaches, such as model fine-tuning and linear probes.</li>
<li>We further demonstrate that <strong>prompt tuning robustness can be further enhanced through the use of a robust training objective</strong>.</li>
<li>We conduct an extensive analysis on why prompt tuning is robust to noisy labels to <strong>discover which components contribute the most to its robustness</strong>.</li>
<li>Motivated by this property, we <strong>propose a simple yet effective method for unsupervised prompt tuning</strong>, showing that randomly selected noisy pseudo labels can be effectively used to enhance CLIP zero-shot performance. The proposed robust prompt tuning outperformed prior work on a variety of datasets, even though noisier pseudo-labels are used for self-training.</li>
</ul>
<h3 id="VII-Algorithm-Flow"><a href="#VII-Algorithm-Flow" class="headerlink" title="VII.Algorithm Flow"></a>VII.Algorithm Flow</h3><h4 id="Recent-Research"><a href="#Recent-Research" class="headerlink" title="Recent Research"></a>Recent Research</h4><ul>
<li>CLIP: CLIP applies prompt engineering to incorporate the category information in the text input such that its pre-trained model can adapt to various image classification tasks without further training.</li>
<li>CoOp: CoOp introduces learnable prompts optimized on target datasets to address CLIP’s problem</li>
<li>ProDA: ProDA tackles CoOp’s issue by utilizing diverse prompts to capture the distribution of varying visual representations.</li>
<li>UPL: UPL proposes a framework to perform prompt tuning without labeled data.</li>
<li>TPT: TPT achieves zero-shot transfer by dynamically adjusting prompts using only a single test sample.</li>
<li>Potential of prompt tuning: Label noise-robust learning</li>
<li>Label noise-robust learning<ul>
<li>robust losses that tolerate noisy labels</li>
<li>loss correction approaches that estimate a transition matrix to correct the predictions</li>
<li>meta-learning frameworks that learn to correct the label noise in training examples</li>
<li>regularization techniques that are customized to lower the negative impact of noise</li>
</ul>
</li>
</ul>
<h4 id="Existing-Problems"><a href="#Existing-Problems" class="headerlink" title="Existing Problems"></a>Existing Problems</h4><ul>
<li>CLIP: the design of a proper prompt is challenging and requires heuristics.</li>
<li>CoOp: CoOp has also faced criticism for disregarding the diversity of visual representations.</li>
</ul>
<h4 id="Author’s-Processing"><a href="#Author’s-Processing" class="headerlink" title="Author’s Processing"></a>Author’s Processing</h4><ul>
<li>Demonstrate that prompt tuning on CLIP naturally holds powerful noise robustness.</li>
<li>Explore the key factors behind such robustness.</li>
<li>Show its application on unsupervised prompt tuning.</li>
</ul>
<h4 id="Constructed-Model"><a href="#Constructed-Model" class="headerlink" title="Constructed Model"></a>Constructed Model</h4><ul>
<li><p>CLIP<br>In the case of image classification, a normalized image embedding $\boldsymbol{f}^v$ is obtained by passing an image $\boldsymbol{x}$ through CLIP’s visual encoder, and a set of normalized class embeddings $[\boldsymbol{f}^t_i]^K_{i=1}$ by feeding template prompts of the form “A photo of a <CLS>“ into CLIP’s text encoder.<br>$$<br>Pr(y=i|\boldsymbol{x})=\frac{\exp(sim(\boldsymbol{f}^v,\boldsymbol{f}^t_i))/\tau}{\sum_{j=1}^K\exp(sim(\boldsymbol{f}^v,\boldsymbol{f}^t_j))/\tau}<br>$$</p>
</li>
<li><p>Prompt Tuning<br>The name of a class c is first converted into a classname embedding $\boldsymbol{w}\in R^d$ and prepended with a sequence of $M$ learnable tokens $\boldsymbol{p_m}\in R^d$ shared across all classes.<br>$$<br>P_c=[\boldsymbol{p_1}, \boldsymbol{p_2}, \cdots, \boldsymbol{p_M}, \boldsymbol{w_c}]\rightarrow \boldsymbol{f}^t_c<br>$$<br>CoOp optimizes the shared learnable tokens $\boldsymbol{p_1}, \boldsymbol{p_1}, \cdots, \boldsymbol{p_M}$ on a small labeled dataset $D = [(\boldsymbol{x_i}, c_i)^N_{i=1}]$ to minimize the cross-entropy loss<br>$$<br>L_{CE}=-E_{(\boldsymbol{x},c)\in D}[\log Pr(y=c|\boldsymbol{x})].<br>$$</p>
</li>
<li><p>Robust Prompt Tuning<br>Further enhance this robustness by optimizing the learnable prompts using the generalized cross-entropy (GCE) loss<br>$$<br>L_{GCE}=E_{(\boldsymbol{x},c)\in D}[\frac{1-Pr(y=c|\boldsymbol{x})^q}{q}].<br>$$</p>
</li>
<li><p>Author’s Conclusion: $q = 0.7$ leads to overall good performance across several experimental settings.</p>
</li>
</ul>
<h3 id="VIII-Robustness-Analysis"><a href="#VIII-Robustness-Analysis" class="headerlink" title="VIII.Robustness Analysis"></a>VIII.Robustness Analysis</h3><img src="https://cdn.jsdelivr.net/gh/LZHMS/picx-images-hosting@master/EBlog/Learning/image.738drfuftks0.webp" alt="Different Model Structures" width="90%"/>

<h4 id="Pre-trained-CLIP-Generates-Effective-Class-Embeddings"><a href="#Pre-trained-CLIP-Generates-Effective-Class-Embeddings" class="headerlink" title="Pre-trained CLIP Generates Effective Class Embeddings"></a>Pre-trained CLIP Generates Effective Class Embeddings</h4><ul>
<li>Author’s Conclusions:<ul>
<li>Classifier-R v.s. Classifier-C: CLIP class embeddings provide a strong initialization for few-shot learning.</li>
<li>TEnc-FT v.s. Classifier-C: The highly expressive CLIP text encoder can easily overfit to the noisy labels.</li>
<li>Prompt Tuning v.s. Classifiers: The text encoder is essential for providing a strong but informative regularization of the text embeddings to combat noisy inputs.</li>
<li>Prompt Tuning v.s. TEnc-FT: The text encoder should be fixed to prevent overfitting.</li>
</ul>
</li>
</ul>
<h4 id="Effectiveness-of-Prompt"><a href="#Effectiveness-of-Prompt" class="headerlink" title="Effectiveness of Prompt"></a>Effectiveness of Prompt</h4><ul>
<li>Author’s Conclusions:<ul>
<li>Full Prompt Tuning v.s. CLS Tuning: The class embeddings generated by CLIP pre-trained text encoder plays a critical role in noise robustness.</li>
</ul>
</li>
<li>Hypothesis:<ul>
<li>The classname token $\boldsymbol{w_c}$ provides a strong regularization to the model, since it is leveraged by the text encoder to encode relationships between the different visual concepts.</li>
</ul>
</li>
</ul>
<h4 id="Prompt-Tuning-Suppresses-Noisy-Gradients"><a href="#Prompt-Tuning-Suppresses-Noisy-Gradients" class="headerlink" title="Prompt Tuning Suppresses Noisy Gradients"></a>Prompt Tuning Suppresses Noisy Gradients</h4><ul>
<li>Prompt tuning can suppress gradient updates from noisy samples, while aggregating gradients from clean samples.</li>
<li>This property likely arises from the highly constrained prompt tuning optimization, which restricting the model to fit the noisy labels.</li>
</ul>
<h4 id="Generalization-Across-Model-Architectures"><a href="#Generalization-Across-Model-Architectures" class="headerlink" title="Generalization Across Model Architectures"></a>Generalization Across Model Architectures</h4><ul>
<li>Context length<ul>
<li>The optimal context length is dataset dependent.</li>
</ul>
</li>
<li>Image encoders<ul>
<li>ViT-B/32-PT outperforms RN50-PT under most settings. Moreover, both methods do not suffer from a large performance drop and maintain competitive accuracy at high noise rates.</li>
</ul>
</li>
</ul>
<h4 id="Robustness-to-Correlated-Label-Noise"><a href="#Robustness-to-Correlated-Label-Noise" class="headerlink" title="Robustness to Correlated Label Noise"></a>Robustness to Correlated Label Noise</h4><ul>
<li>Confusion noise: Each mislabeled sample is labeled as the incorrect class that is most favored by zero-shot CLIP.</li>
<li>Author’s Conclusions:<ul>
<li>Confusion noise presents a bigger challenge to transfer learning, leading to larger degradation of classification accuracy at high noise ratios compared to random noise.</li>
<li>Prompt tuning still achieves the best overall performance, providing further evidence for its robustness even to <strong>more challenging types of noise</strong>.</li>
</ul>
</li>
</ul>
<h3 id="IX-Application-to-Unsupervised-Prompt-Tuning"><a href="#IX-Application-to-Unsupervised-Prompt-Tuning" class="headerlink" title="IX.Application to Unsupervised Prompt Tuning"></a>IX.Application to Unsupervised Prompt Tuning</h3><ul>
<li>Baseline UPL<ul>
<li>Phase 1: Leverage pre-trained CLIP to generate pseudo labels for unlabeled images.</li>
<li>Phase 2: Select <strong>the $K$ most confident samples per class</strong> to optimize the learnable tokens through the typical prompt-tuning optimization process (described in CoOp).</li>
<li>Features: UPL improved transfer performance by ensembling multiple predictions generated by models with different learnable prompts.</li>
</ul>
</li>
<li>Robust UPL<ul>
<li>Overview: Based on UPL, <strong>randomly sample $K$ training samples</strong> and optimize the prompt with the robust GCE loss</li>
</ul>
</li>
</ul>
<h3 id="X-Summary-And-Views"><a href="#X-Summary-And-Views" class="headerlink" title="X.Summary And Views"></a>X.Summary And Views</h3><h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p>This paper focus on <em>prompt tuning</em> to research and analyze the attribution of  <em>robustness to label noise</em> that it has naturally. And the author also combines the findings with the UPL model and proposes a more robust UPL model in unsupervised prompt tuning.</p>
<h4 id="Personal-Views"><a href="#Personal-Views" class="headerlink" title="Personal Views"></a>Personal Views</h4><p>Firstly I learned a lot from this paper which analysis the robust of prompt tuning to label noise. This research spirit and methodology is a great need in motivating me to work on the research of robustness. And what’s impresses me most is the robust UPL model that is the author’s innovation about the previous research.</p>
<h3 id="XI-Domain-Learning"><a href="#XI-Domain-Learning" class="headerlink" title="XI.Domain Learning"></a>XI.Domain Learning</h3><h4 id="Related-Terms"><a href="#Related-Terms" class="headerlink" title="Related Terms"></a>Related Terms</h4><ul>
<li><em>Vision-language model</em></li>
<li><em>text-image embedding and image-text embedding</em></li>
<li><em>few-shot prompt tuning</em></li>
<li><em>fixed classname tokens</em></li>
<li><em>zero-shot learning</em></li>
<li><em>downstream tasks: few-shot learning, continual learning, object segmentation</em></li>
<li><em>model-informed structure</em></li>
<li><em>traditional fine-tuning and linear probing paradigms</em></li>
<li><em>generalized cross-entropy (GCE)</em></li>
<li><em>VisionLanguage Pre-Trained Models (VL-PTMs)</em></li>
<li><em>meta-learning</em></li>
</ul>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2307.11978">Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>A Literature Survey about Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels</p><p><a href="https://jiaweihu-xdu.github.io/readings/PINLPaperReport/">https://jiaweihu-xdu.github.io/readings/PINLPaperReport/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Jiawei Hu</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-01-28</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-01-30</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/Literature-Survey/">Literature Survey </a></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=64ad5faad2ddeb0019614bc5&amp;product=inline-share-buttons&amp;source=platform" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/img/alipay.png" alt="Alipay"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/img/wechat.png" alt="Wechat"></span></a><a class="button donate" href="https://www.buymeacoffee.com/DustValor" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/GeneticAlgorithm/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Genetic Algorithm</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/blog/WritingSkills/"><span class="level-item">My Knowledge about Paper Writing</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://jiaweihu-xdu.github.io/readings/PINLPaperReport/';
            this.page.identifier = 'readings/PINLPaperReport/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'eblog-5' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-96x96 mx-auto mb-2"><img class="avatar is-rounded" src="/img/avatar.jpg" alt="Jiawei Hu"></figure><p class="title is-size-4 is-block" style="line-height: 'inherit'; font-family: Times New Roman">Jiawei Hu</p><p style="white-space: pre-line; font-style: italic; font-family: Times New Roman; margin-bottom: 0.50rem; font-size: 1.0em">Computer Science
Machine Learning
</p><p class="is-size-5 is-flex justify-content-center" style="font-family: Times New Roman"><i class="fas fa-map-marker-alt mr-1"></i><span>Xidian University, China</span></p></div></div></nav><nav class="level menu-list is-mobile" style="margin-bottom:1rem"><a class="level-item has-text-centered is-marginless" href="/archives"><div><p class="heading">Posts</p><div><p class="title">39</p></div></div></a><a class="level-item has-text-centered is-marginless" href="/categories"><div><p class="heading">Categories</p><div><p class="title">7</p></div></div></a><a class="level-item has-text-centered is-marginless" href="/tags"><div><p class="heading">Tags</p><div><p class="title">25</p></div></div></a></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/JiaweiHu-XDU" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/JiaweiHu-XDU"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Cnblog" href="https://www.cnblogs.com/MarkStiff/"><i class="fa-brands fa-blogger"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Notion" href="https://zhihaoli.notion.site/"><i class="fa-solid fa-desktop"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:jiaweihu_xdu@163.com"><i class="fa-solid fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="QQ" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1444980878&amp;website=www.oicqzone.com"><i class="fab fa-qq"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#I-Summary-Overview"><span class="level-left"><span class="level-item">1</span><span class="level-item">I.Summary Overview</span></span></a></li><li><a class="level is-mobile" href="#II-Research-Interests"><span class="level-left"><span class="level-item">2</span><span class="level-item">II.Research Interests</span></span></a></li><li><a class="level is-mobile" href="#III-Problems-Solved"><span class="level-left"><span class="level-item">3</span><span class="level-item">III.Problems Solved</span></span></a></li><li><a class="level is-mobile" href="#IV-Previous-Research"><span class="level-left"><span class="level-item">4</span><span class="level-item">IV.Previous Research</span></span></a></li><li><a class="level is-mobile" href="#V-Author’s-Innovation"><span class="level-left"><span class="level-item">5</span><span class="level-item">V.Author’s Innovation</span></span></a></li><li><a class="level is-mobile" href="#VI-Author’s-Contribution"><span class="level-left"><span class="level-item">6</span><span class="level-item">VI.Author’s Contribution</span></span></a></li><li><a class="level is-mobile" href="#VII-Algorithm-Flow"><span class="level-left"><span class="level-item">7</span><span class="level-item">VII.Algorithm Flow</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Recent-Research"><span class="level-left"><span class="level-item">7.1</span><span class="level-item">Recent Research</span></span></a></li><li><a class="level is-mobile" href="#Existing-Problems"><span class="level-left"><span class="level-item">7.2</span><span class="level-item">Existing Problems</span></span></a></li><li><a class="level is-mobile" href="#Author’s-Processing"><span class="level-left"><span class="level-item">7.3</span><span class="level-item">Author’s Processing</span></span></a></li><li><a class="level is-mobile" href="#Constructed-Model"><span class="level-left"><span class="level-item">7.4</span><span class="level-item">Constructed Model</span></span></a></li></ul></li><li><a class="level is-mobile" href="#VIII-Robustness-Analysis"><span class="level-left"><span class="level-item">8</span><span class="level-item">VIII.Robustness Analysis</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Pre-trained-CLIP-Generates-Effective-Class-Embeddings"><span class="level-left"><span class="level-item">8.1</span><span class="level-item">Pre-trained CLIP Generates Effective Class Embeddings</span></span></a></li><li><a class="level is-mobile" href="#Effectiveness-of-Prompt"><span class="level-left"><span class="level-item">8.2</span><span class="level-item">Effectiveness of Prompt</span></span></a></li><li><a class="level is-mobile" href="#Prompt-Tuning-Suppresses-Noisy-Gradients"><span class="level-left"><span class="level-item">8.3</span><span class="level-item">Prompt Tuning Suppresses Noisy Gradients</span></span></a></li><li><a class="level is-mobile" href="#Generalization-Across-Model-Architectures"><span class="level-left"><span class="level-item">8.4</span><span class="level-item">Generalization Across Model Architectures</span></span></a></li><li><a class="level is-mobile" href="#Robustness-to-Correlated-Label-Noise"><span class="level-left"><span class="level-item">8.5</span><span class="level-item">Robustness to Correlated Label Noise</span></span></a></li></ul></li><li><a class="level is-mobile" href="#IX-Application-to-Unsupervised-Prompt-Tuning"><span class="level-left"><span class="level-item">9</span><span class="level-item">IX.Application to Unsupervised Prompt Tuning</span></span></a></li><li><a class="level is-mobile" href="#X-Summary-And-Views"><span class="level-left"><span class="level-item">10</span><span class="level-item">X.Summary And Views</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Summary"><span class="level-left"><span class="level-item">10.1</span><span class="level-item">Summary</span></span></a></li><li><a class="level is-mobile" href="#Personal-Views"><span class="level-left"><span class="level-item">10.2</span><span class="level-item">Personal Views</span></span></a></li></ul></li><li><a class="level is-mobile" href="#XI-Domain-Learning"><span class="level-left"><span class="level-item">11</span><span class="level-item">XI.Domain Learning</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Related-Terms"><span class="level-left"><span class="level-item">11.1</span><span class="level-item">Related Terms</span></span></a></li></ul></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">12</span><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/blog/"><span class="level-start"><span class="level-item">blog</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile" href="/collaboration/"><span class="level-start"><span class="level-item">collaboration</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/knowledge/"><span class="level-start"><span class="level-item">knowledge</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/life/"><span class="level-start"><span class="level-item">life</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/projects/"><span class="level-start"><span class="level-item">projects</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/publications/"><span class="level-start"><span class="level-item">publications</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/readings/"><span class="level-start"><span class="level-item">readings</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Collaboration-Project/"><span class="tag">Collaboration Project</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/College-Life/"><span class="tag">College Life</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Computer-Network/"><span class="tag">Computer Network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Computer-Principle/"><span class="tag">Computer Principle</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Computer-Vision/"><span class="tag">Computer Vision</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Visualization/"><span class="tag">Data Visualization</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-Reinforcement-Learning/"><span class="tag">Deep Reinforcement Learning</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Eletromagnetic-B%E6%B5%8B/"><span class="tag">Eletromagnetic B测</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Eletromagnetic-Physics/"><span class="tag">Eletromagnetic Physics</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Embedded-System/"><span class="tag">Embedded System</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Health/"><span class="tag">Health</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Life-Knowledge/"><span class="tag">Life Knowledge</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Life-Wisdom/"><span class="tag">Life Wisdom</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Literature-Survey/"><span class="tag">Literature Survey</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mathematical-Modeling/"><span class="tag">Mathematical Modeling</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Microcomputer/"><span class="tag">Microcomputer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenSSL/"><span class="tag">OpenSSL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Professional-Knowledge/"><span class="tag">Professional Knowledge</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Project-Habits/"><span class="tag">Project Habits</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Project-Tools/"><span class="tag">Project Tools</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Reinforcement-Learning/"><span class="tag">Reinforcement Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Research-Habits/"><span class="tag">Research Habits</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Jiawei Hu - Jiawei Hu&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2023-2024 Jiawei Hu</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv"><span id="busuanzi_container_site_uv">Site UV: <span id="busuanzi_value_site_uv"></span></span>   <span id="busuanzi_container_site_pv">Site PV: <span id="busuanzi_value_site_pv"></span></span></span></p><p class="is-size-7"> </p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/jiaweiHu-XDU/jiaweiHu-XDU.github.io"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.css"><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/katex.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/auto-render.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.15.1/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script><div id="dark" onclick="switchDarkMode()"></div><script type="text/javascript" src="/js/universe.js"></script></body></html>